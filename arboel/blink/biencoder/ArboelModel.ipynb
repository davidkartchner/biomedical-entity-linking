{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "import logger\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim, nn, utils, Tensor\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import lightning as L\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_transformers.tokenization_bert import BertTokenizer\n",
    "from DataModule import process_mention_dataset, process_ontology\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler) \n",
    "from pytorch_transformers.optimization import WarmupLinearSchedule \n",
    "from scipy.sparse.csgraph import minimum_spanning_tree \n",
    "# csgraph = compressed sparse graph\n",
    "from scipy.sparse import csr_matrix\n",
    "# csr_matrix = compressed sparse row matrices\n",
    "from collections import Counter \n",
    "\n",
    "import blink.biencoder.data_process_mult as data_process\n",
    "import blink.biencoder.eval_cluster_linking as eval_cluster_linking\n",
    "import blink.candidate_ranking.utils as utils\n",
    "from blink.biencoder.biencoder import BiEncoderRanker\n",
    "from blink.common.optimizer import get_bert_optimizer\n",
    "from blink.common.params import BlinkParser\n",
    "\n",
    "from IPython import embed \n",
    "\n",
    "def evaluate(\n",
    "    reranker,\n",
    "    valid_dict_vecs, \n",
    "    valid_men_vecs, \n",
    "    # device, #no longer used\n",
    "    logger, \n",
    "    # knn,  #not even used\n",
    "    n_gpu, \n",
    "    entity_data, \n",
    "    query_data,\n",
    "    silent=False, \n",
    "    use_types=False, \n",
    "    embed_batch_size=768, \n",
    "    force_exact_search=False, \n",
    "    probe_mult_factor=1,\n",
    "    within_doc=False, \n",
    "    context_doc_ids=None ):\n",
    "    '''\n",
    "    Description \n",
    "    -----------\n",
    "    1) Computes embeddings and indexes for entities and mentions. \n",
    "    2) Performs k-nearest neighbors (k-NN) search to establish relationships between them.\n",
    "    3) Constructs graphs based on these relationships.\n",
    "    4) Evaluates the model's accuracy by analyzing how effectively the model can link mentions to the correct entities.\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    reranker : BiEncoderRanker\n",
    "        NN-based ranking model\n",
    "    valid_dict_vec : list or ndarray\n",
    "        Ground truth dataset containing the entities\n",
    "    valid_men_vecs : list or ndarray\n",
    "        Dataset containing mentions\n",
    "    device : str\n",
    "        cpu or gpu\n",
    "    logger : 'Logger' object\n",
    "        Logging object used to record messages\n",
    "    knn : int\n",
    "        Number of neighbors\n",
    "    n_gpu : int\n",
    "        Number of gpu\n",
    "    entity_data : list or dict\n",
    "        Entities from the data\n",
    "    query_data : list or dict\n",
    "        Queries / mentions against which the entities are evaluated\n",
    "    silent=False : bool \n",
    "        When set to \"True\", likely suppresses the output or logging of progress updates to keep the console output clean.\n",
    "    use_types=False : bool\n",
    "        A boolean flag that indicates whether or not to use type-specific indexes for entities and mentions\n",
    "    embed_batch_size=768 : int\n",
    "        The batch size to use when processing embeddings.\n",
    "    force_exact_search=False : bool\n",
    "        force the embedding process to use exact search methods rather than approximate methods.\n",
    "    probe_mult_factor=1 : int\n",
    "        A multiplier factor used in index building for probing in case of approximate search (bigger = better but slower)\n",
    "    within_doc=False : bool\n",
    "        Boolean flag that indicates whether the evaluation should be constrained to within-document contexts\n",
    "    context_doc_ids=None : bool\n",
    "        This would be used in conjunction with within_doc to limit evaluations within the same document.\n",
    "    '''\n",
    "    torch.cuda.empty_cache() # Empty the CUDA cache to free up GPU memory\n",
    "    \n",
    "    n_entities = len(valid_dict_vecs) # total number of entities\n",
    "    n_mentions = len(valid_men_vecs) # total number of mentions\n",
    "    max_knn = 8 # max number of neighbors\n",
    "    \n",
    "    joint_graphs = {} # Store results of the NN search and distance between entities and mentions\n",
    "    \n",
    "    for k in [0, 1, 2, 4, 8]:\n",
    "        joint_graphs[k] = { #DD3\n",
    "            \"rows\": np.array([]),\n",
    "            \"cols\": np.array([]),\n",
    "            \"data\": np.array([]),\n",
    "            \"shape\": (n_entities + n_mentions, n_entities + n_mentions),\n",
    "        }\n",
    "        \n",
    "    \n",
    "    '1) Computes embeddings and indexes for entities and mentions. '\n",
    "    '''\n",
    "    This block is preparing the data for evaluation by transforming raw vectors into a format that can be efficiently used for retrieval and comparison operations\n",
    "    '''\n",
    "    if use_types: # corpus = entity data\n",
    "                  # corpus is a collection of entities, which is used to build type-specific search indexes if provided.\n",
    "        '''\n",
    "        With a Corpus : Multiple type-specific indexes are created, allowing for more targeted and efficient searches within specific categories of entities.\n",
    "        'dict_embeds' and 'men_embeds': The resulting entity and mention embeddings.\n",
    "        'dict_indexes' and 'men_indexes': Dictionary that will store search indexes (!= indices)for each unique entity type found in the corpus\n",
    "        'dict_idxs_by_type' and 'men_idxs_by_type': Dictionary to store indices of the corpus elements, grouped by their entity type.\n",
    "        !!! idxs = indices / indexes = indexes !!!\n",
    "        '''\n",
    "        logger.info(\"Eval: Dictionary: Embedding and building index\") # For entities\n",
    "        dict_embeds, dict_indexes, dict_idxs_by_type = data_process.embed_and_index(\n",
    "            reranker,\n",
    "            valid_dict_vecs,\n",
    "            encoder_type=\"candidate\",\n",
    "            n_gpu=n_gpu,\n",
    "            corpus=entity_data, \n",
    "            force_exact_search=force_exact_search, \n",
    "            batch_size=embed_batch_size, \n",
    "            probe_mult_factor=probe_mult_factor, \n",
    "            )\n",
    "        logger.info(\"Eval: Queries: Embedding and building index\") # For mentions\n",
    "        men_embeds, men_indexes, men_idxs_by_type = data_process.embed_and_index(\n",
    "            reranker,\n",
    "            valid_men_vecs,\n",
    "            encoder_type=\"context\",\n",
    "            n_gpu=n_gpu,\n",
    "            corpus=query_data,\n",
    "            force_exact_search=force_exact_search,\n",
    "            batch_size=embed_batch_size,\n",
    "            probe_mult_factor=probe_mult_factor,\n",
    "        )\n",
    "    else: # corpus = None\n",
    "        '''\n",
    "        Without a Corpus: A single, general index is created for all embeddings, suitable for broad searches across the entire dataset.\n",
    "        'dict_embeds' and 'men_embeds': The resulting entity and mention embeddings.\n",
    "        'dict_index' and 'men_index': Dictionary that will store search index\n",
    "        '''\n",
    "        logger.info(\"Eval: Dictionary: Embedding and building index\")\n",
    "        dict_embeds, dict_index = data_process.embed_and_index(\n",
    "            reranker,\n",
    "            valid_dict_vecs,\n",
    "            \"candidate\",\n",
    "            n_gpu=n_gpu,\n",
    "            force_exact_search=force_exact_search,\n",
    "            batch_size=embed_batch_size,\n",
    "            probe_mult_factor=probe_mult_factor,\n",
    "        )\n",
    "        logger.info(\"Eval: Queries: Embedding and building index\")\n",
    "        men_embeds, men_index = data_process.embed_and_index(\n",
    "            reranker,\n",
    "            valid_men_vecs,\n",
    "            \"context\",\n",
    "            n_gpu=n_gpu,\n",
    "            force_exact_search=force_exact_search,\n",
    "            batch_size=embed_batch_size,\n",
    "            probe_mult_factor=probe_mult_factor,\n",
    "        )\n",
    "\n",
    "\n",
    "    '2) Performs k-nearest neighbors (k-NN) search to establish relationships between mentions and entities.'\n",
    "    logger.info(\"Eval: Starting KNN search...\") # An informational message is logged to indicate that the k-NN search is starting.\n",
    "    # Fetch recall_k (default 16) knn entities for all mentions\n",
    "    # Fetch (k+1) NN mention candidates; fetching all mentions for within_doc to filter down later\n",
    "    n_men_to_fetch = len(men_embeds) if within_doc else max_knn + 1 # Number of mentions to fetch\n",
    "    if not use_types: # Only one index so only need one search\n",
    "        nn_ent_dists, nn_ent_idxs = dict_index.search(men_embeds, 1) #DD4/DD5 #return the distance and the indice of the closest entity for all mentions in men_embeds\n",
    "        nn_men_dists, nn_men_idxs = men_index.search(men_embeds, n_men_to_fetch) # return the distances and the indices of the k closest mentions for all mentions in men_embeds\n",
    "    else: #C Several indexes corresponding to the different entities in entity_data so we can use the specific search index\n",
    "        # DD6\n",
    "        # DD7\n",
    "        nn_ent_idxs = -1 * np.ones((len(men_embeds), 1), dtype=int) # Indice of the closest entity for all mentions in men_embeds\n",
    "        nn_ent_dists = -1 * np.ones((len(men_embeds), 1), dtype=\"float64\") # Distance of the closest entity for all mentions in men_embeds\n",
    "        nn_men_idxs = -1 * np.ones((len(men_embeds), n_men_to_fetch), dtype=int) # Indice of k closest mentions for all mentions in men_embeds\n",
    "        nn_men_dists = -1 * np.ones((len(men_embeds), n_men_to_fetch), dtype=\"float64\") # Distance of the k closest mentions for all mentions in men_embeds\n",
    "        for entity_type in men_indexes:\n",
    "            #CC3 Creates a new list only containing the mentions for which type = entity_types\n",
    "            men_embeds_by_type = men_embeds[men_idxs_by_type[entity_type]] # Only want to search the mentions that belongs to a specific type of entity.\n",
    "            # Returns the distance and the indice of the closest entity for all mentions in men_embeds by entity type\n",
    "            nn_ent_dists_by_type, nn_ent_idxs_by_type = dict_indexes[entity_type].search(men_embeds_by_type, 1) \n",
    "            nn_ent_idxs_by_type = np.array( #CC4 DD8\n",
    "                list( #DD9\n",
    "                    map( # lambda x : acts as a function\n",
    "                        lambda x: dict_idxs_by_type[entity_type][x], nn_ent_idxs_by_type\n",
    "                    ) # nn_ent_idxs_by_type is the iterable being processed by the map function\n",
    "                    # Each element within nn_ent_idxs_by_type is passed to the lambda function as x.\n",
    "                ) # map alone would return an object, that's why need a list\n",
    "            )\n",
    "            # Returns the distance and the indice of the k closest mentions for all mention in men_embeds by entity type\n",
    "            # Note that here we may not necessarily have k mentions in each entity type which is why we use min(k,len(men_embeds_by_type))\n",
    "            nn_men_dists_by_type, nn_men_idxs_by_type = men_indexes[entity_type].search(\n",
    "                men_embeds_by_type, min(n_men_to_fetch, len(men_embeds_by_type))\n",
    "            )\n",
    "            nn_men_idxs_by_type = np.array(\n",
    "                list(\n",
    "                    map(lambda x: men_idxs_by_type[entity_type][x], nn_men_idxs_by_type)\n",
    "                )\n",
    "            )\n",
    "            for i, idx in enumerate(men_idxs_by_type[entity_type]): #CC5\n",
    "                nn_ent_idxs[idx] = nn_ent_idxs_by_type[i]\n",
    "                nn_ent_dists[idx] = nn_ent_dists_by_type[i]\n",
    "                nn_men_idxs[idx][: len(nn_men_idxs_by_type[i])] = nn_men_idxs_by_type[i]\n",
    "                nn_men_dists[idx][: len(nn_men_dists_by_type[i])] = nn_men_dists_by_type[i]\n",
    "    logger.info(\"Eval: Search finished\") # An informational message is logged to indicate that the k-NN search is finished\n",
    "\n",
    "    '3) Constructs graphs based on these relationships.'\n",
    "    '''\n",
    "    nn_ent_dists contain information about distance of the closest entity\n",
    "    nn_ent_idxs contain information about indice of the closest entity\n",
    "    nn_men_dists contain information about distance of the k nearest mentions\n",
    "    nn_men_idxs contain information about indice of the k nearest mentions\n",
    "    - We can fill in the \"rows\" part (=start nodes) of the graph in the order of the mentions\n",
    "    - We can fill in the \"cols\" part (=end nodes) of the graph with nn_ent_idxs and nn_men_idxs\n",
    "    - We can fill in the \"data\" part (=weights) of the graph with nn_ent_dists and nn_men_dists\n",
    "    '''\n",
    "    logger.info(\"Eval: Building graphs\")\n",
    "    for men_query_idx, men_embed in enumerate(\n",
    "        tqdm(men_embeds, total=len(men_embeds), desc=\"Eval: Building graphs\")\n",
    "    ):\n",
    "        # Get nearest entity candidate\n",
    "        dict_cand_idx = nn_ent_idxs[men_query_idx][0] # Use of [0] to retrieve a scalar and not an 1D array\n",
    "        dict_cand_score = nn_ent_dists[men_query_idx][0]\n",
    "\n",
    "        # Filter candidates to remove -1s, mention query, within doc (if reqd.), and keep only the top k candidates\n",
    "        filter_mask_neg1 = nn_men_idxs[men_query_idx] != -1 # bool ndarray. Ex : np.array([True, False, True, False])\n",
    "        men_cand_idxs = nn_men_idxs[men_query_idx][filter_mask_neg1] # Only keep the elements != -1\n",
    "        men_cand_scores = nn_men_dists[men_query_idx][filter_mask_neg1]\n",
    "\n",
    "        if within_doc:\n",
    "            men_cand_idxs, wd_mask = filter_by_context_doc_id(\n",
    "                men_cand_idxs,\n",
    "                context_doc_ids[men_query_idx],\n",
    "                context_doc_ids,\n",
    "                return_numpy=True,\n",
    "            )\n",
    "            men_cand_scores = men_cand_scores[wd_mask]\n",
    "        \n",
    "        # Filter self-reference + limits the number of candidate to 'max_knn'\n",
    "        filter_mask = men_cand_idxs != men_query_idx\n",
    "        men_cand_idxs, men_cand_scores = (\n",
    "            men_cand_idxs[filter_mask][:max_knn],\n",
    "            men_cand_scores[filter_mask][:max_knn],\n",
    "        )\n",
    "\n",
    "        # Add edges to the graphs\n",
    "        for k in joint_graphs:\n",
    "            joint_graph = joint_graphs[k] # There is no \"s\" in \"joint_graph\", it's not the same ! \n",
    "            # Add mention-entity edge\n",
    "            joint_graph[\"rows\"] = np.append( # Mentions are offset by the total number of entities to differentiate mention nodes from entity nodes\n",
    "                joint_graph[\"rows\"], [n_entities + men_query_idx]\n",
    "            )  \n",
    "            joint_graph[\"cols\"] = np.append(joint_graph[\"cols\"], dict_cand_idx)\n",
    "            joint_graph[\"data\"] = np.append(joint_graph[\"data\"], dict_cand_score)\n",
    "            if k > 0:\n",
    "                # Add mention-mention edges\n",
    "                joint_graph[\"rows\"] = np.append(\n",
    "                    joint_graph[\"rows\"],\n",
    "                    [n_entities + men_query_idx] * len(men_cand_idxs[:k]), # creates an array where the starting node (current mention) is repeated len(men_cand_idxs[:k]) times\n",
    "                ) \n",
    "                joint_graph[\"cols\"] = np.append(\n",
    "                    joint_graph[\"cols\"], n_entities + men_cand_idxs[:k]\n",
    "                )\n",
    "                joint_graph[\"data\"] = np.append(\n",
    "                    joint_graph[\"data\"], men_cand_scores[:k]\n",
    "                )\n",
    "    \n",
    "    \"4) Evaluates the model's accuracy by analyzing how effectively the model can link mentions to the correct entities.\"    \n",
    "    \n",
    "    best_result = {'accuracy': 0}\n",
    "    \n",
    "    dict_acc = {}\n",
    "    max_eval_acc = -1.\n",
    "    for k in joint_graphs:\n",
    "        logger.info(f\"\\nEval: Graph (k={k}):\")\n",
    "        # Partition graph based on cluster-linking constraints (inference procedure)\n",
    "        partitioned_graph, clusters = eval_cluster_linking.partition_graph(\n",
    "            joint_graphs[k], n_entities, directed=True, return_clusters=True)\n",
    "        # Infer predictions from clusters\n",
    "        result = eval_cluster_linking.analyzeClusters(clusters, entity_data, query_data, k)\n",
    "        best_result = result if result['accuracy'] >= best_result['accuracy'] else best_result\n",
    "        acc = float(result['accuracy'].split(' ')[0])\n",
    "        dict_acc[f'k{k}'] = acc\n",
    "        max_eval_acc = max(acc, max_eval_acc)\n",
    "        logger.info(f\"Eval: accuracy for graph@k={k}: {acc}%\")\n",
    "    logger.info(f\"Eval: Best accuracy: {max_eval_acc}%\")\n",
    "    return max_eval_acc, dict_acc, {'dict_embeds': dict_embeds, 'dict_indexes': dict_indexes, 'dict_idxs_by_type': dict_idxs_by_type} if use_types else {'dict_embeds': dict_embeds, 'dict_index': dict_index}\n",
    "\n",
    "def read_data(split, params, logger):\n",
    "    '''\n",
    "    Description \n",
    "    -----------\n",
    "    Loads dataset samples from a specified path\n",
    "    Optionally filters out samples without labels\n",
    "    Checks if the dataset supports multiple labels per sample\n",
    "    \"has_mult_labels\" : bool\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    split : str\n",
    "        Indicates the portion of the dataset to load (\"train\", \"test\", \"valid\"), used by utils.read_dataset to determine which data to read.\n",
    "    params : dict(str)\n",
    "        Contains configuration options\n",
    "    logger : \n",
    "        An object used for logging messages about the process, such as the number of samples read.\n",
    "    '''\n",
    "    samples = utils.read_dataset(split, params[\"data_path\"]) #DD21\n",
    "    # Check if dataset has multiple ground-truth labels\n",
    "    has_mult_labels = \"labels\" in samples[0].keys()\n",
    "    if params[\"filter_unlabeled\"]:\n",
    "        # Filter samples without gold entities\n",
    "        samples = list(\n",
    "            filter(lambda sample: (len(sample[\"labels\"]) > 0) if has_mult_labels else (sample[\"label\"] is not None),\n",
    "                   samples))\n",
    "    logger.info(\"Read %d train samples.\" % len(samples))\n",
    "    return samples, has_mult_labels\n",
    "\n",
    "# Utility function\n",
    "def filter_by_context_doc_id(mention_idxs, doc_id, doc_id_list, return_numpy=False):\n",
    "    '''\n",
    "    Description \n",
    "    -----------\n",
    "    Filters and returns mention indices that belong to a specific document identified by \"doc_id\".\n",
    "    Ensures that the analysis are constrained within the context of that particular document.\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    - mention_idxs : ndarray(int) of dim = (number of mentions)\n",
    "    Represents the indices of mentions\n",
    "    - doc_id : int \n",
    "    Indice of the target document\n",
    "    - doc_id_list : ndarray(int) of dim = (number of mentions)\n",
    "    Array of integers, where each element is a document ID associated with the corresponding mention in mention_idxs. \n",
    "    The length of doc_id_list should match the total number of mentions referenced in mention_idxs.\n",
    "    - return_numpy : bool\n",
    "    A flag indicating whether to return the filtered list of mention indices as a NumPy array. \n",
    "    If True, the function returns a NumPy array; otherwise, it returns a list\n",
    "    -------\n",
    "    Outputs: \n",
    "    - mask : ndarray(bool) of dim = (number of mentions)\n",
    "    Mask indicating where each mention's document ID (from doc_id_list) matches the target doc_id\n",
    "    - mention_idxs : \n",
    "    Only contains mention indices that belong to the target document (=doc_id).\n",
    "    '''\n",
    "    mask = [doc_id_list[i] == doc_id for i in mention_idxs]\n",
    "    if isinstance(mention_idxs, list): # Test if mention_idxs = list. Return a bool\n",
    "        mention_idxs = np.array(mention_idxs) \n",
    "    mention_idxs = mention_idxs[mask] # possible only if mention_idxs is an array, not a list\n",
    "    if not return_numpy:\n",
    "        mention_idxs = list(mention_idxs)\n",
    "    return mention_idxs, mask\n",
    "\n",
    "\n",
    "\n",
    "def loss_function(reranker, \n",
    "    params, \n",
    "    forward_output, \n",
    "    data_module, \n",
    "    n_entities, \n",
    "    knn_dict, \n",
    "    batch_context_inputs, \n",
    "    accumulate_grad_batches\n",
    "    ):\n",
    "    '''\n",
    "    Compute the loss function during the training.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    - reranker : BiEncoderRanker\n",
    "    NN-based ranking model\n",
    "    - params : dict\n",
    "    Contains most of the relevant keys for training (embed_batch_size, train_batch_size, n_gpu, force_exact_search etc...)\n",
    "    - forward_output : dict\n",
    "    Output of the forward() method\n",
    "    - data_module : Instance of ArboelDataModule class\n",
    "    - n_entities : int\n",
    "    Total number of entities\n",
    "    - knn_dict : int (self.knn_dict = self.hparams[\"knn\"]//2)\n",
    "    number of negative entities to fetch. It divides the k-nn evenly between entities and mentions \n",
    "    - accumulate_grad_batches : int\n",
    "    Number of steps to accumulate gradients\n",
    "    '''\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss_dual_negs = loss_ent_negs = 0\n",
    "    # loss of a batch includes both negative mention and entity inputs (alongside positive examples ofc)\n",
    "    loss_dual_negs, _ = reranker(forward_output['context_inputs'], label_input=forward_output['label_inputs'], mst_data={\n",
    "        'positive_embeds': forward_output['positive_embeds'],\n",
    "        'negative_dict_inputs': forward_output['negative_dict_inputs'],\n",
    "        'negative_men_inputs': forward_output['negative_men_inputs']\n",
    "    }, pos_neg_loss=params[\"pos_neg_loss\"]) #A27\n",
    "    skipped_context_inputs = []\n",
    "    if forward_output['skipped'] > 0 and not params[\"within_doc_skip_strategy\"]: #A28\n",
    "        skipped_negative_dict_inputs = torch.tensor(\n",
    "            list(map(lambda x: data_module.entity_dict_vecs[x].numpy(), skipped_negative_dict_inputs)))\n",
    "        skipped_positive_embeds = []\n",
    "        for pos_idx in forward_output['skipped_positive_idxs']:\n",
    "            if pos_idx < n_entities:\n",
    "                pos_embed = reranker.encode_candidate(data_module.entity_dict_vecs[pos_idx:pos_idx + 1],\n",
    "                                                        requires_grad=True)\n",
    "            else:\n",
    "                pos_embed = reranker.encode_context(\n",
    "                    data_module.train_men_vecs[pos_idx - n_entities:pos_idx - n_entities + 1], requires_grad=True)\n",
    "            skipped_positive_embeds.append(pos_embed)\n",
    "        skipped_positive_embeds = torch.cat(skipped_positive_embeds)\n",
    "        skipped_context_inputs = batch_context_inputs[~np.array(forward_output['context_inputs_mask'])]\n",
    "        skipped_context_inputs = skipped_context_inputs\n",
    "        skipped_label_inputs = torch.tensor([[1] + [0] * (knn_dict)] * len(skipped_context_inputs),\n",
    "                                    dtype=torch.float32)\n",
    "        #DD18 loss of a batch that only includes negative entity inputs.\n",
    "        loss_ent_negs, _ = reranker(skipped_context_inputs, label_input=skipped_label_inputs, mst_data={\n",
    "            'positive_embeds': skipped_positive_embeds,\n",
    "            'negative_dict_inputs': skipped_negative_dict_inputs,\n",
    "            'negative_men_inputs': None\n",
    "        }, pos_neg_loss=params[\"pos_neg_loss\"])\n",
    "            \n",
    "    # len(context_input) = Number of mentions in the batch that successfully found negative entities and mentions.\n",
    "    # len(skipped_context_inputs): Number of mentions in the batch that only found negative entities.\n",
    "    loss = ((loss_dual_negs * len(forward_output['context_inputs']) + loss_ent_negs * len(skipped_context_inputs)) / (len(forward_output['context_inputs']) + len(skipped_context_inputs))) / accumulate_grad_batches\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"Data module\"\n",
    "class ArboelDataModule(L.LightningDataModule):\n",
    "    '''\n",
    "    Attributes\n",
    "    ----------\n",
    "    \n",
    "    - entity_dictionary : list of dict\n",
    "    Stores the initial and raw entity dictionary\n",
    "    - train_tensor_data : TensorDataset(context_vecs, label_idxs, n_labels, mention_idx) with :\n",
    "        - “context_vecs” : tensor containing IDs of (mention + surrounding context) tokens \n",
    "        - “label_idxs” : tensor with indices pointing to the entities in the entity dictionary that are considered correct labels for the mention.\n",
    "        - “n_labels” : Number of labels (=entities) associated with the mention\n",
    "        - “mention_idx” : tensor containing a sequence of integers from 0 to N-1 (N = number of mentions in the dataset) serving as a unique identifier for each mention.\n",
    "    - train_processed_data : list of dict\n",
    "    Contains information about mentions (mention_id, mention_name, context, etc…)\n",
    "    - valid_tensor_data : TensorDataset\n",
    "    Same as \"train_tensor_dataset\" but for validation set\n",
    "    - max_gold_cluster_len : int\n",
    "    Maximum length of clusters inside gold_cluster\n",
    "    - train_context_doc_ids : list\n",
    "    # Store the context_doc_id (=context document indice) for every mention in the train set\n",
    "    '''\n",
    "    def __init__(self, params, dataset, ontology):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(params)\n",
    "        \n",
    "        # # First try to load the tokens from a local file. If local file not found, uses a pre-trained model specified by params[\"bert_model\"]\n",
    "        # vocab_path = os.path.join(self.hparams.params[\"bert_model\"], 'vocab.txt') #DD3\n",
    "        # if os.path.isfile(vocab_path): \n",
    "        #     print(f\"Found tokenizer vocabulary at {vocab_path}\")\n",
    "        # self.tokenizer = BertTokenizer.from_pretrained(\n",
    "        #     vocab_path if os.path.isfile(vocab_path) else self.hparams.params[\"bert_model\"], do_lower_case=self.hparams.params[\"lowercase\"]\n",
    "        # )\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.ontology = ontology\n",
    "        \n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "            \n",
    "        self.batch_size = self.hparams.params['batch_size']\n",
    "        \n",
    "        self.train_processed_data = None\n",
    "        self.valid_processed_data = None\n",
    "        self.test_processed_data = None\n",
    "        self.train_tensor_data = None\n",
    "        self.valid_tensor_data = None\n",
    "        self.test_tensor_data = None\n",
    "        self.entity_dict_vecs = None\n",
    "\n",
    "\n",
    "    def prepare_data(self):\n",
    "        'Use this to download and prepare data.'\n",
    "        \n",
    "        # prepare the entity data : dictionary.pickle\n",
    "        process_ontology(self.ontology, self.dataset)\n",
    "        \n",
    "        # prepare the mentions data :  train.jsonl, valid.jsonl, test.jsonl\n",
    "        process_mention_dataset(self.ontology, self.dataset)\n",
    "        \n",
    "        # Path to pickle files\n",
    "        pickle_src_path = self.hparams.params[\"pickle_src_path\"]\n",
    "        \n",
    "        'entity dictionary'\n",
    "        # if entity dictionary already tokenized, load it\n",
    "        pickle_src_path = self.hparams.params[\"pickle_src_path\"]\n",
    "        entity_dictionary_pkl_path = os.path.join(pickle_src_path, 'entity_dictionary.pickle')\n",
    "        self.entity_dictionary_loaded = False\n",
    "        if os.path.isfile(entity_dictionary_pkl_path): \n",
    "            print(\"Loading stored processed entity dictionary...\")\n",
    "            with open(entity_dictionary_pkl_path, 'rb') as read_handle:\n",
    "                self.entity_dictionary = pickle.load(read_handle) # DD12B\n",
    "            self.entity_dictionary_loaded = True\n",
    "        \n",
    "        else : # else load the not processed one\n",
    "            with open(os.path.join(self.hparams.params[\"data_path\"], 'dictionary.pickle'), 'rb') as read_handle: #A11\n",
    "                self.entity_dictionary = pickle.load(read_handle)\n",
    "        \n",
    "        'training mention data'\n",
    "        # path to a file where the training data, already processed into tensors is saved\n",
    "        self.train_tensor_data_pkl_path = os.path.join(pickle_src_path, 'train_tensor_data.pickle')\n",
    "        # path to a file where metadata / additional information about the training data is stored\n",
    "        self.train_processed_data_pkl_path = os.path.join(pickle_src_path, 'train_processed_data.pickle')\n",
    "        \n",
    "        # if the full path to file exist, load the file\n",
    "        if os.path.isfile(self.train_tensor_data_pkl_path) and os.path.isfile(self.train_processed_data_pkl_path):\n",
    "            print(\"Loading stored processed train data...\")\n",
    "            with open(self.train_tensor_data_pkl_path, 'rb') as read_handle:\n",
    "                self.train_tensor_data = pickle.load(read_handle)\n",
    "            with open(self.train_processed_data_pkl_path, 'rb') as read_handle:\n",
    "                self.train_processed_data = pickle.load(read_handle)\n",
    "                \n",
    "                \n",
    "        'validation mention data'\n",
    "        self.valid_tensor_data_pkl_path = os.path.join(pickle_src_path, 'valid_tensor_data.pickle')\n",
    "        self.valid_processed_data_pkl_path = os.path.join(pickle_src_path, 'valid_processed_data.pickle')\n",
    "        \n",
    "        # Same as training data : \n",
    "        # if the full path to file exist, load the file\n",
    "        if os.path.isfile(self.valid_tensor_data_pkl_path) and os.path.isfile(self.valid_processed_data_pkl_path):\n",
    "            print(\"Loading stored processed valid data...\")\n",
    "            with open(self.valid_tensor_data_pkl_path, 'rb') as read_handle:\n",
    "                self.valid_tensor_data = pickle.load(read_handle)\n",
    "            with open(self.valid_processed_data_pkl_path, 'rb') as read_handle:\n",
    "                self.valid_processed_data = pickle.load(read_handle)\n",
    "                \n",
    "        'test mention data'\n",
    "        self.test_tensor_data_pkl_path = os.path.join(pickle_src_path, 'test_tensor_data.pickle')\n",
    "        self.test_processed_data_pkl_path = os.path.join(pickle_src_path, 'test_processed_data.pickle')\n",
    "        \n",
    "        # Same as training data : \n",
    "        # if the full path to file exist, load the file\n",
    "        if os.path.isfile(self.test_tensor_data_pkl_path) and os.path.isfile(self.test_processed_data_pkl_path):\n",
    "            print(\"Loading stored processed valid data...\")\n",
    "            with open(self.test_tensor_data_pkl_path, 'rb') as read_handle: #CC7 'rb' = binary read mode\n",
    "                self.test_tensor_data = pickle.load(read_handle)\n",
    "            with open(self.test_processed_data_pkl_path, 'rb') as read_handle:\n",
    "                self.test_processed_data = pickle.load(read_handle)\n",
    "\n",
    "\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        '''\n",
    "        For processing and splitting. Called at the beginning of fit (train + validate), validate, test, or predict.\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        'Entity dict : drop entity for discovery'\n",
    "        # For discovery experiment: Drop entities used in training that were dropped randomly from dev/test set\n",
    "        if self.hparams.params[\"drop_entities\"]: #A12\n",
    "            assert self.entity_dictionary \n",
    "            drop_set_path = self.hparams.params[\"drop_set\"] if self.hparams.params[\"drop_set\"] is not None else os.path.join(self.hparams.params[\"pickle_src_path\"], 'drop_set_mention_data.pickle') #A12\n",
    "            if not os.path.isfile(drop_set_path):\n",
    "                raise ValueError(\"Invalid or no --drop_set path provided to dev/test mention data\")\n",
    "            with open(drop_set_path, 'rb') as read_handle:\n",
    "                drop_set_data = pickle.load(read_handle)\n",
    "            # gold cuis indices for each mention in drop_set_data\n",
    "            drop_set_mention_gold_cui_idxs = list(map(lambda x: x['label_idxs'][0], drop_set_data))\n",
    "            # Make the set unique\n",
    "            ents_in_data = np.unique(drop_set_mention_gold_cui_idxs)\n",
    "            # % of drop\n",
    "            ent_drop_prop = 0.1\n",
    "            logger.info(f\"Dropping {ent_drop_prop*100}% of {len(ents_in_data)} entities found in drop set\")\n",
    "            # Number of entity indices to drop\n",
    "            n_ents_dropped = int(ent_drop_prop*len(ents_in_data))\n",
    "            # Random selection drop\n",
    "            rng = np.random.default_rng(seed=17)\n",
    "            # Indices of all entities that are dropped\n",
    "            dropped_ent_idxs = rng.choice(ents_in_data, size=n_ents_dropped, replace=False)\n",
    "\n",
    "            # Drop entities from dictionary (subsequent processing will automatically drop corresponding mentions)\n",
    "            keep_mask = np.ones(len(self.entity_dictionary), dtype='bool')\n",
    "            keep_mask[dropped_ent_idxs] = False\n",
    "            self.entity_dictionary = np.array(self.entity_dictionary)[keep_mask]\n",
    "        \n",
    "        \n",
    "        'Train mention data'\n",
    "        # train_samples = list of dict. Each dict contains information about a mention (id, name, context, etc…). \n",
    "        # Each key can have a dictionary itself. Ex : mention[\"context\"][\"tokens\"] or mention[\"context\"][\"ids\"]\n",
    "        train_samples, train_mult_labels = read_data(\"train\", self.hparams.params, logger)\n",
    "        \n",
    "        if not os.path.isfile(self.train_tensor_data_pkl_path) : # Load and Process train data if not done yet\n",
    "            # train_processed_data = (mention + surrounding context) tokens\n",
    "            # entity_dictionary = tokenized entities\n",
    "            # tensor_train_dataset = Dataset containing several tensors (IDs of mention + context / indices of correct entities etc..) # Go check \"process_mention_data\" for more info\n",
    "            self.train_processed_data, self.entity_dictionary, self.train_tensor_data = data_process.process_mention_data(\n",
    "                train_samples,\n",
    "                self.entity_dictionary,\n",
    "                self.tokenizer,\n",
    "                self.hparams.params[\"max_context_length\"],\n",
    "                self.hparams.params[\"max_cand_length\"],\n",
    "                context_key=self.hparams.params[\"context_key\"],\n",
    "                multi_label_key=\"labels\" if train_mult_labels else None,\n",
    "                # silent=self.hparams.params[\"silent\"], \n",
    "                logger=logger,\n",
    "                debug=self.hparams.params[\"debug\"], \n",
    "                knn=self.hparams.params['knn'],\n",
    "                dictionary_processed=self.entity_dictionary_loaded\n",
    "            )\n",
    "            \n",
    "            print(\"Saving processed train data...\")\n",
    "            with open(self.train_tensor_data_pkl_path, 'wb') as write_handle:\n",
    "                pickle.dump(self.train_tensor_data, write_handle,\n",
    "                            protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            with open(self.train_processed_data_pkl_path, 'wb') as write_handle:\n",
    "                pickle.dump(self.train_processed_data, write_handle,\n",
    "                            protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        # Prepare tensor containing only ID of (mention + surrounding context) tokens of training set'\n",
    "        self.train_men_vecs = self.train_tensor_data[:][0] \n",
    "\n",
    "        # Store the IDs of the entity in entity_dictionary # It's the equivalent of train_men_vecs for entities\n",
    "        # (Done here because data_process.process_mention_data will tokenize the entities in entity_dict)\n",
    "        self.entity_dict_vecs = torch.tensor(list(map(lambda x: x['ids'], self.entity_dictionary)), dtype=torch.long)\n",
    "\n",
    "\n",
    "        'Validation mention data'\n",
    "        if not os.path.isfile(self.valid_tensor_data_pkl_path) : \n",
    "            # Load and Process validation data if not done yet\n",
    "            valid_samples, valid_mult_labels = read_data(\"valid\", self.hparams.params, logger)\n",
    "            self.valid_processed_data, _, self.valid_tensor_data = data_process.process_mention_data(\n",
    "                valid_samples,\n",
    "                self.entity_dictionary,\n",
    "                self.tokenizer,\n",
    "                self.hparams.params[\"max_context_length\"],\n",
    "                self.hparams.params[\"max_cand_length\"],\n",
    "                context_key=self.hparams.params[\"context_key\"],\n",
    "                multi_label_key=\"labels\" if valid_mult_labels else None,\n",
    "                # silent=self.hparams.params[\"silent\"],\n",
    "                logger=logger,\n",
    "                debug=self.hparams.params[\"debug\"],\n",
    "                knn=self.hparams.params[\"knn\"],\n",
    "                dictionary_processed=self.entity_dictionary_loaded\n",
    "            )\n",
    "            \n",
    "            print(\"Saving processed valid data...\")\n",
    "            with open(self.valid_tensor_data_pkl_path, 'wb') as write_handle:\n",
    "                pickle.dump(self.valid_tensor_data, write_handle,\n",
    "                            protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            with open(self.valid_processed_data_pkl_path, 'wb') as write_handle:\n",
    "                pickle.dump(self.valid_processed_data, write_handle,\n",
    "                            protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        # Prepare tensor containing only ID of (mention + surrounding context) tokens of validation data'\n",
    "        self.valid_men_vecs = self.valid_tensor_data[:][0]\n",
    "        \n",
    "        \n",
    "        'Test mention data'\n",
    "        if not os.path.isfile(self.train_tensor_data_pkl_path) :\n",
    "            # Load and Process test data if not done yet\n",
    "            test_samples, test_mult_labels = read_data(\"test\", self.hparams.params, logger)\n",
    "            self.test_processed_data, _, self.test_tensor_data = data_process.process_mention_data(\n",
    "                test_samples,\n",
    "                self.entity_dictionary,\n",
    "                self.tokenizer,\n",
    "                self.hparams.params[\"max_context_length\"],\n",
    "                self.hparams.params[\"max_cand_length\"],\n",
    "                context_key=self.hparams.params[\"context_key\"],\n",
    "                multi_label_key=\"labels\" if test_mult_labels else None,\n",
    "                # silent=self.hparams.params[\"silent\"],\n",
    "                logger=logger,\n",
    "                debug=self.hparams.params[\"debug\"],\n",
    "                knn=self.hparams.params[\"knn\"],\n",
    "                dictionary_processed=self.entity_dictionary_loaded\n",
    "            )\n",
    "            \n",
    "            print(\"Saving processed test data...\")\n",
    "            with open(self.test_tensor_data_pkl_path, 'wb') as write_handle:\n",
    "                pickle.dump(self.test_tensor_data, write_handle,\n",
    "                            protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            with open(self.test_processed_data_pkl_path, 'wb') as write_handle:\n",
    "                pickle.dump(self.test_processed_data, write_handle,\n",
    "                            protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        # Prepare tensor containing only ID of (mention + surrounding context) tokens of validation data'\n",
    "        self.test_men_vecs = self.test_tensor_data[:][0]\n",
    "        \n",
    "        \n",
    "        'Within_doc search'\n",
    "        # Consider if it’s within_doc (=search only within the document)'\n",
    "        self.train_context_doc_ids = self.valid_context_doc_ids = None\n",
    "        if self.hparams.params[\"within_doc\"]: \n",
    "            # RR9 : No need of those if conditions\n",
    "            # Store the context_doc_id for every mention in the train and valid sets\n",
    "            # if train_samples is None:\n",
    "            #     train_samples, _ = read_data(\"train\", self.hparams.params, logger)\n",
    "            self.train_context_doc_ids = [s['context_doc_id'] for s in train_samples]\n",
    "            # if valid_samples is None:\n",
    "            #     valid_samples, _ = read_data(\"valid\", self.hparams.params, logger)\n",
    "            self.valid_context_doc_ids = [s['context_doc_id'] for s in valid_samples]\n",
    "            # if test_samples is None:\n",
    "            #     test_samples, _ = read_data(\"valid\", self.hparams.params, logger)\n",
    "            self.test_context_doc_ids = [s['context_doc_id'] for s in test_samples]\n",
    "        \n",
    "        # Get clusters of mentions that map to a gold entity\n",
    "        self.train_gold_clusters = data_process.compute_gold_clusters(self.train_processed_data)\n",
    "        # Maximum length of clusters inside gold_cluster\n",
    "        self.max_gold_cluster_len = 0\n",
    "        for ent in self.train_gold_clusters:\n",
    "            if len(self.train_gold_clusters[ent]) > self.max_gold_cluster_len:\n",
    "                self.max_gold_cluster_len = len(self.train_gold_clusters[ent])\n",
    "\n",
    "\n",
    "    def train_dataloader(self): #RR5\n",
    "        # Return the training DataLoader\n",
    "        # train_sampler = RandomSampler(self.train_tensor_data) if self.params[\"shuffle\"] else SequentialSampler(self.train_tensor_data)\n",
    "        # return DataLoader(self.train_tensor_data, sampler=train_sampler, batch_size=self.batch_size) #DD4\n",
    "        return DataLoader(self.train_tensor_data, batch_size=self.batch_size)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        # Return the validation DataLoader\n",
    "        return DataLoader(self.valid_tensor_data, batch_size=self.batch_size)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        # Return the validation DataLoader\n",
    "        return DataLoader(self.test_tensor_data, batch_size=self.batch_size)\n",
    "\n",
    "\n",
    "'Model Training'\n",
    "class LitArboel(L.LightningModule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        params,  \n",
    "        data_module\n",
    "        ):\n",
    "        '''\n",
    "        - params : dict\n",
    "        Contains most of the relevant keys for training (embed_batch_size, train_batch_size, n_gpu, force_exact_search etc...)\n",
    "        - data_module : Instance of ArboelDataModule class\n",
    "        '''\n",
    "        super(LitArboel, self).__init__()\n",
    "        self.save_hyperparameters(params) #DD1\n",
    "        \n",
    "        self.data_module = data_module\n",
    "        \n",
    "        self.reranker = BiEncoderRanker(params)\n",
    "        # self.tokenizer = self.reranker.tokenizer\n",
    "        self.model = self.reranker.model\n",
    "        \n",
    "        \n",
    "    def forward(self, batch_context_inputs, candidate_idxs, n_gold, mention_idxs):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        -----------\n",
    "        Processes a batch of input data to generate embeddings, and identifies positive and negative examples for training. \n",
    "        It handles the construction of mention-entity graphs, computes nearest neighbors, and organizes the data for subsequent loss calculation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        - “batch_context_inputs” : Tensor\n",
    "            Tensor containing IDs of (mention + surrounding context) tokens. Shape: (batch_size, context_length) \n",
    "        - “candidate_idxs” : Tensor\n",
    "            Tensor with indices pointing to the entities in the entity dictionary that are considered correct labels for the mention. Shape: (batch_size, candidate_count)\n",
    "        - “n_gold” : Tensor\n",
    "            Number of labels (=entities) associated with the mention. Shape: (batch_size,)\n",
    "        - “mention_idx” : Tensor\n",
    "            Tensor containing a sequence of integers from 0 to N-1 (N = number of mentions in the dataset) serbing as a unique identifier for each mention.\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        - label_inputs : Tensor\n",
    "            Tensor of binary labels indicating the correct candidates. Shape: (batch_size, 1 + knn_dict + knn_men), where 1 represents the positive example and the rest are negative examples.\n",
    "        - context_inputs : Tensor\n",
    "            Processed batch context inputs, filtered to remove mentions with no negative examples. Shape: (filtered_batch_size, context_length).\n",
    "        - negative_men_inputs : Tensor\n",
    "            Tensor of negative mention inputs. Shape: (filtered_batch_size * knn_men,)\n",
    "        - negative_dict_inputs : Tensor\n",
    "            Tensor of negative dictionary (entity) inputs. Shape: (filtered_batch_size * knn_dict,)\n",
    "        - positive_embeds : Tensor\n",
    "            Tensor of embeddings for the positive examples. Shape: (filtered_batch_size, embedding_dim)\n",
    "        - skipped : int \n",
    "            The number of mentions skipped due to lack of valid negative examples.\n",
    "        - skipped_positive_idxs : list(int)\n",
    "            List of indices for positive examples that were skipped.\n",
    "        - skipped_negative_dict_inputs :\n",
    "            Tensor of negative dictionary inputs for skipped examples. Shape may vary based on the number of skipped examples and available negative dictionary entries.\n",
    "        - context_inputs_mask : list(bool)\n",
    "            Mask indicating which entries in batch_context_inputs were retained after filtering out mentions with no negative examples.\n",
    "        \"\"\"\n",
    "        \n",
    "        # mentions within the batch\n",
    "        mention_embeddings = self.train_men_embeddings[mention_idxs.cpu()]\n",
    "        if len(mention_embeddings.shape) == 1:\n",
    "            mention_embeddings = np.expand_dims(mention_embeddings, axis=0)\n",
    "\n",
    "        positive_idxs = []\n",
    "        negative_dict_inputs = []\n",
    "        negative_men_inputs = []\n",
    "\n",
    "        skipped_positive_idxs = []\n",
    "        skipped_negative_dict_inputs = []\n",
    "\n",
    "        min_neg_mens = float('inf')\n",
    "        skipped = 0\n",
    "        context_inputs_mask = [True]*len(batch_context_inputs)\n",
    "        \n",
    "        'IV.4.B) For each mention within the batch'\n",
    "        # For each mention within the batch\n",
    "        for m_embed_idx, m_embed in enumerate(mention_embeddings):\n",
    "            mention_idx = int(mention_idxs[m_embed_idx])\n",
    "            #CC11 ground truth entities of the mention \"mention_idx\"\n",
    "            gold_idxs = set(self.data_module.train_processed_data[mention_idx]['label_idxs'][:n_gold[m_embed_idx]])\n",
    "            \n",
    "            # TEMPORARY: Assuming that there is only 1 gold label, TODO: Incorporate multiple case\n",
    "            assert n_gold[m_embed_idx] == 1\n",
    "\n",
    "            if mention_idx in self.gold_links:\n",
    "                gold_link_idx = self.gold_links[mention_idx]\n",
    "            else:\n",
    "                'IV.4.B.a) Create the graph with positive edges'\n",
    "                # This block creates all the positive edges of the mention in this iteration\n",
    "                # Run MST on mention clusters of all the gold entities of the current query mention to find its positive edge\n",
    "                rows, cols, data, shape = [], [], [], (self.n_entities+self.n_mentions, self.n_entities+self.n_mentions)\n",
    "                seen = set()\n",
    "\n",
    "                # Set whether the gold edge should be the nearest or the farthest neighbor\n",
    "                sim_order = 1 if self.hparams.params[\"farthest_neighbor\"] else -1 #A26\n",
    "\n",
    "                for cluster_ent in gold_idxs:\n",
    "                    #CC12 IDs of all the mentions inside the gold cluster with entity id = \"cluster_ent\"\n",
    "                    cluster_mens = self.hparams.train_gold_clusters[cluster_ent]\n",
    "\n",
    "                    if self.hparams.param[\"within_doc\"]:\n",
    "                        # Filter the gold cluster to within-doc\n",
    "                        cluster_mens, _ = filter_by_context_doc_id(cluster_mens,\n",
    "                                                                    self.data_module.train_context_doc_ids[mention_idx],\n",
    "                                                                    self.data_module.train_context_doc_ids)\n",
    "                    \n",
    "                    # weights for all the mention-entity links inside the cluster of the current mention\n",
    "                    to_ent_data = self.train_men_embeddings[cluster_mens] @ self.train_dict_embeddings[cluster_ent].T\n",
    "\n",
    "                    # weights for all the mention-mention links inside the cluster of the current mention\n",
    "                    to_men_data = self.train_men_embeddings[cluster_mens] @ self.train_men_embeddings[cluster_mens].T\n",
    "\n",
    "                    if self.hparams.params['gold_arbo_knn'] is not None:\n",
    "                        # Descending order of similarity if nearest-neighbor, else ascending order\n",
    "                        sorti = np.argsort(sim_order * to_men_data, axis=1)\n",
    "                        sortv = np.take_along_axis(to_men_data, sorti, axis=1)\n",
    "                        if self.hparams.params[\"rand_gold_arbo\"]:\n",
    "                            randperm = np.random.permutation(sorti.shape[1])\n",
    "                            sortv, sorti = sortv[:, randperm], sorti[:, randperm]\n",
    "\n",
    "                    for i in range(len(cluster_mens)):\n",
    "                        from_node = self.n_entities + cluster_mens[i]\n",
    "                        to_node = cluster_ent\n",
    "                        # Add mention-entity link\n",
    "                        rows.append(from_node)\n",
    "                        cols.append(to_node)\n",
    "                        data.append(-1 * to_ent_data[i])\n",
    "                        if self.hparams.params['gold_arbo_knn'] is None:\n",
    "                            # Add forward and reverse mention-mention links over the entire MST\n",
    "                            for j in range(i+1, len(cluster_mens)):\n",
    "                                to_node = self.n_entities + cluster_mens[j]\n",
    "                                if (from_node, to_node) not in seen:\n",
    "                                    score = to_men_data[i,j]\n",
    "                                    rows.append(from_node)\n",
    "                                    cols.append(to_node)\n",
    "                                    data.append(-1 * score) # Negatives needed for SciPy's Minimum Spanning Tree computation\n",
    "                                    seen.add((from_node, to_node))\n",
    "                                    seen.add((to_node, from_node))\n",
    "                        else:\n",
    "                            # Approximate the MST using <gold_arbo_knn> nearest mentions from the gold cluster\n",
    "                            added = 0\n",
    "                            approx_k = min(self.hparams.params['gold_arbo_knn']+1, len(cluster_mens))\n",
    "                            for j in range(approx_k):\n",
    "                                if added == approx_k - 1:\n",
    "                                    break\n",
    "                                to_node = self.n_entities + cluster_mens[sorti[i, j]]\n",
    "                                if to_node == from_node:\n",
    "                                    continue\n",
    "                                added += 1\n",
    "                                if (from_node, to_node) not in seen:\n",
    "                                    score = sortv[i, j]\n",
    "                                    rows.append(from_node)\n",
    "                                    cols.append(to_node)\n",
    "                                    data.append(\n",
    "                                        -1 * score)  # Negatives needed for SciPy's Minimum Spanning Tree computation\n",
    "                                    seen.add((from_node, to_node))\n",
    "\n",
    "                'IV.4.B.b) Fine tuning with inference procedure to get a mst'\n",
    "                # Creates MST with entity constraint (inference procedure)\n",
    "                csr = csr_matrix((-sim_order * np.array(data), (rows, cols)), shape=shape)\n",
    "                # Note: minimum_spanning_tree expects distances as edge weights\n",
    "                mst = minimum_spanning_tree(csr).tocoo()\n",
    "                # Note: cluster_linking_partition expects similarities as edge weights # Convert directed to undirected graph\n",
    "                rows, cols, data = cluster_linking_partition(np.concatenate((mst.row, mst.col)), # cluster_linking_partition is imported from eval_cluster_linking\n",
    "                                                                np.concatenate((mst.col, mst.row)),\n",
    "                                                                np.concatenate((sim_order * mst.data, sim_order * mst.data)),\n",
    "                                                                self.n_entities,\n",
    "                                                                directed=True,\n",
    "                                                                silent=True)\n",
    "                assert np.array_equal(rows - self.n_entities, cluster_mens)\n",
    "                \n",
    "                for i in range(len(rows)):\n",
    "                    men_idx = rows[i] - self.n_entities\n",
    "                    if men_idx in self.gold_links:\n",
    "                        continue\n",
    "                    assert men_idx >= 0\n",
    "                    add_link = True\n",
    "                    # Store the computed positive edges for the mentions in the clusters only if they have the same gold entities as the query mention\n",
    "                    for l in self.data_module.train_processed_data[men_idx]['label_idxs'][:self.data_module.train_processed_data[men_idx]['n_labels']]:\n",
    "                        if l not in gold_idxs:\n",
    "                            add_link = False\n",
    "                            break\n",
    "                    if add_link:\n",
    "                        self.gold_links[men_idx] = cols[i]\n",
    "                gold_link_idx = self.gold_links[mention_idx]\n",
    "                \n",
    "            'IV.4.B.c) Retrieve the pre-computed nearest neighbors'\n",
    "            knn_dict_idxs = self.dict_nns[mention_idx]\n",
    "            knn_dict_idxs = knn_dict_idxs.astype(np.int64).flatten()\n",
    "            knn_men_idxs = self.men_nns[mention_idx][self.men_nns[mention_idx] != -1]\n",
    "            knn_men_idxs = knn_men_idxs.astype(np.int64).flatten()\n",
    "            if self.hparams.params['within_doc']:\n",
    "                knn_men_idxs, _ = filter_by_context_doc_id(knn_men_idxs,\n",
    "                                                        self.data_module.train_context_doc_ids[mention_idx],\n",
    "                                                        self.data_module.train_context_doc_ids, return_numpy=True)\n",
    "            'IV.4.B.d) Add negative examples'\n",
    "            neg_mens = list(knn_men_idxs[~np.isin(knn_men_idxs, np.concatenate([self.data_module.train_gold_clusters[gi] for gi in gold_idxs]))][:self.knn_men])\n",
    "            # Track queries with no valid mention negatives\n",
    "            if len(neg_mens) == 0:\n",
    "                context_inputs_mask[m_embed_idx] = False\n",
    "                skipped_negative_dict_inputs += list(knn_dict_idxs[~np.isin(knn_dict_idxs, list(gold_idxs))][:self.knn_dict])\n",
    "                skipped_positive_idxs.append(gold_link_idx)\n",
    "                skipped += 1\n",
    "                continue\n",
    "            else:\n",
    "                min_neg_mens = min(min_neg_mens, len(neg_mens))\n",
    "            negative_men_inputs.append(knn_men_idxs[~np.isin(knn_men_idxs, np.concatenate([self.data_module.train_gold_clusters[gi] for gi in gold_idxs]))][:self.knn_men])\n",
    "            negative_dict_inputs += list(knn_dict_idxs[~np.isin(knn_dict_idxs, list(gold_idxs))][:self.knn_dict])\n",
    "            # Add the positive example\n",
    "            positive_idxs.append(gold_link_idx)\n",
    "\n",
    "        \n",
    "        'IV.4.C) Skip this iteration if no suitable negative examples found'\n",
    "        if len(negative_men_inputs) == 0 :\n",
    "            return None #DD8 instead of continue\n",
    "        \n",
    "        # Sets the minimum number of negative mentions found across all processed mentions in the current batch\n",
    "        self.knn_men = min_neg_mens\n",
    "        \n",
    "        # This step ensures that each mention is compared against a uniform number of negative mentions\n",
    "        filtered_negative_men_inputs = []\n",
    "        for row in negative_men_inputs:\n",
    "            filtered_negative_men_inputs += list(row[:self.knn_men])\n",
    "        negative_men_inputs = filtered_negative_men_inputs\n",
    "\n",
    "        # Assertions for Data Integrity\n",
    "        assert len(negative_dict_inputs) == (len(mention_embeddings) - skipped) * self.knn_dict\n",
    "        assert len(negative_men_inputs) == (len(mention_embeddings) - skipped) * self.knn_men\n",
    "\n",
    "        self.total_skipped += skipped\n",
    "        self.total_knn_men_negs += self.knn_men\n",
    "\n",
    "        # Convert to tensors\n",
    "        negative_dict_inputs = torch.tensor(list(map(lambda x: self.data_module.entity_dict_vecs[x].numpy(), negative_dict_inputs)))\n",
    "        negative_men_inputs = torch.tensor(list(map(lambda x: self.data_module.train_men_vecs[x].numpy(), negative_men_inputs)))\n",
    "        \n",
    "        # Labels indicating the correct candidates. Used for computing loss.\n",
    "        positive_embeds = []\n",
    "        for pos_idx in positive_idxs:\n",
    "            if pos_idx < self.n_entities:\n",
    "                pos_embed = self.reranker.encode_candidate(self.data_module.entity_dict_vecs[pos_idx:pos_idx + 1], requires_grad=True)\n",
    "            else:\n",
    "                pos_embed = self.reranker.encode_context(self.data_module.train_men_vecs[pos_idx - self.n_entities:pos_idx - self.n_entities + 1], requires_grad=True)\n",
    "            positive_embeds.append(pos_embed)\n",
    "        positive_embeds = torch.cat(positive_embeds)\n",
    "        \n",
    "        # Remove mentions with no negative examples\n",
    "        context_inputs = batch_context_inputs[context_inputs_mask]\n",
    "        context_inputs = context_inputs\n",
    "        \n",
    "        # Tensor containing binary values that act as indicator variables in the paper:\n",
    "        # Contains Indicator variable such that I_{u,m_i} = 1 if(u,mi) ∈ E'_{m_i} and I{u,m_i} = 0 otherwise.\n",
    "        label_inputs = torch.tensor([[1]+[0]*(self.knn_dict+self.knn_men)]*len(context_inputs), dtype=torch.float32)\n",
    "        \n",
    "        return {'label_inputs':label_inputs, 'context_inputs' : context_inputs, \"negative_men_inputs\" : negative_men_inputs,\n",
    "                'negative_dict_inputs' : negative_dict_inputs, 'positive_embeds' : positive_embeds, 'skipped' : skipped,\n",
    "                'skipped_positive_idxs' : skipped_positive_idxs, 'skipped_negative_dict_inputs' : skipped_negative_dict_inputs,\n",
    "                'context_inputs_mask' : context_inputs_mask\n",
    "                }\n",
    "\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        # batch = tuple(t.to(device) for t in batch) : automated in pytorch #DD5\n",
    "        \n",
    "        # Initialize the parameters\n",
    "        # batch is a subsample from tensor_dataset\n",
    "        batch_context_inputs, candidate_idxs, n_gold, mention_idxs = batch\n",
    "        \n",
    "        f = self.forward(batch_context_inputs, candidate_idxs, n_gold, mention_idxs)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_function(self.reranker, \n",
    "            self.hparams.params, \n",
    "            f, \n",
    "            self.data_module, \n",
    "            self.n_entities, \n",
    "            self.knn_dict, \n",
    "            batch_context_inputs, \n",
    "            self.trainer.accumulate_grad_batches\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def configure_optimizers(self): #DD2\n",
    "        \n",
    "        # Define optimizer\n",
    "        optimizer = get_bert_optimizer(\n",
    "        [self.model],\n",
    "        self.hparams.params[\"type_optimization\"],\n",
    "        self.hparams.params[\"learning_rate\"],\n",
    "        fp16=self.hparams.params.get(\"fp16\"),\n",
    "        )\n",
    "        \n",
    "        # Define scheduler\n",
    "        batch_size = self.hparams.params[\"train_batch_size\"]\n",
    "        epochs = self.trainer.max_epochs\n",
    "\n",
    "        num_train_steps = int(len(self.train_tensor_data) / batch_size / self.trainer.accumulate_grad_batches) * epochs\n",
    "        num_warmup_steps = int(num_train_steps * self.hparams.params[\"warmup_proportion\"])\n",
    "\n",
    "        scheduler = WarmupLinearSchedule(\n",
    "            optimizer, warmup_steps=num_warmup_steps, t_total=num_train_steps,\n",
    "        )\n",
    "        logger.info(\" Num optimization steps = %d\" % num_train_steps)\n",
    "        logger.info(\" Num warmup steps = %d\", num_warmup_steps)\n",
    "        return [optimizer], [{'scheduler': scheduler, 'interval': 'step'}]\n",
    "\n",
    "\n",
    "\n",
    "    def on_train_start(self):\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    \n",
    "    \n",
    "    def on_fit_start(self):\n",
    "        # Compute n_gpu once, at the start of training, and store it as an instance attribute\n",
    "        if self.trainer.devices is None:\n",
    "            self.n_gpu = 0\n",
    "        elif isinstance(self.trainer.devices, list):\n",
    "            self.n_gpu = len(self.trainer.devices)\n",
    "        elif isinstance(self.trainer.devices, int):\n",
    "            self.n_gpu = self.trainer.devices\n",
    "        else:\n",
    "            # For other configurations, such as auto-select or when specific GPUs are selected as a string\n",
    "            # It's safer to rely on the actual allocated devices by the trainer\n",
    "            self.n_gpu = len(self.trainer.accelerator_connector.parallel_devices)\n",
    "            \n",
    "            \n",
    "    def on_train_epoch_start(self):\n",
    "        # To do at the start of each epoch\n",
    "        self.tr_loss = 0\n",
    "        \n",
    "        'IV.1) Compute mention and entity embeddings and indexes at the start of each epoch'\n",
    "        # Compute mention and entity embeddings and indexes at the start of each epoch\n",
    "        if self.hparams.params['use_types']: # type-specific indexes \n",
    "            self.train_dict_embeddings, self.train_dict_indexes, self.dict_idxs_by_type = data_process.embed_and_index(self.reranker, self.data_module.entity_dict_vecs, encoder_type=\"candidate\", n_gpu=self.n_gpu, corpus= self.data_module.entity_dictionary, force_exact_search= self.hparams.params['force_exact_search'], batch_size= self.hparams.params['embed_batch_size'], probe_mult_factor= self.hparams.params['probe_mult_factor']) #D11\n",
    "            self.train_men_embeddings, self.train_men_indexes, self.men_idxs_by_type = data_process.embed_and_index(self.reranker, self.data_module.train_men_vecs, encoder_type=\"context\", n_gpu=self.n_gpu, corpus= self.data_module.train_processed_data, force_exact_search= self.hparams.params['force_exact_search'], batch_size= self.hparams.params['embed_batch_size'], probe_mult_factor= self.hparams.params['probe_mult_factor'])\n",
    "        else: # general indexes\n",
    "            self.train_dict_embeddings, self.train_dict_index = data_process.embed_and_index(self.reranker, self.data_module.entity_dict_vecs, encoder_type=\"candidate\", n_gpu=self.n_gpu, force_exact_search= self.hparams.params['force_exact_search'], batch_size= self.hparams.params['embed_batch_size'], probe_mult_factor= self.hparams.params['probe_mult_factor'])\n",
    "            self.train_men_embeddings, self.train_men_index = data_process.embed_and_index(self.reranker, self.train_men_vecs, encoder_type=\"context\", n_gpu=self.n_gpu, force_exact_search= self.hparams.params['force_exact_search'], batch_size= self.hparams.params['embed_batch_size'], probe_mult_factor= self.hparams.params['probe_mult_factor'])\n",
    "        \n",
    "        # Number of entities and mentions\n",
    "        self.n_entities = len(self.data_module.entity_dictionary)\n",
    "        self.n_mentions = len(self.data_module.train_processed_data)\n",
    "        \n",
    "        # Store golden MST links\n",
    "        self.gold_links = {}\n",
    "        # Calculate the number of negative entities and mentions to fetch # Divides the k-nn evenly between entities and mentions\n",
    "        self.knn_dict = self.hparams[\"knn\"]//2\n",
    "        self.knn_men = self.hparams[\"knn\"] - self.knn_dict\n",
    "        \n",
    "        'IV.3) knn search : indice and distance of k closest mentions and entities'\n",
    "        logger.info(\"Starting KNN search...\")\n",
    "        # INFO: Fetching all sorted mentions to be able to filter to within-doc later=\n",
    "        n_men_to_fetch = len(self.train_men_embeddings) if self.hparams.params[\"use_types\"] else self.knn_men + self.data_module.max_gold_cluster_len\n",
    "        n_ent_to_fetch = self.knn_dict + 1 # +1 accounts for the possibility of self-reference\n",
    "        if not self.hparams.params[\"use_types\"]:\n",
    "            _, self.dict_nns = self.train_dict_index.search(self.train_men_embeddings, n_ent_to_fetch)\n",
    "            _, self.men_nns = self.train_men_index.search(self.train_men_embeddings, n_men_to_fetch)\n",
    "        else:\n",
    "            self.dict_nns = -1 * np.ones((len(self.train_men_embeddings), n_ent_to_fetch))\n",
    "            self.men_nns = -1 * np.ones((len(self.train_men_embeddings), n_men_to_fetch))\n",
    "            for entity_type in self.train_men_indexes:\n",
    "                self.men_embeds_by_type = self.train_men_embeddings[self.men_idxs_by_type[entity_type]]\n",
    "                _, self.dict_nns_by_type = self.train_dict_indexes[entity_type].search(self.men_embeds_by_type, n_ent_to_fetch)\n",
    "                _, self.men_nns_by_type = self.train_men_indexes[entity_type].search(self.men_embeds_by_type, min(n_men_to_fetch, len(self.men_embeds_by_type)))\n",
    "                self.dict_nns_idxs = np.array(list(map(lambda x: self.dict_idxs_by_type[entity_type][x], self.dict_nns_by_type)))\n",
    "                self.men_nns_idxs = np.array(list(map(lambda x: self.men_idxs_by_type[entity_type][x], self.men_nns_by_type)))\n",
    "                for i, idx in enumerate(self.men_idxs_by_type[entity_type]):\n",
    "                    self.dict_nns[idx] = self.dict_nns_idxs[i]\n",
    "                    self.men_nns[idx][:len(self.men_nns_idxs[i])] = self.men_nns_idxs[i]\n",
    "        logger.info(\"Search finished\")\n",
    "        \n",
    "        self.total_skipped = self.total_knn_men_negs = 0\n",
    "        \n",
    "    \n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        # To do at the end of each epoch\n",
    "        # May not need it \n",
    "        pass\n",
    "    \n",
    "    def on_after_backward(self): #DD11\n",
    "        # After .backward()\n",
    "        if (self.trainer.global_step + 1) % self.trainer.accumulate_grad_batches == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                self.parameters(), self.trainer.grad_clip_norm\n",
    "            )\n",
    "            \n",
    "    # RR2 : Most of the info is already in the Trainer's callback        \n",
    "    # def on_train_batch_end(self, outputs, batch, batch_idx, dataloader_idx = 0):\n",
    "    #     # end of training batch\n",
    "    #     'IV.4.E) Information about the training (step, epoch, average_loss)'\n",
    "    #     n_print_iters = self.hparams.params[\"print_interval\"] * self.trainer.accumulate_grad_batches #29\n",
    "    #     if (batch_idx + 1) % n_print_iters == 0:\n",
    "    #         # DD13\n",
    "    #         self.log(\"train/average_loss\", self.tr_loss / n_print_iters, on_step=False, on_epoch=True, prog_bar=True)\n",
    "    #         if self.total_skipped > 0:\n",
    "    #             self.log(\"train/queries_without_negs\", self.total_skipped / n_print_iters, on_step=False, on_epoch=True)\n",
    "    #             self.log(\"train/negative_mentions_per_query\", self.total_knn_men_negs / n_print_iters, on_step=False, on_epoch=True)\n",
    "            \n",
    "    #         # Reset your tracking variables for the next interval\n",
    "    #         self.total_skipped = 0\n",
    "    #         self.total_knn_men_negs = 0\n",
    "    #         self.tr_loss = 0\n",
    "\n",
    "    #     # DD14\n",
    "    #     '''\n",
    "    #     # Regular checks on model performance against a validation dataset without interrupting the training more often than desired\n",
    "    #     if self.hparams.params[\"eval_interval\"] != -1: #A31\n",
    "    #         if (batch_idx + 1) % (self.hparams.params[\"eval_interval\"] * self.hparams.params[\"gradient_accumulation_steps\"]) == 0:\n",
    "    #             logger.info(\"Evaluation on the development dataset\")\n",
    "    #             evaluate(\n",
    "    #                 self.reranker, self.data_module.entity_dict_vecs, self.data_module.valid_men_vecs, device=self.device, logger=logger, knn=self.hparams[\"knn\"], n_gpu=self.n_gpu,\n",
    "    #                 entity_data=self.data_module.entity_dictionary, query_data=self.valid_processed_data, silent=self.hparams.params[\"silent\"],\n",
    "    #                 use_types=self.hparams.params['use_types'] or self.hparams.params[\"use_types_for_eval\"], embed_batch_size=self.hparams.params[\"embed_batch_size\"],\n",
    "    #                 force_exact_search=self.hparams.params['use_types'] or self.hparams.params[\"use_types_for_eval\"] or self.hparams.params[\"force_exact_search\"],\n",
    "    #                 probe_mult_factor=self.hparams.params['probe_mult_factor'], within_doc=self.hparams.params['within_doc'],\n",
    "    #                 context_doc_ids=self.data_module.valid_context_doc_ids\n",
    "    #             )\n",
    "    #             self.model.train()\n",
    "    #             logger.info(\"\\n\")\n",
    "    #     '''\n",
    "        \n",
    "    #     pass\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx): #DD14\n",
    "\n",
    "        max_acc, dict_acc, embed_and_index_dict = evaluate(\n",
    "            self.reranker, \n",
    "            self.data_module.entity_dict_vecs, \n",
    "            self.data_module.valid_men_vecs, \n",
    "            device=self.device, \n",
    "            logger=logger, \n",
    "            # knn=self.hparams.params[\"knn\"], \n",
    "            n_gpu=self.n_gpu,\n",
    "            entity_data=self.data_module.entity_dictionary, \n",
    "            query_data=self.data_module.valid_processed_data, \n",
    "            # silent=self.hparams.params[\"silent\"],\n",
    "            use_types=self.hparams.params['use_types'], #use_types=self.hparams.params['use_types'] or self.hparams.params[\"use_types_for_eval\"] \n",
    "            embed_batch_size=self.hparams.params[\"embed_batch_size\"],\n",
    "            force_exact_search=self.hparams.params['use_types'] or self.hparams.params[\"force_exact_search\"], #force_exact_search= self.hparams.params['use_types'] or use_types or self.hparams.params[\"use_types_for_eval\"] or self.hparams.params[\"force_exact_search\"]\n",
    "            probe_mult_factor=self.hparams.params['probe_mult_factor'], \n",
    "            within_doc=self.hparams.params['within_doc'],\n",
    "            context_doc_ids=self.data_module.valid_context_doc_ids\n",
    "        )\n",
    "        self.log(\"max_acc\", max_acc, on_epoch=True)\n",
    "        self.log(\"dict_acc\", dict_acc, on_epoch=True)\n",
    "        self.log(\"embedding_and_indexing\", embed_and_index_dict, on_epoch=True)\n",
    "        \n",
    "        \n",
    "    def test_step(self, batch, batch_idx): #DD14\n",
    "        max_acc, dict_acc, embed_and_index_dict = evaluate(\n",
    "            self.reranker, \n",
    "            self.data_module.entity_dict_vecs, \n",
    "            self.data_module.test_men_vecs, \n",
    "            device=self.device, \n",
    "            logger=logger, \n",
    "            # knn=self.hparams.params[\"knn\"], \n",
    "            n_gpu=self.n_gpu,\n",
    "            entity_data=self.data_module.entity_dictionary, \n",
    "            query_data=self.data_module.test_processed_data,\n",
    "            # silent=self.hparams.params[\"silent\"],\n",
    "            use_types=self.hparams.params['use_types'], #use_types=self.hparams.params['use_types'] or self.hparams.params[\"use_types_for_eval\"] \n",
    "            embed_batch_size=self.hparams.params[\"embed_batch_size\"],\n",
    "            force_exact_search=self.hparams.params['use_types'] or self.hparams.params[\"force_exact_search\"], #force_exact_search= self.hparams.params['use_types'] or use_types or self.hparams.params[\"use_types_for_eval\"] or self.hparams.params[\"force_exact_search\"]\n",
    "            probe_mult_factor=self.hparams.params['probe_mult_factor'], \n",
    "            within_doc=self.hparams.params['within_doc'],\n",
    "            context_doc_ids=self.data_module.test_context_doc_ids\n",
    "        )\n",
    "        self.log(\"max_acc\", max_acc, on_epoch=True)\n",
    "        self.log(\"dict_acc\", dict_acc, on_epoch=True)\n",
    "        self.log(\"embedding_and_indexing\", embed_and_index_dict, on_epoch=True)\n",
    "        \n",
    "\n",
    "    def on_train_end(self):\n",
    "        execution_time = (time.time() - self.start_time) / 60\n",
    "        self.logger.info(f\"The training took {execution_time} minutes\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def main(params):\n",
    "    \n",
    "    # Model output path\n",
    "    root_output_dir = params[\"model_output_path\"]  # Root directory for all outputs\n",
    "    experiment_name = f\"experiment_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "    model_output_path = os.path.join(root_output_dir, experiment_name)\n",
    "\n",
    "    # Ensuring the output directory exists\n",
    "    os.makedirs(model_output_path, exist_ok=True)\n",
    "    \n",
    "    # Initialize the data module\n",
    "    data_module = ArboelDataModule(params)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LitArboel(params,data_module)\n",
    "    \n",
    "    # retrieve info during training\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "    monitor='max_acc',  # Metric to monitor\n",
    "    dirpath=model_output_path,  # Directory to save the model\n",
    "    filename='{epoch}-{max_acc:.2f}',  # Saves the model with epoch and val_loss in the filename\n",
    "    save_top_k= 1,  # Number of best models to save; -1 means save all of them\n",
    "    mode='max',  # 'max' means the highest max_acc will be considered as the best model\n",
    "    verbose=True,  # Logs a message whenever a model checkpoint is saved\n",
    "    )\n",
    "\n",
    "    # Initialize PyTorch Lightning trainer\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=params[\"num_train_epochs\"],\n",
    "        devices=1,\n",
    "        accelerator=\"gpu\",\n",
    "        strategy=\"ddp\",\n",
    "        val_check_interval=1, \n",
    "        enable_progress_bar=True,\n",
    "        callbacks=[model_checkpoint]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "    \n",
    "    best_model_path = model_checkpoint.best_model_path\n",
    "    best_model_score = model_checkpoint.best_model_score\n",
    "    logger.info(f\"Best model saved at {best_model_path} with score {best_model_score}\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = BlinkParser(add_model_args=True)\n",
    "    parser.add_training_args()\n",
    "    args = parser.parse_args()\n",
    "    print(args)\n",
    "    main(args.__dict__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "\"model_output_path\": \"(str) Path where the model outputs and logs should be saved\",\n",
    "\"data_path\": \"(str) Path to dictionary.pickle file\",\n",
    "\"knn\" : \"(int) Number of nearest neighbors to consider\" ,\n",
    "\"use_types\" : \"(bool) Whether to use type-specific indexes\" ,\n",
    "\"max_context_length\" : \"(int) Maximum length of context tokens\" ,\n",
    "\"max_cand_length\" : \"(int) Maximum length of candidate entity tokens\" ,\n",
    "\"context_key\" : \"(str) Key for context in the processed data. (= “context”)\" ,\n",
    "\"debug\" : \"(bool) If set to True, run in debug mode (test only on 200 first samples)\" ,\n",
    "\"gold_arbo_knn\" : \"(int) Number of gold nearest neighbors to consider\" ,\n",
    "\"within_doc\" : \"(bool) If True, constrain evaluation within documents\" ,\n",
    "\"within_doc_skip_strategy\" : \"specific strategy used with within_doc\" ,\n",
    "\"filter_unlabeled\" : \"(bool) If True, filter out samples without labels\" ,\n",
    "\"type_optimization\" : \"(string) Type of optimization to use (e.g., AdamW)\", \n",
    "\"learning_rate\" : \"(float) Learning rate for the optimizer\" ,\n",
    "\"warmup_proportion\" : \"(float) Proportion of warmup steps in the total number of training steps\" ,\n",
    "\"fp16\" : \"(bool) Whether to use mixed precision training\" ,\n",
    "\"embed_batch_size\" : \"(int) Batch size for embedding during evaluation\" ,\n",
    "\"force_exact_search\" : \"(bool) If True, use exact search methods during evaluation\" ,\n",
    "\"probe_mult_factor\" : \"(int) Multiplier factor used in index building for probing in approximate search\" ,\n",
    "\"pos_neg_loss\" : \"Specific positive + negative type of loss\" ,\n",
    "\"pickle_src_path\" : \"(string) Path to the directory containing preprocessed data in pickle format\" , \n",
    "\"use_types_for_eval\" : \"(bool) Whether to use type-specific indexes for evaluation\" ,\n",
    "\"drop_entities\" : \"(bool) If True, drop entities from training data based on some criterion\" ,\n",
    "\"drop_set\" : \"(string) Path to the mention data from which entities are to be dropped\" ,\n",
    "\"farthest_neighbor\" : \"(bool) If True, consider the farthest neighbor instead of the nearest for positive linkage\" ,\n",
    "\"rand_gold_arbo\" : \"(bool) If True, use random permutation for gold MST approximation\",\n",
    "\"bert_model\" : \"bert-base-uncased / bert-large-uncased / bert-base-cased\",\n",
    "\"out_dim\": \"(int = 768) Output dimension = length of the encoded mention/entity\",\n",
    "\"pull_from_layer\" : \"(int=11 or 23) From which layer shall we pull the encoded embedding\",\n",
    "\"add_linear\" : \"(bool : True) Whether an additional linear transformation is applied to the output\"\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
