{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27/Mar/2024 23:06:45] INFO - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "[27/Mar/2024 23:06:45] INFO - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "[27/Mar/2024 23:06:45] INFO - Loading faiss with AVX2 support.\n",
      "[27/Mar/2024 23:06:45] INFO - Successfully loaded faiss with AVX2 support.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../..')\n",
    "import ujson\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "import logger\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim, nn, utils, Tensor\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import lightning as L\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_transformers.tokenization_bert import BertTokenizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datetime import datetime\n",
    "from typing import Optional, Union\n",
    "\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler) \n",
    "from pytorch_transformers.optimization import WarmupLinearSchedule \n",
    "from scipy.sparse.csgraph import minimum_spanning_tree \n",
    "# csgraph = compressed sparse graph\n",
    "from scipy.sparse import csr_matrix\n",
    "# csr_matrix = compressed sparse row matrices\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import blink.biencoder.data_process_mult as data_process\n",
    "# import blink.biencoder.eval_cluster_linking as eval_cluster_linking\n",
    "import blink.candidate_ranking.utils as utils\n",
    "from blink.biencoder.biencoder import BiEncoderRanker\n",
    "from blink.common.optimizer import get_bert_optimizer\n",
    "from blink.common.params import BlinkParser\n",
    "\n",
    "from IPython import embed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27/Mar/2024 23:06:45] INFO - PyTorch version 2.2.0 available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for an_em contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/an_em/an_em.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for anat_em contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/anat_em/anat_em.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for ask_a_patient contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/ask_a_patient/ask_a_patient.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bc5cdr contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bc5cdr/bc5cdr.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bc7_litcovid contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bc7_litcovid/bc7_litcovid.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bio_sim_verb contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bio_sim_verb/bio_sim_verb.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bio_simlex contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bio_simlex/bio_simlex.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bioasq_2021_mesinesp contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bioasq_2021_mesinesp/bioasq_2021_mesinesp.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bioasq_task_b contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bioasq_task_b/bioasq_task_b.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bioasq_task_c_2017 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bioasq_task_c_2017/bioasq_task_c_2017.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bioid contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bioid/bioid.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bioinfer contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bioinfer/bioinfer.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for biology_how_why_corpus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/biology_how_why_corpus/biology_how_why_corpus.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for biomrc contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/biomrc/biomrc.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bionlp_shared_task_2009 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bionlp_shared_task_2009/bionlp_shared_task_2009.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bionlp_st_2011_epi contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bionlp_st_2011_epi/bionlp_st_2011_epi.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bionlp_st_2011_ge contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bionlp_st_2011_ge/bionlp_st_2011_ge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bionlp_st_2011_id contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bionlp_st_2011_id/bionlp_st_2011_id.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bionlp_st_2011_rel contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bionlp_st_2011_rel/bionlp_st_2011_rel.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bionlp_st_2013_cg contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bionlp_st_2013_cg/bionlp_st_2013_cg.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bionlp_st_2013_ge contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bionlp_st_2013_ge/bionlp_st_2013_ge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bionlp_st_2013_gro contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bionlp_st_2013_gro/bionlp_st_2013_gro.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bionlp_st_2013_pc contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bionlp_st_2013_pc/bionlp_st_2013_pc.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bionlp_st_2019_bb contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bionlp_st_2019_bb/bionlp_st_2019_bb.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for biored contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/biored/biored.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for biorelex contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/biorelex/biorelex.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bioscope contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bioscope/bioscope.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for biosses contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/biosses/biosses.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for blurb contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/blurb/blurb.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bronco contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bronco/bronco.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for cantemist contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/cantemist/cantemist.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for cardiode contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/cardiode/cardiode.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for cas contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/cas/cas.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for cellfinder contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/cellfinder/cellfinder.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for chebi_nactem contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/chebi_nactem/chebi_nactem.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for chemdner contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/chemdner/chemdner.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for chemprot contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/chemprot/chemprot.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for chia contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/chia/chia.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for citation_gia_test_collection contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/citation_gia_test_collection/citation_gia_test_collection.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for codiesp contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/codiesp/codiesp.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for cpi contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/cpi/cpi.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for ctebmsp contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/ctebmsp/ctebmsp.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for czi_drsm contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/czi_drsm/czi_drsm.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for ddi_corpus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/ddi_corpus/ddi_corpus.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for distemist contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/distemist/distemist.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for drugprot contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/drugprot/drugprot.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for ebm_pico contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/ebm_pico/ebm_pico.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for ehr_rel contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/ehr_rel/ehr_rel.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for essai contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/essai/essai.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for euadr contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/euadr/euadr.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for evidence_inference contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/evidence_inference/evidence_inference.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for gad contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/gad/gad.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for genetag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/genetag/genetag.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for genia_ptm_event_corpus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/genia_ptm_event_corpus/genia_ptm_event_corpus.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for genia_relation_corpus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/genia_relation_corpus/genia_relation_corpus.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for genia_term_corpus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/genia_term_corpus/genia_term_corpus.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for geokhoj_v1 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/geokhoj_v1/geokhoj_v1.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for ggponc2 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/ggponc2/ggponc2.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for gnormplus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/gnormplus/gnormplus.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for hallmarks_of_cancer contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/hallmarks_of_cancer/hallmarks_of_cancer.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for hprd50 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/hprd50/hprd50.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for iepa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/iepa/iepa.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for jnlpba contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/jnlpba/jnlpba.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for linnaeus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/linnaeus/linnaeus.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for lll contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/lll/lll.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for mayosrs contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/mayosrs/mayosrs.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for med_qa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/med_qa/med_qa.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for medal contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/medal/medal.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for meddialog contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/meddialog/meddialog.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for meddocan contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/meddocan/meddocan.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for medhop contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/medhop/medhop.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for medical_data contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/medical_data/medical_data.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for mediqa_nli contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/mediqa_nli/mediqa_nli.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for mediqa_qa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/mediqa_qa/mediqa_qa.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for mediqa_rqe contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/mediqa_rqe/mediqa_rqe.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for medmentions contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/medmentions/medmentions.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for mednli contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/mednli/mednli.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for meqsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/meqsum/meqsum.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for minimayosrs contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/minimayosrs/minimayosrs.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for mirna contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/mirna/mirna.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for mlee contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/mlee/mlee.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for mqp contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/mqp/mqp.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for msh_wsd contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/msh_wsd/msh_wsd.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for muchmore contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/muchmore/muchmore.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for multi_xscience contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/multi_xscience/multi_xscience.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for n2c2_2006_deid contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/n2c2_2006_deid/n2c2_2006_deid.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for n2c2_2006_smokers contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/n2c2_2006_smokers/n2c2_2006_smokers.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for n2c2_2008 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/n2c2_2008/n2c2_2008.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for n2c2_2009 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/n2c2_2009/n2c2_2009.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for n2c2_2010 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/n2c2_2010/n2c2_2010.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for n2c2_2011 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/n2c2_2011/n2c2_2011.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for n2c2_2014_deid contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/n2c2_2014_deid/n2c2_2014_deid.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for n2c2_2018_track1 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/n2c2_2018_track1/n2c2_2018_track1.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for n2c2_2018_track2 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/n2c2_2018_track2/n2c2_2018_track2.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for ncbi_disease contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/ncbi_disease/ncbi_disease.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for nlm_gene contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/nlm_gene/nlm_gene.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for nlm_wsd contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/nlm_wsd/nlm_wsd.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for nlmchem contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/nlmchem/nlmchem.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for ntcir_13_medweb contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/ntcir_13_medweb/ntcir_13_medweb.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for osiris contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/osiris/osiris.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for paramed contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/paramed/paramed.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for pcr contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/pcr/pcr.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for pdr contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/pdr/pdr.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for pharmaconer contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/pharmaconer/pharmaconer.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for pico_extraction contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/pico_extraction/pico_extraction.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for pmc_patients contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/pmc_patients/pmc_patients.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for progene contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/progene/progene.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for psytar contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/psytar/psytar.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for pubhealth contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/pubhealth/pubhealth.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for pubmed_qa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/pubmed_qa/pubmed_qa.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for pubtator_central contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/pubtator_central/pubtator_central.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for quaero contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/quaero/quaero.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for scai_chemical contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/scai_chemical/scai_chemical.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for scai_disease contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/scai_disease/scai_disease.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for scicite contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/scicite/scicite.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for scielo contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/scielo/scielo.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for scifact contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/scifact/scifact.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for sciq contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/sciq/sciq.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for scitail contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/scitail/scitail.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for sem_eval_2024_task_2 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/sem_eval_2024_task_2/sem_eval_2024_task_2.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for seth_corpus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/seth_corpus/seth_corpus.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for spl_adr_200db contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/spl_adr_200db/spl_adr_200db.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for swedish_medical_ner contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/swedish_medical_ner/swedish_medical_ner.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for tmvar_v1 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/tmvar_v1/tmvar_v1.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for tmvar_v2 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/tmvar_v2/tmvar_v2.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for tmvar_v3 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/tmvar_v3/tmvar_v3.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for twadrl contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/twadrl/twadrl.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for umnsrs contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/umnsrs/umnsrs.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for verspoor_2013 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/verspoor_2013/verspoor_2013.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sys.path.append('../../../..')\n",
    "sys.path.append('..')\n",
    "from DataModule import process_mention_dataset\n",
    "\n",
    "from bigbio.dataloader import BigBioConfigHelpers\n",
    "from umls_utils import UmlsMappings\n",
    "from bigbio_utils import CUIS_TO_REMAP, CUIS_TO_EXCLUDE, DATASET_NAMES, VALIDATION_DOCUMENT_IDS\n",
    "from bigbio_utils import dataset_to_documents, dataset_to_df, resolve_abbreviation\n",
    "\n",
    "\n",
    "conhelps = BigBioConfigHelpers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configure the logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(split, params, logger):\n",
    "    '''\n",
    "    Description \n",
    "    -----------\n",
    "    Loads dataset samples from a specified path\n",
    "    Optionally filters out samples without labels\n",
    "    Checks if the dataset supports multiple labels per sample\n",
    "    \"has_mult_labels\" : bool\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    split : str\n",
    "        Indicates the portion of the dataset to load (\"train\", \"test\", \"valid\"), used by utils.read_dataset to determine which data to read.\n",
    "    params : dict(str)\n",
    "        Contains configuration options\n",
    "    logger : \n",
    "        An object used for logging messages about the process, such as the number of samples read.\n",
    "    '''\n",
    "    samples = utils.read_dataset(split, params[\"data_path\"]) #DD21\n",
    "    \n",
    "    # Check if dataset has multiple ground-truth labels\n",
    "    has_mult_labels = \"labels\" in samples[0].keys()\n",
    "    if params[\"filter_unlabeled\"]:\n",
    "        # Filter samples without gold entities\n",
    "        samples = list(\n",
    "            filter(lambda sample: (len(sample[\"labels\"]) > 0) if has_mult_labels else (sample[\"label\"] is not None),\n",
    "                   samples))\n",
    "    logger.info(f\"Read %d {split} samples.\" % len(samples))\n",
    "    return samples, has_mult_labels\n",
    "\n",
    "\n",
    "# Utility function\n",
    "def filter_by_context_doc_id(mention_idxs, doc_id, doc_id_list, return_numpy=False):\n",
    "    '''\n",
    "    Description \n",
    "    -----------\n",
    "    Filters and returns mention indices that belong to a specific document identified by \"doc_id\".\n",
    "    Ensures that the analysis are constrained within the context of that particular document.\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    - mention_idxs : ndarray(int) of dim = (number of mentions)\n",
    "    Represents the indices of mentions\n",
    "    - doc_id : int \n",
    "    Indice of the target document\n",
    "    - doc_id_list : ndarray(int) of dim = (number of mentions)\n",
    "    Array of integers, where each element is a document ID associated with the corresponding mention in mention_idxs. \n",
    "    The length of doc_id_list should match the total number of mentions referenced in mention_idxs.\n",
    "    - return_numpy : bool\n",
    "    A flag indicating whether to return the filtered list of mention indices as a NumPy array. \n",
    "    If True, the function returns a NumPy array; otherwise, it returns a list\n",
    "    -------\n",
    "    Outputs: \n",
    "    - mask : ndarray(bool) of dim = (number of mentions)\n",
    "    Mask indicating where each mention's document ID (from doc_id_list) matches the target doc_id\n",
    "    - mention_idxs : \n",
    "    Only contains mention indices that belong to the target document (=doc_id).\n",
    "    '''\n",
    "    mask = [doc_id_list[i] == doc_id for i in mention_idxs]\n",
    "    if isinstance(mention_idxs, list): # Test if mention_idxs = list. Return a bool\n",
    "        mention_idxs = np.array(mention_idxs) \n",
    "    mention_idxs = mention_idxs[mask] # possible only if mention_idxs is an array, not a list\n",
    "    if not return_numpy:\n",
    "        mention_idxs = list(mention_idxs)\n",
    "    return mention_idxs, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Data module\"\n",
    "class ArboelDataModule2(L.LightningDataModule):\n",
    "    '''\n",
    "    Attributes\n",
    "    ----------\n",
    "    \n",
    "    - entity_dictionary : list of dict\n",
    "    Stores the initial and raw entity dictionary\n",
    "    - train_tensor_data : TensorDataset(context_vecs, label_idxs, n_labels, mention_idx) with :\n",
    "        - context_vecs : tensor containing IDs of (mention + surrounding context) tokens \n",
    "        - label_idxs : tensor with indices pointing to the entities in the entity dictionary that are considered correct labels for the mention.\n",
    "        - n_labels : Number of labels (=entities) associated with the mention\n",
    "        - mention_idx : tensor containing a sequence of integers from 0 to N-1 (N = number of mentions in the dataset) serving as a unique identifier for each mention.\n",
    "    - train_processed_data : list of dict\n",
    "    Contains information about mentions (mention_id, mention_name, context, etc)\n",
    "    - valid_tensor_data : TensorDataset\n",
    "    Same as \"train_tensor_dataset\" but for validation set\n",
    "    - max_gold_cluster_len : int\n",
    "    Maximum length of clusters inside gold_cluster\n",
    "    - train_context_doc_ids : list\n",
    "    # Store the context_doc_id (=context document indice) for every mention in the train set\n",
    "    '''\n",
    "    def __init__(self, params):\n",
    "        '''\n",
    "        Parameters \n",
    "        ----------\n",
    "        - params : dict(str)\n",
    "        Contains configuration options\n",
    "        - dataset : str\n",
    "        Name of the dataset\n",
    "        - ontology : str (only umls for now)\n",
    "        Ontology associated with the dataset\n",
    "        - model : \n",
    "        model used : arboel / krissbert / sapbert etc...\n",
    "        - ontology_type : str\n",
    "        'obo' or 'umls' and possibly others\n",
    "        - ontology_dir : str\n",
    "        Path to ontology\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(params)\n",
    "        # #First try to load the tokens from a local file. If local file not found, uses a pre-trained model specified by params[\"bert_model\"]\n",
    "        # vocab_path = os.path.join(self.hparams[\"bert_model\"], 'vocab.txt') #DD3\n",
    "        # if os.path.isfile(vocab_path): \n",
    "        #     print(f\"Found tokenizer vocabulary at {vocab_path}\")\n",
    "        # self.tokenizer = BertTokenizer.from_pretrained(\n",
    "        #     vocab_path if os.path.isfile(vocab_path) else self.hparams[\"bert_model\"], do_lower_case=self.hparams[\"lowercase\"]\n",
    "        # )\n",
    "        \n",
    "        self.dataset = self.hparams[\"dataset\"]\n",
    "        self.ontology = self.hparams[\"ontology\"]\n",
    "        self.data_path = self.hparams[\"data_path\"]\n",
    "        self.ontology_dir = self.hparams[\"ontology_dir\"]\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.hparams[\"bert_model\"])\n",
    "            \n",
    "        self.batch_size = self.hparams.get(\"train_batch_size\", \"scoring_batch_size\")\n",
    "        \n",
    "        self.train_processed_data = None\n",
    "        self.valid_processed_data = None\n",
    "        self.test_processed_data = None\n",
    "        self.train_tensor_data = None\n",
    "        self.valid_tensor_data = None\n",
    "        self.test_tensor_data = None\n",
    "        self.train_samples = None\n",
    "        self.valid_samples = None\n",
    "        self.test_samples = None\n",
    "        self.entity_dict_vecs = None\n",
    "\n",
    "\n",
    "    def prepare_data(self):\n",
    "        'Use this to download and prepare data.'\n",
    "        \n",
    "        # Create the entity data files: dictionary.pickle\n",
    "        # Create the mentions data files:  train.jsonl, valid.jsonl, test.jsonl\n",
    "        \n",
    "        # path to a file where the training data is stored\n",
    "        self.train_data = os.path.join(\n",
    "            self.data_path, \"train.jsonl\"\n",
    "        )\n",
    "\n",
    "        # if the full path to file exist, no need to prepare them, they are already ready\n",
    "        if not os.path.isfile(self.train_data):\n",
    "            process_mention_dataset(\n",
    "                ontology=self.ontology,\n",
    "                dataset=self.dataset,\n",
    "                data_path=self.data_path,\n",
    "                ontology_type=self.ontology_type,\n",
    "                ontology_dir=self.ontology_dir,\n",
    "                mention_id=True,\n",
    "                context_doc_id=True,\n",
    "            )\n",
    "        \n",
    "\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        '''\n",
    "        For processing and splitting. Called at the beginning of fit (train + validate), validate, test, or predict.\n",
    "        '''\n",
    "        self.entity_dictionary_pkl_path = os.path.join(self.data_path, 'entity_dictionary.pickle')\n",
    "        self.train_tensor_data_pkl_path = os.path.join(self.data_path, 'train_tensor_data.pickle')\n",
    "        self.train_processed_data_pkl_path = os.path.join(self.data_path, 'train_processed_data.pickle')\n",
    "        self.valid_tensor_data_pkl_path = os.path.join(self.data_path, 'valid_tensor_data.pickle')\n",
    "        self.valid_processed_data_pkl_path = os.path.join(self.data_path, 'valid_processed_data.pickle')\n",
    "        self.test_tensor_data_pkl_path = os.path.join(self.data_path, 'test_tensor_data.pickle')\n",
    "        self.test_processed_data_pkl_path = os.path.join(self.data_path, 'test_processed_data.pickle')\n",
    "        \n",
    "        'entity dictionary'\n",
    "        # if entity dictionary already tokenized, load it\n",
    "        # self.entity_dictionary_pkl_path = os.path.join(self.data_path, 'entity_dictionary.pickle')\n",
    "        self.entity_dictionary_loaded = False\n",
    "        if os.path.isfile(self.entity_dictionary_pkl_path): \n",
    "            print(\"Loading stored processed entity dictionary...\")\n",
    "            with open(self.entity_dictionary_pkl_path, 'rb') as read_handle:\n",
    "                self.entity_dictionary = pickle.load(read_handle) # DD12B\n",
    "            self.entity_dictionary_loaded = True\n",
    "        \n",
    "        else : # else load the not processed one\n",
    "            with open(os.path.join(self.data_path, 'dictionary.pickle'), 'rb') as read_handle: #A11\n",
    "                self.entity_dictionary = pickle.load(read_handle)\n",
    "        \n",
    "        'training mention data'\n",
    "        # #path to a file where the training data, already processed into tensors is saved\n",
    "        # self.train_tensor_data_pkl_path = os.path.join(self.data_path, 'train_tensor_data.pickle')\n",
    "        # #path to a file where metadata / additional information about the training data is stored\n",
    "        # self.train_processed_data_pkl_path = os.path.join(self.data_path, 'train_processed_data.pickle')\n",
    "\n",
    "            \n",
    "        # if the full path to file exist, load the file\n",
    "        if os.path.isfile(self.train_tensor_data_pkl_path) and os.path.isfile(self.train_processed_data_pkl_path):\n",
    "            print(\"Loading stored processed train data...\")\n",
    "            with open(self.train_tensor_data_pkl_path, 'rb') as read_handle:\n",
    "                self.train_tensor_data = pickle.load(read_handle)\n",
    "            with open(self.train_processed_data_pkl_path, 'rb') as read_handle:\n",
    "                self.train_processed_data = pickle.load(read_handle)\n",
    "                \n",
    "                \n",
    "        'validation mention data'\n",
    "        # self.valid_tensor_data_pkl_path = os.path.join(self.data_path, 'valid_tensor_data.pickle')\n",
    "        # self.valid_processed_data_pkl_path = os.path.join(self.data_path, 'valid_processed_data.pickle')\n",
    "        \n",
    "        # Same as training data : \n",
    "        # if the full path to file exist, load the file\n",
    "        if os.path.isfile(self.valid_tensor_data_pkl_path) and os.path.isfile(self.valid_processed_data_pkl_path):\n",
    "            print(\"Loading stored processed valid data...\")\n",
    "            with open(self.valid_tensor_data_pkl_path, 'rb') as read_handle:\n",
    "                self.valid_tensor_data = pickle.load(read_handle)\n",
    "            with open(self.valid_processed_data_pkl_path, 'rb') as read_handle:\n",
    "                self.valid_processed_data = pickle.load(read_handle)\n",
    "                \n",
    "        'test mention data'\n",
    "        # self.test_tensor_data_pkl_path = os.path.join(self.data_path, 'test_tensor_data.pickle')\n",
    "        # self.test_processed_data_pkl_path = os.path.join(self.data_path, 'test_processed_data.pickle')\n",
    "        \n",
    "        # Same as training data : \n",
    "        # if the full path to file exist, load the file\n",
    "        if os.path.isfile(self.test_tensor_data_pkl_path) and os.path.isfile(self.test_processed_data_pkl_path):\n",
    "            print(\"Loading stored processed test data...\")\n",
    "            with open(self.test_tensor_data_pkl_path, 'rb') as read_handle: #CC7 'rb' = binary read mode\n",
    "                self.test_tensor_data = pickle.load(read_handle)\n",
    "            with open(self.test_processed_data_pkl_path, 'rb') as read_handle:\n",
    "                self.test_processed_data = pickle.load(read_handle)\n",
    "        \n",
    "        \n",
    "        \n",
    "        'Entity dict : drop entity for discovery'\n",
    "        # For discovery experiment: Drop entities used in training that were dropped randomly from dev/test set\n",
    "        if self.hparams[\"drop_entities\"]: #A12\n",
    "            assert self.entity_dictionary \n",
    "            drop_set_path = self.hparams[\"drop_set\"] if self.hparams[\"drop_set\"] is not None else os.path.join(self.data_path, 'drop_set_mention_data.pickle') #A12\n",
    "            if not os.path.isfile(drop_set_path):\n",
    "                raise ValueError(\"Invalid or no --drop_set path provided to dev/test mention data\")\n",
    "            with open(drop_set_path, 'rb') as read_handle:\n",
    "                drop_set_data = pickle.load(read_handle)\n",
    "            # gold cuis indices for each mention in drop_set_data\n",
    "            drop_set_mention_gold_cui_idxs = list(map(lambda x: x['label_idxs'][0], drop_set_data))\n",
    "            # Make the set unique\n",
    "            ents_in_data = np.unique(drop_set_mention_gold_cui_idxs)\n",
    "            # % of drop\n",
    "            ent_drop_prop = 0.1\n",
    "            logger.info(f\"Dropping {ent_drop_prop*100}% of {len(ents_in_data)} entities found in drop set\")\n",
    "            # Number of entity indices to drop\n",
    "            n_ents_dropped = int(ent_drop_prop*len(ents_in_data))\n",
    "            # Random selection drop\n",
    "            rng = np.random.default_rng(seed=17)\n",
    "            #Indices of all entities that are dropped\n",
    "            dropped_ent_idxs = rng.choice(ents_in_data, size=n_ents_dropped, replace=False)\n",
    "\n",
    "            # Drop entities from dictionary (subsequent processing will automatically drop corresponding mentions)\n",
    "            keep_mask = np.ones(len(self.entity_dictionary), dtype='bool')\n",
    "            keep_mask[dropped_ent_idxs] = False\n",
    "            self.entity_dictionary = np.array(self.entity_dictionary)[keep_mask]\n",
    "        \n",
    "        \n",
    "        'Train mention data'\n",
    "        \n",
    "        if not os.path.isfile(self.train_tensor_data_pkl_path) : # Load and Process train data if not done yet\n",
    "            # train_samples = list of dict. Each dict contains information about a mention (id, name, context, etc). \n",
    "            # Each key can have a dictionary itself. Ex : mention[\"context\"][\"tokens\"] or mention[\"context\"][\"ids\"]\n",
    "            self.train_samples, self.train_mult_labels = read_data(\"train\", self.hparams, logger)\n",
    "        \n",
    "            # train_processed_data = (mention + surrounding context) tokens\n",
    "            # entity_dictionary = tokenized entities\n",
    "            # tensor_train_dataset = Dataset containing several tensors (IDs of mention + context / indices of correct entities etc..) # Go check \"process_mention_data\" for more info\n",
    "            self.train_processed_data, self.entity_dictionary, self.train_tensor_data = data_process.process_mention_data(\n",
    "                self.train_samples,\n",
    "                self.entity_dictionary,\n",
    "                self.tokenizer,\n",
    "                self.hparams[\"max_context_length\"],\n",
    "                self.hparams[\"max_cand_length\"],\n",
    "                context_key=self.hparams[\"context_key\"],\n",
    "                multi_label_key=\"labels\" if self.train_mult_labels else None,\n",
    "                #silent=self.hparams[\"silent\"], \n",
    "                logger=logger,\n",
    "                debug=self.hparams[\"debug\"], \n",
    "                knn=self.hparams['knn'],\n",
    "                dictionary_processed=self.entity_dictionary_loaded\n",
    "            )\n",
    "            \n",
    "            # Save the entity dictionary if not already done\n",
    "            if not self.entity_dictionary_loaded:\n",
    "                print(\"Saving entity dictionary...\")\n",
    "                with open(self.entity_dictionary_pkl_path, 'wb') as write_handle:\n",
    "                    pickle.dump(self.entity_dictionary, write_handle,\n",
    "                            protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                    \n",
    "            self.entity_dictionary_loaded = True\n",
    "            \n",
    "            print(\"Saving processed train data...\")\n",
    "            with open(self.train_tensor_data_pkl_path, 'wb') as write_handle:\n",
    "                pickle.dump(self.train_tensor_data, write_handle,\n",
    "                            protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            with open(self.train_processed_data_pkl_path, 'wb') as write_handle:\n",
    "                pickle.dump(self.train_processed_data, write_handle,\n",
    "                            protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        # Prepare tensor containing only ID of (mention + surrounding context) tokens of training set'\n",
    "        self.train_men_vecs = self.train_tensor_data[:][0] \n",
    "\n",
    "        # Store the IDs of the entity in entity_dictionary # It's the equivalent of train_men_vecs for entities\n",
    "        #(Done here because data_process.process_mention_data will tokenize the entities in entity_dict)\n",
    "        self.entity_dict_vecs = torch.tensor(list(map(lambda x: x['ids'], self.entity_dictionary)), dtype=torch.long)\n",
    "\n",
    "\n",
    "        'Validation mention data'\n",
    "        if not os.path.isfile(self.valid_tensor_data_pkl_path) : \n",
    "            # Load and Process validation data if not done yet\n",
    "            self.valid_samples, self.valid_mult_labels = read_data(\"valid\", self.hparams, logger)\n",
    "            self.valid_processed_data, _, self.valid_tensor_data = data_process.process_mention_data(\n",
    "                self.valid_samples,\n",
    "                self.entity_dictionary,\n",
    "                self.tokenizer,\n",
    "                self.hparams[\"max_context_length\"],\n",
    "                self.hparams[\"max_cand_length\"],\n",
    "                context_key=self.hparams[\"context_key\"],\n",
    "                multi_label_key=\"labels\" if self.valid_mult_labels else None,\n",
    "                # silent=self.hparams[\"silent\"],\n",
    "                logger=logger,\n",
    "                debug=self.hparams[\"debug\"],\n",
    "                knn=self.hparams[\"knn\"],\n",
    "                dictionary_processed=self.entity_dictionary_loaded\n",
    "            )\n",
    "            \n",
    "            print(\"Saving processed valid data...\")\n",
    "            with open(self.valid_tensor_data_pkl_path, 'wb') as write_handle:\n",
    "                pickle.dump(self.valid_tensor_data, write_handle,\n",
    "                            protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            with open(self.valid_processed_data_pkl_path, 'wb') as write_handle:\n",
    "                pickle.dump(self.valid_processed_data, write_handle,\n",
    "                            protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        # Prepare tensor containing only ID of (mention + surrounding context) tokens of validation data'\n",
    "        self.valid_men_vecs = self.valid_tensor_data[:][0]\n",
    "        \n",
    "        \n",
    "        'Test mention data'\n",
    "        if not os.path.isfile(self.test_tensor_data_pkl_path) :\n",
    "            # Load and Process test data if not done yet\n",
    "            self.test_samples, self.test_mult_labels = read_data(\"test\", self.hparams, logger)\n",
    "            self.test_processed_data, _, self.test_tensor_data = data_process.process_mention_data(\n",
    "                self.test_samples,\n",
    "                self.entity_dictionary,\n",
    "                self.tokenizer,\n",
    "                self.hparams[\"max_context_length\"],\n",
    "                self.hparams[\"max_cand_length\"],\n",
    "                context_key=self.hparams[\"context_key\"],\n",
    "                multi_label_key=\"labels\" if self.test_mult_labels else None,\n",
    "                # silent=self.hparams[\"silent\"],\n",
    "                logger=logger,\n",
    "                debug=self.hparams[\"debug\"],\n",
    "                knn=self.hparams[\"knn\"],\n",
    "                dictionary_processed=self.entity_dictionary_loaded\n",
    "            )\n",
    "            \n",
    "            print(\"Saving processed test data...\")\n",
    "            with open(self.test_tensor_data_pkl_path, 'wb') as write_handle:\n",
    "                pickle.dump(self.test_tensor_data, write_handle,\n",
    "                            protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            with open(self.test_processed_data_pkl_path, 'wb') as write_handle:\n",
    "                pickle.dump(self.test_processed_data, write_handle,\n",
    "                            protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        # Prepare tensor containing only ID of (mention + surrounding context) tokens of validation data'\n",
    "        self.test_men_vecs = self.test_tensor_data[:][0]\n",
    "        \n",
    "        \n",
    "        'Within_doc search'\n",
    "        #Consider if its within_doc (=search only within the document)'\n",
    "        self.train_context_doc_ids = self.valid_context_doc_ids = self.test_context_doc_ids = None\n",
    "        if self.hparams[\"within_doc\"]: \n",
    "            #RR9 : If path exist, train_samples, valid_samples, test_samples haven't been defined yet\n",
    "            # Store the context_doc_id for every mention in the train and valid sets\n",
    "            if self.train_samples is None:\n",
    "                self.train_samples, _ = read_data(\"train\", self.hparams, logger)\n",
    "            self.train_context_doc_ids = [s['context_doc_id'] for s in self.train_samples]\n",
    "            if self.valid_samples is None:\n",
    "                self.valid_samples, _ = read_data(\"valid\", self.hparams, logger)\n",
    "            self.valid_context_doc_ids = [s['context_doc_id'] for s in self.valid_samples]\n",
    "            if self.test_samples is None:\n",
    "                self.test_samples, _ = read_data(\"test\", self.hparams, logger)\n",
    "            self.test_context_doc_ids = [s['context_doc_id'] for s in self.test_samples]\n",
    "        \n",
    "        # Get clusters of mentions that map to a gold entity\n",
    "        self.train_gold_clusters = data_process.compute_gold_clusters(self.train_processed_data)\n",
    "        # Maximum length of clusters inside gold_cluster\n",
    "        self.max_gold_cluster_len = 0\n",
    "        for ent in self.train_gold_clusters:\n",
    "            if len(self.train_gold_clusters[ent]) > self.max_gold_cluster_len:\n",
    "                self.max_gold_cluster_len = len(self.train_gold_clusters[ent])\n",
    "        \n",
    "        # print(\n",
    "        #     f\"entity_dictionary : {self.entity_dictionary[0]}, size : {len(self.entity_dictionary)}, type : {type(self.entity_dictionary)}\"\n",
    "        # )\n",
    "\n",
    "        # print(\n",
    "        #     f\"train_samples : {self.train_samples[0]}, size : {len(self.train_samples)}, type : {type(self.train_processed_data)}\"\n",
    "        # )\n",
    "        # print(\n",
    "        #     f\"valid_samples :{self.valid_samples[0]}, size : {len(self.valid_samples)}, type : {type(self.train_processed_data)}\"\n",
    "        # )\n",
    "        # print(\n",
    "        #     f\"test_samples : {self.test_samples[0]}, size : {len(self.test_samples)}, type : {type(self.train_processed_data)}\"\n",
    "        # )\n",
    "\n",
    "        # print(\n",
    "        #     f\"train_processed_data : {self.train_processed_data[0]} , size : {len(self.train_processed_data)}, type : {type(self.train_processed_data)} \"\n",
    "        # )\n",
    "        # print(\n",
    "        #     f\"valid_processed_data : {self.valid_processed_data[0]} , size : {len(self.valid_processed_data)}, type : {type(self.valid_processed_data)}\"\n",
    "        # )\n",
    "        # print(\n",
    "        #     f\"test_processed_data : {self.test_processed_data[0]} , size : {len(self.test_processed_data)}, type : {type(self.test_processed_data)}\"\n",
    "        # )\n",
    "\n",
    "        # print(\n",
    "        #     f\"train_tensor_data : {self.train_tensor_data[0]} , size : {len(self.train_tensor_data)}, type : {type(self.train_tensor_data)}\"\n",
    "        # )\n",
    "        # print(\n",
    "        #     f\"valid_tensor_data : {self.valid_tensor_data[0]} , size : {len(self.valid_tensor_data)}, type : {type(self.valid_tensor_data)}\"\n",
    "        # )\n",
    "        # print(\n",
    "        #     f\"test_tensor_data : {self.test_tensor_data[0]} , size : {len(self.test_tensor_data)}, type : {type(self.test_tensor_data)}\"\n",
    "        # )\n",
    "\n",
    "        # print(\n",
    "        #     f\"train_men_vecs : {self.train_men_vecs[0]} , size : {self.train_men_vecs.size}, type : {type(self.train_men_vecs)}\"\n",
    "        # )\n",
    "        # print(\n",
    "        #     f\"entity_dict_vecs : {self.train_men_vecs[0]} , size :{self.entity_dict_vecs.size}, type : {type(self.entity_dict_vecs)}\"\n",
    "        # )\n",
    "        \n",
    "        # print(\n",
    "        #     f\"train_context_doc_ids : {self.train_context_doc_ids[0]}, size : {len(self.train_context_doc_ids)}, type : {type(self.train_context_doc_ids)}\"\n",
    "        # )\n",
    "        \n",
    "        # print(\n",
    "        #     f\"train_gold_clusters : {self.train_gold_clusters}, size : {len(self.train_gold_clusters)}, type : {type(self.train_gold_clusters)}\"\n",
    "        # )\n",
    "\n",
    "\n",
    "    def train_dataloader(self): #RR5\n",
    "        # Return the training DataLoader\n",
    "        #train_sampler = RandomSampler(self.train_tensor_data) if self.params[\"shuffle\"] else SequentialSampler(self.train_tensor_data)\n",
    "        #return DataLoader(self.train_tensor_data, sampler=train_sampler, batch_size=self.batch_size) #DD4\n",
    "        return DataLoader(dataset = self.train_tensor_data, batch_size=self.batch_size, shuffle=True,\n",
    "            drop_last=True,\n",
    "            )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        # Return the validation DataLoader\n",
    "        return DataLoader(dataset = self.valid_tensor_data, batch_size=self.batch_size)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        # Return the validation DataLoader\n",
    "        return DataLoader(dataset = self.test_tensor_data, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home2/cye73/data_test/arboel/ncbi_disease\n"
     ]
    }
   ],
   "source": [
    "# ontology = \"MeSH\"\n",
    "# dataset = \"bc5cdr\"\n",
    "ontology = \"medic\"\n",
    "dataset = \"ncbi_disease\"\n",
    "model = \"arboel\"\n",
    "abs_path = \"/home2/cye73/data_test\"\n",
    "data_path = os.path.join(abs_path, model, dataset)\n",
    "print(data_path)\n",
    "\n",
    "\n",
    "# ontology = \"MeSH\"\n",
    "# model = \"arboel\"\n",
    "# dataset = \"bc5cdr\"\n",
    "# abs_path = \"/home2/cye73/data\"\n",
    "# data_path = os.path.join(abs_path, model, dataset)\n",
    "# print(data_path)\n",
    "# abs_path2 = \"/home2/cye73/results\"\n",
    "# model_output_path = os.path.join(abs_path2, model, dataset)\n",
    "# ontology_type = \"umls\"\n",
    "# ontology_dir = \"/mitchell/entity-linking/2017AA/META/\"\n",
    "\n",
    "params_test = {\"data_path\" : data_path, \n",
    "               \"train_batch_size\" : 64,\n",
    "               \"max_context_length\": 128 ,\n",
    "               \"max_cand_length\" : 128 ,\n",
    "               \"context_key\" : \"context\",\n",
    "               \"debug\" : False,\n",
    "               \"knn\" : 4,\n",
    "               #\"bert_model\": 'michiyasunaga/BioLinkBERT-base',\n",
    "               \"bert_model\": \"dmis-lab/biobert-base-cased-v1.1\",\n",
    "               \"out_dim\": 768 ,\n",
    "               \"pull_from_layer\":11,\n",
    "               \"add_linear\":True,\n",
    "               \"use_types\" : True,\n",
    "               \"force_exact_search\" : True,\n",
    "               \"probe_mult_factor\" : 1,\n",
    "               \"embed_batch_size\" : 768,\n",
    "               \"drop_entities\" : False,\n",
    "               \"within_doc\" : True,\n",
    "               \"filter_unlabeled\" : True,\n",
    "               \"dataset\" : \"ncbi_disease\",\n",
    "               \"model\" : \"arboel\",\n",
    "               \"ontology_dir\" : '/mitchell/entity-linking/kbs/medic.tsv',\n",
    "               \"ontology\" : \"medic\"\n",
    "               }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LightningModule import LitArboel\n",
    "from LightningDataModule import ArboelDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = ArboelDataModule(params = params_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_data is being executed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:1454: FutureWarning: The repository for bigbio/ncbi_disease contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/bigbio/ncbi_disease\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stored processed entity dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 13189/13189 [00:00<00:00, 1882099.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max labels on one doc: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating correct mention format for train dataset: 100%|| 5065/5065 [00:00<00:00, 216188.04it/s]\n",
      "Creating correct mention format for validation dataset: 100%|| 780/780 [00:00<00:00, 278311.96it/s]\n",
      "Creating correct mention format for test dataset: 100%|| 960/960 [00:00<00:00, 360897.36it/s]\n"
     ]
    }
   ],
   "source": [
    "data_module.prepare_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup() is being executed\n",
      "Read 4782 samples.\n",
      "[25/Mar/2024 18:37:08] INFO - Read 4782 train samples..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing dictionary: 100%|| 13189/13189 [00:04<00:00, 3037.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25/Mar/2024 18:37:18] INFO - ====Processed samples: ====\n",
      "[25/Mar/2024 18:37:18] INFO - Context tokens : [CLS] identification of a ##p ##c ##2 , a ho ##mo ##logue of the [unused1] ad ##eno ##mat ##ous p ##oly ##po ##sis co ##li t ##umour [unused2] suppress ##or . the ad ##eno ##mat ##ous p ##oly ##po ##sis co ##li ( a ##p ##c ) t ##umour - suppress ##or protein controls the w ##nt signalling pathway by forming a complex with g ##ly ##co ##gen s ##ynth ##ase kinase 3 ##bet ##a ( g ##sk - 3 ##bet ##a ) , a ##xin / conduct ##in and beta ##cate ##nin . complex formation induce ##s the rapid degradation of beta ##cate ##nin . in co ##lon car ##cin ##oma cells , loss of a ##p ##c leads to the accumulation of beta ##cate ##nin [SEP]\n",
      "[25/Mar/2024 18:37:18] INFO - Context ids : 101 9117 1104 170 1643 1665 1477 117 170 16358 3702 12733 1104 1103 1 8050 26601 21943 2285 185 23415 5674 4863 1884 2646 189 27226 2 17203 1766 119 1103 8050 26601 21943 2285 185 23415 5674 4863 1884 2646 113 170 1643 1665 114 189 27226 118 17203 1766 4592 7451 1103 192 2227 25498 13548 1118 5071 170 2703 1114 176 1193 2528 4915 188 26588 6530 24779 124 16632 1161 113 176 5276 118 124 16632 1161 114 117 170 16594 120 5880 1394 1105 11933 20127 10430 119 2703 3855 21497 1116 1103 6099 18126 1104 11933 20127 10430 119 1107 1884 4934 1610 16430 7903 3652 117 2445 1104 170 1643 1665 4501 1106 1103 23168 1104 11933 20127 10430 102\n",
      "[25/Mar/2024 18:37:18] INFO - Label 299 tokens : [CLS] ad ##eno ##mat ##ous p ##oly ##po ##sis co ##li [unused3] ( cancer | dig ##estive system disease | genetic disease ( in ##born ) : a ##ap ##c , included | ad ##eno ##ma , per ##iam ##pu ##llar ##y , so ##matic , included | ad ##eno ##mat ##ous in ##test ##inal p ##oly ##pose ##s | ad ##eno ##mat ##ous in ##test ##inal p ##oly ##po ##sis | ad ##eno ##mat ##ous p ##oly ##pose ##s , f ##ami ##lial | ad ##eno ##mat ##ous p ##oly ##po ##sis co ##li , at ##ten ##uated , included | ad ##eno ##mat ##ous p ##oly ##po ##sis co ##li , f ##ami ##lial | ad ##eno ##mat ##ous p ##oly ##po ##sis co ##lus [SEP]\n",
      "[25/Mar/2024 18:37:18] INFO - Label 299 ids : 101 8050 26601 21943 2285 185 23415 5674 4863 1884 2646 3 113 4182 197 11902 23536 1449 3653 197 7434 3653 113 1107 7107 114 131 170 11478 1665 117 1529 197 8050 26601 1918 117 1679 18331 16091 12576 1183 117 1177 10734 117 1529 197 8050 26601 21943 2285 1107 13053 14196 185 23415 14811 1116 197 8050 26601 21943 2285 1107 13053 14196 185 23415 5674 4863 197 8050 26601 21943 2285 185 23415 14811 1116 117 175 11787 25737 197 8050 26601 21943 2285 185 23415 5674 4863 1884 2646 117 1120 5208 13567 117 1529 197 8050 26601 21943 2285 185 23415 5674 4863 1884 2646 117 175 11787 25737 197 8050 26601 21943 2285 185 23415 5674 4863 1884 5954 102\n",
      "[25/Mar/2024 18:37:18] INFO - Context tokens : [CLS] identification of a ##p ##c ##2 , a ho ##mo ##logue of the ad ##eno ##mat ##ous p ##oly ##po ##sis co ##li t ##umour suppress ##or . the [unused1] ad ##eno ##mat ##ous p ##oly ##po ##sis co ##li ( a ##p ##c ) t ##umour [unused2] - suppress ##or protein controls the w ##nt signalling pathway by forming a complex with g ##ly ##co ##gen s ##ynth ##ase kinase 3 ##bet ##a ( g ##sk - 3 ##bet ##a ) , a ##xin / conduct ##in and beta ##cate ##nin . complex formation induce ##s the rapid degradation of beta ##cate ##nin . in co ##lon car ##cin ##oma cells , loss of a ##p ##c leads to the accumulation of beta ##cate ##nin [SEP]\n",
      "[25/Mar/2024 18:37:18] INFO - Context ids : 101 9117 1104 170 1643 1665 1477 117 170 16358 3702 12733 1104 1103 8050 26601 21943 2285 185 23415 5674 4863 1884 2646 189 27226 17203 1766 119 1103 1 8050 26601 21943 2285 185 23415 5674 4863 1884 2646 113 170 1643 1665 114 189 27226 2 118 17203 1766 4592 7451 1103 192 2227 25498 13548 1118 5071 170 2703 1114 176 1193 2528 4915 188 26588 6530 24779 124 16632 1161 113 176 5276 118 124 16632 1161 114 117 170 16594 120 5880 1394 1105 11933 20127 10430 119 2703 3855 21497 1116 1103 6099 18126 1104 11933 20127 10430 119 1107 1884 4934 1610 16430 7903 3652 117 2445 1104 170 1643 1665 4501 1106 1103 23168 1104 11933 20127 10430 102\n",
      "[25/Mar/2024 18:37:18] INFO - Label 299 tokens : [CLS] ad ##eno ##mat ##ous p ##oly ##po ##sis co ##li [unused3] ( cancer | dig ##estive system disease | genetic disease ( in ##born ) : a ##ap ##c , included | ad ##eno ##ma , per ##iam ##pu ##llar ##y , so ##matic , included | ad ##eno ##mat ##ous in ##test ##inal p ##oly ##pose ##s | ad ##eno ##mat ##ous in ##test ##inal p ##oly ##po ##sis | ad ##eno ##mat ##ous p ##oly ##pose ##s , f ##ami ##lial | ad ##eno ##mat ##ous p ##oly ##po ##sis co ##li , at ##ten ##uated , included | ad ##eno ##mat ##ous p ##oly ##po ##sis co ##li , f ##ami ##lial | ad ##eno ##mat ##ous p ##oly ##po ##sis co ##lus [SEP]\n",
      "[25/Mar/2024 18:37:18] INFO - Label 299 ids : 101 8050 26601 21943 2285 185 23415 5674 4863 1884 2646 3 113 4182 197 11902 23536 1449 3653 197 7434 3653 113 1107 7107 114 131 170 11478 1665 117 1529 197 8050 26601 1918 117 1679 18331 16091 12576 1183 117 1177 10734 117 1529 197 8050 26601 21943 2285 1107 13053 14196 185 23415 14811 1116 197 8050 26601 21943 2285 1107 13053 14196 185 23415 5674 4863 197 8050 26601 21943 2285 185 23415 14811 1116 117 175 11787 25737 197 8050 26601 21943 2285 185 23415 5674 4863 1884 2646 117 1120 5208 13567 117 1529 197 8050 26601 21943 2285 185 23415 5674 4863 1884 2646 117 175 11787 25737 197 8050 26601 21943 2285 185 23415 5674 4863 1884 5954 102\n",
      "[25/Mar/2024 18:37:18] INFO - Context tokens : [CLS] ##umour - suppress ##or protein controls the w ##nt signalling pathway by forming a complex with g ##ly ##co ##gen s ##ynth ##ase kinase 3 ##bet ##a ( g ##sk - 3 ##bet ##a ) , a ##xin / conduct ##in and beta ##cate ##nin . complex formation induce ##s the rapid degradation of beta ##cate ##nin . in [unused1] co ##lon car ##cin ##oma [unused2] cells , loss of a ##p ##c leads to the accumulation of beta ##cate ##nin in the nucleus , where it binds to and activate ##s the t ##c ##f - 4 transcription factor ( reviewed in [ 1 ] [ 2 ] ) . here , we report the identification and g ##eno ##mic structure of a ##p ##c [SEP]\n",
      "[25/Mar/2024 18:37:18] INFO - Context ids : 101 27226 118 17203 1766 4592 7451 1103 192 2227 25498 13548 1118 5071 170 2703 1114 176 1193 2528 4915 188 26588 6530 24779 124 16632 1161 113 176 5276 118 124 16632 1161 114 117 170 16594 120 5880 1394 1105 11933 20127 10430 119 2703 3855 21497 1116 1103 6099 18126 1104 11933 20127 10430 119 1107 1 1884 4934 1610 16430 7903 2 3652 117 2445 1104 170 1643 1665 4501 1106 1103 23168 1104 11933 20127 10430 1107 1103 14297 117 1187 1122 23126 1106 1105 23162 1116 1103 189 1665 2087 118 125 15416 5318 113 7815 1107 164 122 166 164 123 166 114 119 1303 117 1195 2592 1103 9117 1105 176 26601 7257 2401 1104 170 1643 1665 102\n",
      "[25/Mar/2024 18:37:18] INFO - Label 2779 tokens : [CLS] co ##lon ##ic neo ##p ##las ##ms [unused3] ( cancer | dig ##estive system disease : ad ##eno ##car ##cin ##oma , co ##lon | ad ##eno ##car ##cin ##oma ##s , co ##lon | cancer , co ##lon | cancer , co ##lon ##ic | cancer of co ##lon | cancer of the co ##lon | cancer ##s , co ##lon | cancer ##s , co ##lon ##ic | co ##lon ad ##eno ##car ##cin ##oma | co ##lon ad ##eno ##car ##cin ##oma ##s | co ##lon cancer | co ##lon cancer ##s | co ##lon ##ic cancer | co ##lon ##ic cancer ##s | co ##lon ##ic neo ##p ##las ##m | co ##lon neo ##p ##las ##m | co ##lon neo ##p [SEP]\n",
      "[25/Mar/2024 18:37:18] INFO - Label 2779 ids : 101 1884 4934 1596 15242 1643 7580 4206 3 113 4182 197 11902 23536 1449 3653 131 8050 26601 8766 16430 7903 117 1884 4934 197 8050 26601 8766 16430 7903 1116 117 1884 4934 197 4182 117 1884 4934 197 4182 117 1884 4934 1596 197 4182 1104 1884 4934 197 4182 1104 1103 1884 4934 197 4182 1116 117 1884 4934 197 4182 1116 117 1884 4934 1596 197 1884 4934 8050 26601 8766 16430 7903 197 1884 4934 8050 26601 8766 16430 7903 1116 197 1884 4934 4182 197 1884 4934 4182 1116 197 1884 4934 1596 4182 197 1884 4934 1596 4182 1116 197 1884 4934 1596 15242 1643 7580 1306 197 1884 4934 15242 1643 7580 1306 197 1884 4934 15242 1643 102\n",
      "[25/Mar/2024 18:37:18] INFO - Context tokens : [CLS] . ma ##mmal ##ian a ##p ##c ##2 , which closely resembles a ##p ##c in overall domain structure , was functional ##ly analyzed and shown to contain two sa ##mp domains , both of which are required for binding to conduct ##in . like a ##p ##c , a ##p ##c ##2 regulate ##s the formation of active beta ##cate ##nin - t ##c ##f complexes , as demonstrated using trans ##ient transcription ##al activation ass ##ays in a ##p ##c - / - [unused1] co ##lon car ##cin ##oma [unused2] cells . human a ##p ##c ##2 maps to chromosome 19 ##p ##13 . 3 . a ##p ##c and a ##p ##c ##2 may therefore have comparable functions in development and cancer . [SEP]\n",
      "[25/Mar/2024 18:37:18] INFO - Context ids : 101 119 12477 27568 1811 170 1643 1665 1477 117 1134 4099 13198 170 1643 1665 1107 2905 5777 2401 117 1108 8458 1193 17689 1105 2602 1106 4651 1160 21718 8223 13770 117 1241 1104 1134 1132 2320 1111 7861 1106 5880 1394 119 1176 170 1643 1665 117 170 1643 1665 1477 16146 1116 1103 3855 1104 2327 11933 20127 10430 118 189 1665 2087 16575 117 1112 7160 1606 14715 9080 15416 1348 14915 3919 22979 1107 170 1643 1665 118 120 118 1 1884 4934 1610 16430 7903 2 3652 119 1769 170 1643 1665 1477 7415 1106 18697 1627 1643 17668 119 124 119 170 1643 1665 1105 170 1643 1665 1477 1336 3335 1138 12763 4226 1107 1718 1105 4182 119 102\n",
      "[25/Mar/2024 18:37:18] INFO - Label 2779 tokens : [CLS] co ##lon ##ic neo ##p ##las ##ms [unused3] ( cancer | dig ##estive system disease : ad ##eno ##car ##cin ##oma , co ##lon | ad ##eno ##car ##cin ##oma ##s , co ##lon | cancer , co ##lon | cancer , co ##lon ##ic | cancer of co ##lon | cancer of the co ##lon | cancer ##s , co ##lon | cancer ##s , co ##lon ##ic | co ##lon ad ##eno ##car ##cin ##oma | co ##lon ad ##eno ##car ##cin ##oma ##s | co ##lon cancer | co ##lon cancer ##s | co ##lon ##ic cancer | co ##lon ##ic cancer ##s | co ##lon ##ic neo ##p ##las ##m | co ##lon neo ##p ##las ##m | co ##lon neo ##p [SEP]\n",
      "[25/Mar/2024 18:37:18] INFO - Label 2779 ids : 101 1884 4934 1596 15242 1643 7580 4206 3 113 4182 197 11902 23536 1449 3653 131 8050 26601 8766 16430 7903 117 1884 4934 197 8050 26601 8766 16430 7903 1116 117 1884 4934 197 4182 117 1884 4934 197 4182 117 1884 4934 1596 197 4182 1104 1884 4934 197 4182 1104 1103 1884 4934 197 4182 1116 117 1884 4934 197 4182 1116 117 1884 4934 1596 197 1884 4934 8050 26601 8766 16430 7903 197 1884 4934 8050 26601 8766 16430 7903 1116 197 1884 4934 4182 197 1884 4934 4182 1116 197 1884 4934 1596 4182 197 1884 4934 1596 4182 1116 197 1884 4934 1596 15242 1643 7580 1306 197 1884 4934 15242 1643 7580 1306 197 1884 4934 15242 1643 102\n",
      "[25/Mar/2024 18:37:18] INFO - Context tokens : [CLS] . ma ##mmal ##ian a ##p ##c ##2 , which closely resembles a ##p ##c in overall domain structure , was functional ##ly analyzed and shown to contain two sa ##mp domains , both of which are required for binding to conduct ##in . like a ##p ##c , a ##p ##c ##2 regulate ##s the formation of active beta ##cate ##nin - t ##c ##f complexes , as demonstrated using trans ##ient transcription ##al activation ass ##ays in a ##p ##c - / - co ##lon car ##cin ##oma cells . human a ##p ##c ##2 maps to chromosome 19 ##p ##13 . 3 . a ##p ##c and a ##p ##c ##2 may therefore have comparable functions in development and [unused1] cancer [unused2] . [SEP]\n",
      "[25/Mar/2024 18:37:18] INFO - Context ids : 101 119 12477 27568 1811 170 1643 1665 1477 117 1134 4099 13198 170 1643 1665 1107 2905 5777 2401 117 1108 8458 1193 17689 1105 2602 1106 4651 1160 21718 8223 13770 117 1241 1104 1134 1132 2320 1111 7861 1106 5880 1394 119 1176 170 1643 1665 117 170 1643 1665 1477 16146 1116 1103 3855 1104 2327 11933 20127 10430 118 189 1665 2087 16575 117 1112 7160 1606 14715 9080 15416 1348 14915 3919 22979 1107 170 1643 1665 118 120 118 1884 4934 1610 16430 7903 3652 119 1769 170 1643 1665 1477 7415 1106 18697 1627 1643 17668 119 124 119 170 1643 1665 1105 170 1643 1665 1477 1336 3335 1138 12763 4226 1107 1718 1105 1 4182 2 119 102\n",
      "[25/Mar/2024 18:37:18] INFO - Label 8741 tokens : [CLS] neo ##p ##las ##ms [unused3] ( cancer : ben ##ign neo ##p ##las ##m | ben ##ign neo ##p ##las ##ms | cancer | cancer ##s | ma ##li ##gna ##ncies | ma ##li ##gna ##ncy | ma ##li ##gnant neo ##p ##las ##m | ma ##li ##gnant neo ##p ##las ##ms | neo ##p ##lasia | neo ##p ##lasia ##s | neo ##p ##las ##m | neo ##p ##las ##m , ben ##ign | neo ##p ##las ##m , ma ##li ##gnant | neo ##p ##las ##ms , ben ##ign | neo ##p ##las ##ms , ma ##li ##gnant | tumor | tumors ) [ new abnormal growth of tissue . ma ##li ##gnant neo ##p ##las ##ms show a greater degree of an ##ap [SEP]\n",
      "[25/Mar/2024 18:37:18] INFO - Label 8741 ids : 101 15242 1643 7580 4206 3 113 4182 131 26181 11368 15242 1643 7580 1306 197 26181 11368 15242 1643 7580 4206 197 4182 197 4182 1116 197 12477 2646 12149 9885 197 12477 2646 12149 7232 197 12477 2646 15454 15242 1643 7580 1306 197 12477 2646 15454 15242 1643 7580 4206 197 15242 1643 22992 197 15242 1643 22992 1116 197 15242 1643 7580 1306 197 15242 1643 7580 1306 117 26181 11368 197 15242 1643 7580 1306 117 12477 2646 15454 197 15242 1643 7580 4206 117 26181 11368 197 15242 1643 7580 4206 117 12477 2646 15454 197 14601 197 24309 114 164 1207 22832 3213 1104 7918 119 12477 2646 15454 15242 1643 7580 4206 1437 170 3407 2178 1104 1126 11478 102\n",
      "Saving entity dictionary...\n",
      "Saving processed train data...\n",
      "Read 722 samples.\n",
      "[25/Mar/2024 18:37:18] INFO - Read 722 valid samples..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing dictionary: 100%|| 13189/13189 [00:00<00:00, 1673331.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25/Mar/2024 18:37:19] INFO - ====Processed samples: ====\n",
      "[25/Mar/2024 18:37:19] INFO - Context tokens : [CLS] br ##ca ##1 is secret ##ed and exhibits properties of a g ##rani ##n . g ##er ##m ##line mutations in br ##ca ##1 are responsible for most cases of [unused1] inherited breast and o ##var ##ian cancer [unused2] . however , the function of the br ##ca ##1 protein has remained el ##usive . we now show that br ##ca ##1 en ##codes a 190 - k ##d protein with sequence ho ##mology and bio ##chemical analogy to the g ##rani ##n protein family . interesting ##ly , br ##ca ##2 also includes a motif similar to the g ##rani ##n consensus at the c terminus of the protein . both br ##ca ##1 and the g ##rani ##ns local ##ize to secret ##ory ve [SEP]\n",
      "[25/Mar/2024 18:37:19] INFO - Context ids : 101 9304 2599 1475 1110 3318 1174 1105 10877 4625 1104 170 176 23851 1179 119 176 1200 1306 2568 17157 1107 9304 2599 1475 1132 2784 1111 1211 2740 1104 1 7459 7209 1105 184 8997 1811 4182 2 119 1649 117 1103 3053 1104 1103 9304 2599 1475 4592 1144 1915 8468 17849 119 1195 1208 1437 1115 9304 2599 1475 4035 25634 170 11876 118 180 1181 4592 1114 4954 16358 19969 1105 25128 16710 26799 1106 1103 176 23851 1179 4592 1266 119 5426 1193 117 9304 2599 1477 1145 2075 170 17853 1861 1106 1103 176 23851 1179 10923 1120 1103 172 7132 1104 1103 4592 119 1241 9304 2599 1475 1105 1103 176 23851 2316 1469 3708 1106 3318 4649 1396 102\n",
      "[25/Mar/2024 18:37:19] INFO - Label 5655 tokens : [CLS] hereditary breast and o ##var ##ian cancer syndrome [unused3] ( cancer | end ##oc ##rine system disease | genetic disease ( in ##born ) | skin disease | u ##rogen ##ital disease ( female ) : h ##bo ##c syndrome | h ##bo ##c syndrome ##s | syndrome , h ##bo ##c | syndrome ##s , h ##bo ##c ) [ auto ##so ##mal dominant hereditary cancer syndrome in which a mutation most often in either br ##ca ##1 or br ##ca ##2 is associated with a significantly increased risk for breast and o ##var ##ian cancer ##s . ] [SEP]\n",
      "[25/Mar/2024 18:37:19] INFO - Label 5655 ids : 101 17676 7209 1105 184 8997 1811 4182 9318 3 113 4182 197 1322 13335 8643 1449 3653 197 7434 3653 113 1107 7107 114 197 2241 3653 197 190 26767 19058 3653 113 2130 114 131 177 4043 1665 9318 197 177 4043 1665 9318 1116 197 9318 117 177 4043 1665 197 9318 1116 117 177 4043 1665 114 164 12365 7301 7435 7065 17676 4182 9318 1107 1134 170 17895 1211 1510 1107 1719 9304 2599 1475 1137 9304 2599 1477 1110 2628 1114 170 5409 2569 3187 1111 7209 1105 184 8997 1811 4182 1116 119 166 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[25/Mar/2024 18:37:19] INFO - Context tokens : [CLS] ##ca ##1 en ##codes a 190 - k ##d protein with sequence ho ##mology and bio ##chemical analogy to the g ##rani ##n protein family . interesting ##ly , br ##ca ##2 also includes a motif similar to the g ##rani ##n consensus at the c terminus of the protein . both br ##ca ##1 and the g ##rani ##ns local ##ize to secret ##ory ve ##si ##cles , are secret ##ed by a regulated pathway , are post - translation ##ally g ##ly ##cos ##yla ##ted and are re ##sp ##ons ##ive to hormones . as a regulated secret ##ory protein , br ##ca ##1 appears to function by a mechanism not previously described for [unused1] t ##umour [unused2] suppress ##or gene products . . [SEP]\n",
      "[25/Mar/2024 18:37:19] INFO - Context ids : 101 2599 1475 4035 25634 170 11876 118 180 1181 4592 1114 4954 16358 19969 1105 25128 16710 26799 1106 1103 176 23851 1179 4592 1266 119 5426 1193 117 9304 2599 1477 1145 2075 170 17853 1861 1106 1103 176 23851 1179 10923 1120 1103 172 7132 1104 1103 4592 119 1241 9304 2599 1475 1105 1103 176 23851 2316 1469 3708 1106 3318 4649 1396 5053 17566 117 1132 3318 1174 1118 170 12521 13548 117 1132 2112 118 5179 2716 176 1193 13538 22948 1906 1105 1132 1231 20080 4199 2109 1106 23602 119 1112 170 12521 3318 4649 4592 117 9304 2599 1475 2691 1106 3053 1118 170 6978 1136 2331 1758 1111 1 189 27226 2 17203 1766 5565 2982 119 119 102\n",
      "[25/Mar/2024 18:37:19] INFO - Label 8741 tokens : [CLS] neo ##p ##las ##ms [unused3] ( cancer : ben ##ign neo ##p ##las ##m | ben ##ign neo ##p ##las ##ms | cancer | cancer ##s | ma ##li ##gna ##ncies | ma ##li ##gna ##ncy | ma ##li ##gnant neo ##p ##las ##m | ma ##li ##gnant neo ##p ##las ##ms | neo ##p ##lasia | neo ##p ##lasia ##s | neo ##p ##las ##m | neo ##p ##las ##m , ben ##ign | neo ##p ##las ##m , ma ##li ##gnant | neo ##p ##las ##ms , ben ##ign | neo ##p ##las ##ms , ma ##li ##gnant | tumor | tumors ) [ new abnormal growth of tissue . ma ##li ##gnant neo ##p ##las ##ms show a greater degree of an ##ap [SEP]\n",
      "[25/Mar/2024 18:37:19] INFO - Label 8741 ids : 101 15242 1643 7580 4206 3 113 4182 131 26181 11368 15242 1643 7580 1306 197 26181 11368 15242 1643 7580 4206 197 4182 197 4182 1116 197 12477 2646 12149 9885 197 12477 2646 12149 7232 197 12477 2646 15454 15242 1643 7580 1306 197 12477 2646 15454 15242 1643 7580 4206 197 15242 1643 22992 197 15242 1643 22992 1116 197 15242 1643 7580 1306 197 15242 1643 7580 1306 117 26181 11368 197 15242 1643 7580 1306 117 12477 2646 15454 197 15242 1643 7580 4206 117 26181 11368 197 15242 1643 7580 4206 117 12477 2646 15454 197 14601 197 24309 114 164 1207 22832 3213 1104 7918 119 12477 2646 15454 15242 1643 7580 4206 1437 170 3407 2178 1104 1126 11478 102\n",
      "[25/Mar/2024 18:37:19] INFO - Context tokens : [CLS] [unused1] o ##var ##ian cancer [unused2] risk in br ##ca ##1 carriers is modified by the h ##ras ##1 variable number of tandem repeat ( v ##nt ##r ) lo ##cus . women who carry a mutation in the br ##ca ##1 gene ( on chromosome 17 ##q ##21 ) , have an 80 % risk of breast cancer and a 40 % risk of o ##var ##ian cancer by the age of 70 ( re ##f . 1 ) . the variable pen ##et ##rance of br ##ca ##1 suggests that other genetic and non - genetic factors play a role in t ##umour ##ige ##nes ##is in these individuals . the h ##ras ##1 variable number of tandem repeats ( v ##nt ##r ) [SEP]\n",
      "[25/Mar/2024 18:37:19] INFO - Context ids : 101 1 184 8997 1811 4182 2 3187 1107 9304 2599 1475 11837 1110 5847 1118 1103 177 7297 1475 7898 1295 1104 22090 9488 113 191 2227 1197 114 25338 6697 119 1535 1150 3564 170 17895 1107 1103 9304 2599 1475 5565 113 1113 18697 1542 4426 18202 114 117 1138 1126 2908 110 3187 1104 7209 4182 1105 170 1969 110 3187 1104 184 8997 1811 4182 1118 1103 1425 1104 3102 113 1231 2087 119 122 114 119 1103 7898 8228 2105 10555 1104 9304 2599 1475 5401 1115 1168 7434 1105 1664 118 7434 5320 1505 170 1648 1107 189 27226 13417 3965 1548 1107 1292 2833 119 1103 177 7297 1475 7898 1295 1104 22090 19811 113 191 2227 1197 114 102\n",
      "[25/Mar/2024 18:37:19] INFO - Label 9491 tokens : [CLS] o ##var ##ian neo ##p ##las ##ms [unused3] ( cancer | end ##oc ##rine system disease | u ##rogen ##ital disease ( female ) : cancer of o ##vary | cancer of the o ##vary | cancer , o ##var ##ian | cancer , o ##vary | cancer ##s , o ##var ##ian | cancer ##s , o ##vary | neo ##p ##las ##m , o ##var ##ian | neo ##p ##las ##m , o ##vary | neo ##p ##las ##ms , o ##var ##ian | neo ##p ##las ##ms , o ##vary | o ##var ##ian cancer | o ##var ##ian cancer , e ##pit ##hel ##ial , included | o ##var ##ian cancer ##s | o ##var ##ian cancer , su ##s ##ce ##pt [SEP]\n",
      "[25/Mar/2024 18:37:19] INFO - Label 9491 ids : 101 184 8997 1811 15242 1643 7580 4206 3 113 4182 197 1322 13335 8643 1449 3653 197 190 26767 19058 3653 113 2130 114 131 4182 1104 184 24022 197 4182 1104 1103 184 24022 197 4182 117 184 8997 1811 197 4182 117 184 24022 197 4182 1116 117 184 8997 1811 197 4182 1116 117 184 24022 197 15242 1643 7580 1306 117 184 8997 1811 197 15242 1643 7580 1306 117 184 24022 197 15242 1643 7580 4206 117 184 8997 1811 197 15242 1643 7580 4206 117 184 24022 197 184 8997 1811 4182 197 184 8997 1811 4182 117 174 18965 18809 2916 117 1529 197 184 8997 1811 4182 1116 197 184 8997 1811 4182 117 28117 1116 2093 6451 102\n",
      "[25/Mar/2024 18:37:19] INFO - Context tokens : [CLS] o ##var ##ian cancer risk in br ##ca ##1 carriers is modified by the h ##ras ##1 variable number of tandem repeat ( v ##nt ##r ) lo ##cus . women who carry a mutation in the br ##ca ##1 gene ( on chromosome 17 ##q ##21 ) , have an 80 % risk of [unused1] breast cancer [unused2] and a 40 % risk of o ##var ##ian cancer by the age of 70 ( re ##f . 1 ) . the variable pen ##et ##rance of br ##ca ##1 suggests that other genetic and non - genetic factors play a role in t ##umour ##ige ##nes ##is in these individuals . the h ##ras ##1 variable number of tandem repeats ( v ##nt ##r ) [SEP]\n",
      "[25/Mar/2024 18:37:19] INFO - Context ids : 101 184 8997 1811 4182 3187 1107 9304 2599 1475 11837 1110 5847 1118 1103 177 7297 1475 7898 1295 1104 22090 9488 113 191 2227 1197 114 25338 6697 119 1535 1150 3564 170 17895 1107 1103 9304 2599 1475 5565 113 1113 18697 1542 4426 18202 114 117 1138 1126 2908 110 3187 1104 1 7209 4182 2 1105 170 1969 110 3187 1104 184 8997 1811 4182 1118 1103 1425 1104 3102 113 1231 2087 119 122 114 119 1103 7898 8228 2105 10555 1104 9304 2599 1475 5401 1115 1168 7434 1105 1664 118 7434 5320 1505 170 1648 1107 189 27226 13417 3965 1548 1107 1292 2833 119 1103 177 7297 1475 7898 1295 1104 22090 19811 113 191 2227 1197 114 102\n",
      "[25/Mar/2024 18:37:19] INFO - Label 1613 tokens : [CLS] breast neo ##p ##las ##ms [unused3] ( cancer | skin disease : breast cancer | breast cancer , f ##ami ##lial breast cancer , f ##ami ##lial male , included | breast car ##cin ##oma | breast car ##cin ##oma ##s | breast ma ##li ##gnant neo ##p ##las ##m | breast ma ##li ##gnant neo ##p ##las ##ms | breast ma ##li ##gnant tumor | breast ma ##li ##gnant tumors | breast neo ##p ##las ##m | breast tumor | breast tumors | cancer , breast | cancer , ma ##mma ##ry | cancer of breast | cancer of the breast | cancer ##s , ma ##mma ##ry | car ##cin ##oma , breast | car ##cin ##oma , human ma ##mma ##ry | car [SEP]\n",
      "[25/Mar/2024 18:37:19] INFO - Label 1613 ids : 101 7209 15242 1643 7580 4206 3 113 4182 197 2241 3653 131 7209 4182 197 7209 4182 117 175 11787 25737 7209 4182 117 175 11787 25737 2581 117 1529 197 7209 1610 16430 7903 197 7209 1610 16430 7903 1116 197 7209 12477 2646 15454 15242 1643 7580 1306 197 7209 12477 2646 15454 15242 1643 7580 4206 197 7209 12477 2646 15454 14601 197 7209 12477 2646 15454 24309 197 7209 15242 1643 7580 1306 197 7209 14601 197 7209 24309 197 4182 117 7209 197 4182 117 12477 12917 1616 197 4182 1104 7209 197 4182 1104 1103 7209 197 4182 1116 117 12477 12917 1616 197 1610 16430 7903 117 7209 197 1610 16430 7903 117 1769 12477 12917 1616 197 1610 102\n",
      "[25/Mar/2024 18:37:19] INFO - Context tokens : [CLS] cancer risk in br ##ca ##1 carriers is modified by the h ##ras ##1 variable number of tandem repeat ( v ##nt ##r ) lo ##cus . women who carry a mutation in the br ##ca ##1 gene ( on chromosome 17 ##q ##21 ) , have an 80 % risk of breast cancer and a 40 % risk of [unused1] o ##var ##ian cancer [unused2] by the age of 70 ( re ##f . 1 ) . the variable pen ##et ##rance of br ##ca ##1 suggests that other genetic and non - genetic factors play a role in t ##umour ##ige ##nes ##is in these individuals . the h ##ras ##1 variable number of tandem repeats ( v ##nt ##r ) p ##oly ##mor [SEP]\n",
      "[25/Mar/2024 18:37:19] INFO - Context ids : 101 4182 3187 1107 9304 2599 1475 11837 1110 5847 1118 1103 177 7297 1475 7898 1295 1104 22090 9488 113 191 2227 1197 114 25338 6697 119 1535 1150 3564 170 17895 1107 1103 9304 2599 1475 5565 113 1113 18697 1542 4426 18202 114 117 1138 1126 2908 110 3187 1104 7209 4182 1105 170 1969 110 3187 1104 1 184 8997 1811 4182 2 1118 1103 1425 1104 3102 113 1231 2087 119 122 114 119 1103 7898 8228 2105 10555 1104 9304 2599 1475 5401 1115 1168 7434 1105 1664 118 7434 5320 1505 170 1648 1107 189 27226 13417 3965 1548 1107 1292 2833 119 1103 177 7297 1475 7898 1295 1104 22090 19811 113 191 2227 1197 114 185 23415 26271 102\n",
      "[25/Mar/2024 18:37:19] INFO - Label 9491 tokens : [CLS] o ##var ##ian neo ##p ##las ##ms [unused3] ( cancer | end ##oc ##rine system disease | u ##rogen ##ital disease ( female ) : cancer of o ##vary | cancer of the o ##vary | cancer , o ##var ##ian | cancer , o ##vary | cancer ##s , o ##var ##ian | cancer ##s , o ##vary | neo ##p ##las ##m , o ##var ##ian | neo ##p ##las ##m , o ##vary | neo ##p ##las ##ms , o ##var ##ian | neo ##p ##las ##ms , o ##vary | o ##var ##ian cancer | o ##var ##ian cancer , e ##pit ##hel ##ial , included | o ##var ##ian cancer ##s | o ##var ##ian cancer , su ##s ##ce ##pt [SEP]\n",
      "[25/Mar/2024 18:37:19] INFO - Label 9491 ids : 101 184 8997 1811 15242 1643 7580 4206 3 113 4182 197 1322 13335 8643 1449 3653 197 190 26767 19058 3653 113 2130 114 131 4182 1104 184 24022 197 4182 1104 1103 184 24022 197 4182 117 184 8997 1811 197 4182 117 184 24022 197 4182 1116 117 184 8997 1811 197 4182 1116 117 184 24022 197 15242 1643 7580 1306 117 184 8997 1811 197 15242 1643 7580 1306 117 184 24022 197 15242 1643 7580 4206 117 184 8997 1811 197 15242 1643 7580 4206 117 184 24022 197 184 8997 1811 4182 197 184 8997 1811 4182 117 174 18965 18809 2916 117 1529 197 184 8997 1811 4182 1116 197 184 8997 1811 4182 117 28117 1116 2093 6451 102\n",
      "Saving processed valid data...\n",
      "Read 879 samples.\n",
      "[25/Mar/2024 18:37:19] INFO - Read 879 test samples..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing dictionary: 100%|| 13189/13189 [00:00<00:00, 1851671.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25/Mar/2024 18:37:20] INFO - ====Processed samples: ====\n",
      "[25/Mar/2024 18:37:20] INFO - Context tokens : [CLS] cluster ##ing of miss ##ense mutations in the [unused1] at ##ax ##ia - te ##lang ##ie ##ct ##asi ##a [unused2] gene in a s ##poradic t - cell le ##uka ##emia . at ##ax ##ia - te ##lang ##ie ##ct ##asi ##a ( a - t ) is a re ##cess ##ive multi - system disorder caused by mutations in the at ##m gene at 11 ##q ##22 - q ##23 ( re ##f . 3 ) . the risk of cancer , especially l ##ymph ##oid neo ##p ##lasia ##s , is substantially elevated in a - t patients and has long been associated with ch ##rom ##oso ##mal instability . by anal ##ys ##ing t ##umour d ##na from patients with s ##poradic t [SEP]\n",
      "[25/Mar/2024 18:37:20] INFO - Context ids : 101 10005 1158 1104 5529 22615 17157 1107 1103 1 1120 7897 1465 118 21359 19514 1663 5822 17506 1161 2 5565 1107 170 188 27695 189 118 2765 5837 12658 20504 119 1120 7897 1465 118 21359 19514 1663 5822 17506 1161 113 170 118 189 114 1110 170 1231 22371 2109 4321 118 1449 8936 2416 1118 17157 1107 1103 1120 1306 5565 1120 1429 4426 20581 118 186 22737 113 1231 2087 119 124 114 119 1103 3187 1104 4182 117 2108 181 25698 7874 15242 1643 22992 1116 117 1110 12613 8208 1107 170 118 189 4420 1105 1144 1263 1151 2628 1114 22572 16071 22354 7435 20482 119 1118 24443 6834 1158 189 27226 173 1605 1121 4420 1114 188 27695 189 102\n",
      "[25/Mar/2024 18:37:20] INFO - Label 1056 tokens : [CLS] at ##ax ##ia te ##lang ##ie ##ct ##asi ##a [unused3] ( card ##iovascular disease | genetic disease ( in ##born ) | immune system disease | metabolic disease | nervous system disease : at | at ##1 | at ##a , included | at ##ax ##ia - te ##lang ##ie ##ct ##asi ##a | at ##ax ##ia te ##lang ##ie ##ct ##asi ##a syndrome | at ##ax ##ia - te ##lang ##ie ##ct ##asi ##a variant , included | at ##c , included | at , complement ##ation group c , included | at , complement ##ation group d , included | at , complement ##ation group e , included | at ##d , included | ate , included | lo ##ui ##s bar syndrome | [SEP]\n",
      "[25/Mar/2024 18:37:20] INFO - Label 1056 ids : 101 1120 7897 1465 21359 19514 1663 5822 17506 1161 3 113 3621 25575 3653 197 7434 3653 113 1107 7107 114 197 11650 1449 3653 197 25158 3653 197 5604 1449 3653 131 1120 197 1120 1475 197 1120 1161 117 1529 197 1120 7897 1465 118 21359 19514 1663 5822 17506 1161 197 1120 7897 1465 21359 19514 1663 5822 17506 1161 9318 197 1120 7897 1465 118 21359 19514 1663 5822 17506 1161 8120 117 1529 197 1120 1665 117 1529 197 1120 117 14348 1891 1372 172 117 1529 197 1120 117 14348 1891 1372 173 117 1529 197 1120 117 14348 1891 1372 174 117 1529 197 1120 1181 117 1529 197 8756 117 1529 197 25338 6592 1116 2927 9318 197 102\n",
      "[25/Mar/2024 18:37:20] INFO - Context tokens : [CLS] cluster ##ing of miss ##ense mutations in the at ##ax ##ia - te ##lang ##ie ##ct ##asi ##a gene in a [unused1] s ##poradic t - cell le ##uka ##emia [unused2] . at ##ax ##ia - te ##lang ##ie ##ct ##asi ##a ( a - t ) is a re ##cess ##ive multi - system disorder caused by mutations in the at ##m gene at 11 ##q ##22 - q ##23 ( re ##f . 3 ) . the risk of cancer , especially l ##ymph ##oid neo ##p ##lasia ##s , is substantially elevated in a - t patients and has long been associated with ch ##rom ##oso ##mal instability . by anal ##ys ##ing t ##umour d ##na from patients with s ##poradic t [SEP]\n",
      "[25/Mar/2024 18:37:20] INFO - Context ids : 101 10005 1158 1104 5529 22615 17157 1107 1103 1120 7897 1465 118 21359 19514 1663 5822 17506 1161 5565 1107 170 1 188 27695 189 118 2765 5837 12658 20504 2 119 1120 7897 1465 118 21359 19514 1663 5822 17506 1161 113 170 118 189 114 1110 170 1231 22371 2109 4321 118 1449 8936 2416 1118 17157 1107 1103 1120 1306 5565 1120 1429 4426 20581 118 186 22737 113 1231 2087 119 124 114 119 1103 3187 1104 4182 117 2108 181 25698 7874 15242 1643 22992 1116 117 1110 12613 8208 1107 170 118 189 4420 1105 1144 1263 1151 2628 1114 22572 16071 22354 7435 20482 119 1118 24443 6834 1158 189 27226 173 1605 1121 4420 1114 188 27695 189 102\n",
      "[25/Mar/2024 18:37:20] INFO - Label 7142 tokens : [CLS] le ##uke ##mia , t - cell [unused3] ( cancer | immune system disease | l ##ymph ##atic disease : le ##uke ##mia , l ##ymph ##oc ##ytic , t cell | le ##uke ##mia , l ##ymph ##oc ##ytic , t - cell | le ##uke ##mia ##s , t - cell | le ##uke ##mia ##s , t - cell l ##ymph ##oc ##ytic | le ##uke ##mia ##s , t l ##ymph ##oc ##ytic | le ##uke ##mia ##s , t - l ##ymph ##oc ##ytic | le ##uke ##mia , t cell | le ##uke ##mia , t - cell l ##ymph ##oc ##ytic | le ##uke ##mia , t l ##ymph ##oc ##ytic | le ##uke ##mia , t - l [SEP]\n",
      "[25/Mar/2024 18:37:20] INFO - Label 7142 ids : 101 5837 16140 8191 117 189 118 2765 3 113 4182 197 11650 1449 3653 197 181 25698 7698 3653 131 5837 16140 8191 117 181 25698 13335 23894 117 189 2765 197 5837 16140 8191 117 181 25698 13335 23894 117 189 118 2765 197 5837 16140 8191 1116 117 189 118 2765 197 5837 16140 8191 1116 117 189 118 2765 181 25698 13335 23894 197 5837 16140 8191 1116 117 189 181 25698 13335 23894 197 5837 16140 8191 1116 117 189 118 181 25698 13335 23894 197 5837 16140 8191 117 189 2765 197 5837 16140 8191 117 189 118 2765 181 25698 13335 23894 197 5837 16140 8191 117 189 181 25698 13335 23894 197 5837 16140 8191 117 189 118 181 102\n",
      "[25/Mar/2024 18:37:20] INFO - Context tokens : [CLS] cluster ##ing of miss ##ense mutations in the at ##ax ##ia - te ##lang ##ie ##ct ##asi ##a gene in a s ##poradic t - cell le ##uka ##emia . [unused1] at ##ax ##ia - te ##lang ##ie ##ct ##asi ##a [unused2] ( a - t ) is a re ##cess ##ive multi - system disorder caused by mutations in the at ##m gene at 11 ##q ##22 - q ##23 ( re ##f . 3 ) . the risk of cancer , especially l ##ymph ##oid neo ##p ##lasia ##s , is substantially elevated in a - t patients and has long been associated with ch ##rom ##oso ##mal instability . by anal ##ys ##ing t ##umour d ##na from patients with s ##poradic t [SEP]\n",
      "[25/Mar/2024 18:37:20] INFO - Context ids : 101 10005 1158 1104 5529 22615 17157 1107 1103 1120 7897 1465 118 21359 19514 1663 5822 17506 1161 5565 1107 170 188 27695 189 118 2765 5837 12658 20504 119 1 1120 7897 1465 118 21359 19514 1663 5822 17506 1161 2 113 170 118 189 114 1110 170 1231 22371 2109 4321 118 1449 8936 2416 1118 17157 1107 1103 1120 1306 5565 1120 1429 4426 20581 118 186 22737 113 1231 2087 119 124 114 119 1103 3187 1104 4182 117 2108 181 25698 7874 15242 1643 22992 1116 117 1110 12613 8208 1107 170 118 189 4420 1105 1144 1263 1151 2628 1114 22572 16071 22354 7435 20482 119 1118 24443 6834 1158 189 27226 173 1605 1121 4420 1114 188 27695 189 102\n",
      "[25/Mar/2024 18:37:20] INFO - Label 1056 tokens : [CLS] at ##ax ##ia te ##lang ##ie ##ct ##asi ##a [unused3] ( card ##iovascular disease | genetic disease ( in ##born ) | immune system disease | metabolic disease | nervous system disease : at | at ##1 | at ##a , included | at ##ax ##ia - te ##lang ##ie ##ct ##asi ##a | at ##ax ##ia te ##lang ##ie ##ct ##asi ##a syndrome | at ##ax ##ia - te ##lang ##ie ##ct ##asi ##a variant , included | at ##c , included | at , complement ##ation group c , included | at , complement ##ation group d , included | at , complement ##ation group e , included | at ##d , included | ate , included | lo ##ui ##s bar syndrome | [SEP]\n",
      "[25/Mar/2024 18:37:20] INFO - Label 1056 ids : 101 1120 7897 1465 21359 19514 1663 5822 17506 1161 3 113 3621 25575 3653 197 7434 3653 113 1107 7107 114 197 11650 1449 3653 197 25158 3653 197 5604 1449 3653 131 1120 197 1120 1475 197 1120 1161 117 1529 197 1120 7897 1465 118 21359 19514 1663 5822 17506 1161 197 1120 7897 1465 21359 19514 1663 5822 17506 1161 9318 197 1120 7897 1465 118 21359 19514 1663 5822 17506 1161 8120 117 1529 197 1120 1665 117 1529 197 1120 117 14348 1891 1372 172 117 1529 197 1120 117 14348 1891 1372 173 117 1529 197 1120 117 14348 1891 1372 174 117 1529 197 1120 1181 117 1529 197 8756 117 1529 197 25338 6592 1116 2927 9318 197 102\n",
      "[25/Mar/2024 18:37:20] INFO - Context tokens : [CLS] cluster ##ing of miss ##ense mutations in the at ##ax ##ia - te ##lang ##ie ##ct ##asi ##a gene in a s ##poradic t - cell le ##uka ##emia . at ##ax ##ia - te ##lang ##ie ##ct ##asi ##a ( [unused1] a - t [unused2] ) is a re ##cess ##ive multi - system disorder caused by mutations in the at ##m gene at 11 ##q ##22 - q ##23 ( re ##f . 3 ) . the risk of cancer , especially l ##ymph ##oid neo ##p ##lasia ##s , is substantially elevated in a - t patients and has long been associated with ch ##rom ##oso ##mal instability . by anal ##ys ##ing t ##umour d ##na from patients with s ##poradic t [SEP]\n",
      "[25/Mar/2024 18:37:20] INFO - Context ids : 101 10005 1158 1104 5529 22615 17157 1107 1103 1120 7897 1465 118 21359 19514 1663 5822 17506 1161 5565 1107 170 188 27695 189 118 2765 5837 12658 20504 119 1120 7897 1465 118 21359 19514 1663 5822 17506 1161 113 1 170 118 189 2 114 1110 170 1231 22371 2109 4321 118 1449 8936 2416 1118 17157 1107 1103 1120 1306 5565 1120 1429 4426 20581 118 186 22737 113 1231 2087 119 124 114 119 1103 3187 1104 4182 117 2108 181 25698 7874 15242 1643 22992 1116 117 1110 12613 8208 1107 170 118 189 4420 1105 1144 1263 1151 2628 1114 22572 16071 22354 7435 20482 119 1118 24443 6834 1158 189 27226 173 1605 1121 4420 1114 188 27695 189 102\n",
      "[25/Mar/2024 18:37:20] INFO - Label 1056 tokens : [CLS] at ##ax ##ia te ##lang ##ie ##ct ##asi ##a [unused3] ( card ##iovascular disease | genetic disease ( in ##born ) | immune system disease | metabolic disease | nervous system disease : at | at ##1 | at ##a , included | at ##ax ##ia - te ##lang ##ie ##ct ##asi ##a | at ##ax ##ia te ##lang ##ie ##ct ##asi ##a syndrome | at ##ax ##ia - te ##lang ##ie ##ct ##asi ##a variant , included | at ##c , included | at , complement ##ation group c , included | at , complement ##ation group d , included | at , complement ##ation group e , included | at ##d , included | ate , included | lo ##ui ##s bar syndrome | [SEP]\n",
      "[25/Mar/2024 18:37:20] INFO - Label 1056 ids : 101 1120 7897 1465 21359 19514 1663 5822 17506 1161 3 113 3621 25575 3653 197 7434 3653 113 1107 7107 114 197 11650 1449 3653 197 25158 3653 197 5604 1449 3653 131 1120 197 1120 1475 197 1120 1161 117 1529 197 1120 7897 1465 118 21359 19514 1663 5822 17506 1161 197 1120 7897 1465 21359 19514 1663 5822 17506 1161 9318 197 1120 7897 1465 118 21359 19514 1663 5822 17506 1161 8120 117 1529 197 1120 1665 117 1529 197 1120 117 14348 1891 1372 172 117 1529 197 1120 117 14348 1891 1372 173 117 1529 197 1120 117 14348 1891 1372 174 117 1529 197 1120 1181 117 1529 197 8756 117 1529 197 25338 6592 1116 2927 9318 197 102\n",
      "[25/Mar/2024 18:37:20] INFO - Context tokens : [CLS] cluster ##ing of miss ##ense mutations in the at ##ax ##ia - te ##lang ##ie ##ct ##asi ##a gene in a s ##poradic t - cell le ##uka ##emia . at ##ax ##ia - te ##lang ##ie ##ct ##asi ##a ( a - t ) is a [unused1] re ##cess ##ive multi - system disorder [unused2] caused by mutations in the at ##m gene at 11 ##q ##22 - q ##23 ( re ##f . 3 ) . the risk of cancer , especially l ##ymph ##oid neo ##p ##lasia ##s , is substantially elevated in a - t patients and has long been associated with ch ##rom ##oso ##mal instability . by anal ##ys ##ing t ##umour d ##na from patients with s ##poradic t [SEP]\n",
      "[25/Mar/2024 18:37:20] INFO - Context ids : 101 10005 1158 1104 5529 22615 17157 1107 1103 1120 7897 1465 118 21359 19514 1663 5822 17506 1161 5565 1107 170 188 27695 189 118 2765 5837 12658 20504 119 1120 7897 1465 118 21359 19514 1663 5822 17506 1161 113 170 118 189 114 1110 170 1 1231 22371 2109 4321 118 1449 8936 2 2416 1118 17157 1107 1103 1120 1306 5565 1120 1429 4426 20581 118 186 22737 113 1231 2087 119 124 114 119 1103 3187 1104 4182 117 2108 181 25698 7874 15242 1643 22992 1116 117 1110 12613 8208 1107 170 118 189 4420 1105 1144 1263 1151 2628 1114 22572 16071 22354 7435 20482 119 1118 24443 6834 1158 189 27226 173 1605 1121 4420 1114 188 27695 189 102\n",
      "[25/Mar/2024 18:37:20] INFO - Label 5087 tokens : [CLS] genetic diseases , in ##born [unused3] ( genetic disease ( in ##born ) : defect , single - gene | defects , single - gene | disease , genetic | disease , hereditary | disease , in ##born genetic | diseases , genetic | diseases , hereditary | diseases , in ##born genetic | disorder , genetic | disorders , genetic | genetic disease | genetic disease , in ##born | genetic diseases | genetic disorder | genetic disorders | hereditary disease | hereditary diseases | in ##born genetic disease | in ##born genetic diseases | single - gene defect | single gene defects | single - gene defects ) [ diseases that are caused by genetic mutations present during em ##b ##ryo or f ##etal [SEP]\n",
      "[25/Mar/2024 18:37:20] INFO - Label 5087 ids : 101 7434 8131 117 1107 7107 3 113 7434 3653 113 1107 7107 114 131 23912 117 1423 118 5565 197 20705 117 1423 118 5565 197 3653 117 7434 197 3653 117 17676 197 3653 117 1107 7107 7434 197 8131 117 7434 197 8131 117 17676 197 8131 117 1107 7107 7434 197 8936 117 7434 197 11759 117 7434 197 7434 3653 197 7434 3653 117 1107 7107 197 7434 8131 197 7434 8936 197 7434 11759 197 17676 3653 197 17676 8131 197 1107 7107 7434 3653 197 1107 7107 7434 8131 197 1423 118 5565 23912 197 1423 5565 20705 197 1423 118 5565 20705 114 164 8131 1115 1132 2416 1118 7434 17157 1675 1219 9712 1830 26503 1137 175 21470 102\n",
      "Saving processed test data...\n",
      "[25/Mar/2024 18:37:20] INFO - within_doc\n"
     ]
    }
   ],
   "source": [
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4782"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module.train_tensor_data\n",
    "len(data_module.train_tensor_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataloader() is being executed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Load the train DataLoader\n",
    "train_dataloader = data_module.train_dataloader()\n",
    "assert next(iter(train_dataloader)) is not None, \"Training DataLoader is empty!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "mentions_id = []\n",
    "for batch in train_dataloader:\n",
    "    # batch_context_inputs, candidate_idxs, n_gold, mention_idxs = batch\n",
    "    # mentions_id.append(max(mention_idxs))\n",
    "    first_batch = batch\n",
    "    #print(len(batch[0]))\n",
    "    break  # Exit the loop after the first iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4781)\n"
     ]
    }
   ],
   "source": [
    "# print(max(mentions_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_context_inputs, candidate_idxs, n_gold, mention_idxs = first_batch\n",
    "print(\"batch_context_inputs: Indices of the 64 tokens of the first element (mention + context)\\n\", batch_context_inputs[0])\n",
    "print(\"\\ncandidate_idxs: Indices of the correct_entity\\n\", candidate_idxs[0])\n",
    "print(\"\\nn_gold: Number of correct entity\\n\", n_gold[0])\n",
    "print(\"\\nmention_idxs: Unique mention identifier id\\n\", mention_idxs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "722"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_module.valid_tensor_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "64\n",
      "64\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# Load the valid DataLoader\n",
    "val_dataloader = data_module.val_dataloader()\n",
    "for batch in val_dataloader:\n",
    "    print(len(batch[0]))\n",
    "    #break  # Exit the loop after the first iteration\n",
    "\n",
    "# batch_context_inputs, candidate_idxs, n_gold, mention_idxs = first_batch\n",
    "# print(\"batch_context_inputs: Indices of the 64 tokens of the first element (mention + context)\\n\", batch_context_inputs[:2])\n",
    "# print(\"\\ncandidate_idxs: Indices of the correct_entity\\n\", candidate_idxs[:3])\n",
    "# print(\"\\nn_gold: Number of correct entity\\n\", n_gold)\n",
    "# print(\"\\nmention_idxs: Unique mention identifier id\\n\", mention_idxs[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "878"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_module.test_tensor_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "# Load the test DataLoader\n",
    "test_dataloader = data_module.test_dataloader()\n",
    "for batch in test_dataloader:\n",
    "    print(len(batch[0]))\n",
    "    #break  # Exit the loop after the first iteration\n",
    "\n",
    "# batch_context_inputs, candidate_idxs, n_gold, mention_idxs = first_batch\n",
    "# print(\"batch_context_inputs: Indices of the 64 tokens of the first element (mention + context)\\n\", batch_context_inputs[:2])\n",
    "# print(\"\\ncandidate_idxs: Indices of the correct_entity\\n\", candidate_idxs[:3])\n",
    "# print(\"\\nn_gold: Number of correct entity\\n\", n_gold)\n",
    "# print(\"\\nmention_idxs: Unique mention identifier id\\n\", mention_idxs[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests my ncbi_disease vs David's one after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict :\n",
      " {'type': 'Disease', 'cui': 'MESH:C579850', 'title': '16p11.2 Deletion Syndrome', 'cuis': ['MESH:C579850'], 'description': '16p11.2 Deletion Syndrome ( Disease)'}\n",
      "dict2 :\n",
      " {'type': 'Disease', 'cui': 'MESH:C579850', 'title': '16p11.2 Deletion Syndrome', 'cuis': ['MESH:C579850'], 'description': '16p11.2 Deletion Syndrome ( Disease :  )'}\n"
     ]
    }
   ],
   "source": [
    "# path_entity = '/home2/cye73/data_test2/arboel/ncbi_disease/dictionary.pickle'\n",
    "path_entity = '/home2/cye73/data/arboel/ncbi_disease/dictionary.pickle'\n",
    "path_entity2 = '/home2/cye73/arboEL2/data/arboel/ncbi_disease/dictionary.pickle'\n",
    "with open(path_entity, 'rb') as read_handle:\n",
    "    dict = pickle.load(read_handle)\n",
    "with open(path_entity2, 'rb') as read_handle:\n",
    "    dict2 = pickle.load(read_handle)\n",
    "\n",
    "print(\"dict :\\n\", dict[3])\n",
    "print(\"dict2 :\\n\", dict2[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_entity = '/home2/cye73/data_test2/arboel/ncbi_disease/entity_dictionary.pickle'\n",
    "# path_entity = '/home2/cye73/data/arboel/ncbi_disease/entity_dictionary.pickle'\n",
    "path_entity2 = '/home2/cye73/arboEL2/data/arboel/ncbi_disease/entity_dictionary.pickle'\n",
    "with open(path_entity, 'rb') as read_handle:\n",
    "    entity_dict = pickle.load(read_handle)\n",
    "with open(path_entity2, 'rb') as read_handle:\n",
    "    entity_dict2 = pickle.load(read_handle)\n",
    "    \n",
    "entity_dict_vecs = torch.tensor(list(map(lambda x: x['ids'], entity_dict)), dtype=torch.long)\n",
    "entity_dict_vecs2 = torch.tensor(list(map(lambda x: x['ids'], entity_dict2)), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity_dict :\n",
      " {'type': 'Disease', 'cui': 'MESH:C538288', 'title': '10p Deletion Syndrome (Partial)', 'cuis': ['MESH:C538288'], 'description': '10p Deletion Syndrome (Partial) ( Disease : Chromosome 10, 10p- Partial|Chromosome 10, monosomy 10p|Chromosome 10, Partial Deletion (short arm)|Monosomy 10p )', 'tokens': ['[CLS]', '10', '##p', 'del', '##eti', '##on', 'syndrome', '(', 'partial', ')', '[unused3]', '(', 'disease', ':', 'chromosome', '10', ',', '10', '##p', '-', 'partial', '|', 'chromosome', '10', ',', 'mon', '##oso', '##my', '10', '##p', '|', 'chromosome', '10', ',', 'partial', 'del', '##eti', '##on', '(', 'short', 'arm', ')', '|', 'mon', '##oso', '##my', '10', '##p', ')', '[SEP]'], 'ids': [101, 1275, 1643, 3687, 26883, 1320, 9318, 113, 7597, 114, 3, 113, 3653, 131, 18697, 1275, 117, 1275, 1643, 118, 7597, 197, 18697, 1275, 117, 19863, 22354, 4527, 1275, 1643, 197, 18697, 1275, 117, 7597, 3687, 26883, 1320, 113, 1603, 1981, 114, 197, 19863, 22354, 4527, 1275, 1643, 114, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "entity_dict2 :\n",
      " {'type': 'Disease', 'cui': 'MESH:C538288', 'title': '10p Deletion Syndrome (Partial)', 'cuis': ['MESH:C538288'], 'description': '10p Deletion Syndrome (Partial) ( Disease : Chromosome 10, 10p- Partial ; Chromosome 10, monosomy 10p ; Chromosome 10, Partial Deletion (short arm) ; Monosomy 10p )', 'tokens': ['[CLS]', '10', '##p', 'Del', '##eti', '##on', 'S', '##yn', '##drome', '(', 'Part', '##ial', ')', '[unused3]', '(', 'Disease', ':', 'Ch', '##rom', '##oso', '##me', '10', ',', '10', '##p', '-', 'Part', '##ial', ';', 'Ch', '##rom', '##oso', '##me', '10', ',', 'mon', '##oso', '##my', '10', '##p', ';', 'Ch', '##rom', '##oso', '##me', '10', ',', 'Part', '##ial', 'Del', '##eti', '##on', '(', 'short', 'arm', ')', ';', 'Mon', '##oso', '##my', '10', '##p', ')', '[SEP]'], 'ids': [101, 1275, 1643, 9352, 26883, 1320, 156, 5730, 12743, 113, 4539, 2916, 114, 3, 113, 20012, 131, 20394, 16071, 22354, 3263, 1275, 117, 1275, 1643, 118, 4539, 2916, 132, 20394, 16071, 22354, 3263, 1275, 117, 19863, 22354, 4527, 1275, 1643, 132, 20394, 16071, 22354, 3263, 1275, 117, 4539, 2916, 9352, 26883, 1320, 113, 1603, 1981, 114, 132, 22401, 22354, 4527, 1275, 1643, 114, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(\"entity_dict :\\n\", entity_dict[0])\n",
    "print(\"entity_dict2 :\\n\", entity_dict2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity_dict_vecs :\n",
      " tensor([  101,  1275,  1643,  3687, 26883,  1320,  9318,   113,  7597,   114,\n",
      "            3,   113,  3653,   131, 18697,  1275,   117,  1275,  1643,   118,\n",
      "         7597,   132, 18697,  1275,   117, 19863, 22354,  4527,  1275,  1643,\n",
      "          132, 18697,  1275,   117,  7597,  3687, 26883,  1320,   113,  1603,\n",
      "         1981,   114,   132, 19863, 22354,  4527,  1275,  1643,   114,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "entity_dict_vecs2 :\n",
      " tensor([  101,  1275,  1643,  9352, 26883,  1320,   156,  5730, 12743,   113,\n",
      "         4539,  2916,   114,     3,   113, 20012,   131, 20394, 16071, 22354,\n",
      "         3263,  1275,   117,  1275,  1643,   118,  4539,  2916,   132, 20394,\n",
      "        16071, 22354,  3263,  1275,   117, 19863, 22354,  4527,  1275,  1643,\n",
      "          132, 20394, 16071, 22354,  3263,  1275,   117,  4539,  2916,  9352,\n",
      "        26883,  1320,   113,  1603,  1981,   114,   132, 22401, 22354,  4527,\n",
      "         1275,  1643,   114,   102,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "print(\"entity_dict_vecs :\\n\", entity_dict_vecs[0])\n",
    "print(\"entity_dict_vecs2 :\\n\", entity_dict_vecs2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "NULL_IDX=0\n",
    "token_idx_cands, segment_idx_cands, mask_cands = to_bert_input(\n",
    "            entity_dict_vecs, NULL_IDX\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_idx_cands:\n",
      " tensor([    2,  2073,  1014,  6749,  3328,    11,  4782,    12,     1,    11,\n",
      "         6414, 10281,    69,  3056,  2174,    11,  1682,  8328,    12,    69,\n",
      "         6029,    11,  2377,    12,    29,  5206,  2073,    15,  2073,  1014,\n",
      "           16,  4782,    69,  5206,  2073,    15, 16639,  2508,  2073,  1014,\n",
      "           69,  5206,  2073,    15,  4782,  6749,    11,  3274,  5996,    12,\n",
      "           69, 16639,  2508,  2073,  1014,    12,     3,     0,     0,     0,\n",
      "            0,     0,     0,     0])\n",
      "segment_idx_cands:\n",
      " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "mask_cands:\n",
      " tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
      "        False, False, False, False])\n"
     ]
    }
   ],
   "source": [
    "print(\"token_idx_cands:\\n\",token_idx_cands[0])\n",
    "print(\"segment_idx_cands:\\n\",segment_idx_cands[0])\n",
    "print(\"mask_cands:\\n\",mask_cands[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiEncoderModule(params_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, embedding_cands = model(\n",
    "            None, None, None, token_idx_cands[:5000], segment_idx_cands[:5000], mask_cands[:5000])\n",
    "# _, embedding_cands = model(\n",
    "#             None, None, None, token_idx_cands, segment_idx_cands, mask_cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_cands[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "mention: Selegiline\n",
      "mention_id: 10091617.1\n",
      "context_left: \n",
      "context_right: -induced postural hypotension in Parkinson's disease: a longitudinal study on the effects of drug withdrawal.\n",
      "OBJECTIVES: The United Kingdom Parkinson's Disease Research Group (UKPDRG) trial found an increased mortality in patients with Parkinson's disease (PD) randomized to receive 10 mg selegiline per day and L-dopa compared with those taking L-dopa alone. Recently, we found that therapy with selegiline and L-dopa was associated with selective systolic orthostatic hypotension which was abolished by withdrawal of selegiline. This unwanted effect on postural blood pressure was not the result of underlying autonomic failure. The aims of this study were to confirm our previous findings in a separate cohort of patients and to determine the time course of the cardiovascular consequences of stopping selegiline in the expectation that this might shed light on the mechanisms by which the drug causes orthostatic hypotension. METHODS: The cardiovascular responses to standing and head-up tilt were studied repeatedly in PD patients receiving selegiline and as the drug was withdrawn. RESULTS: Head-up tilt caused systolic orthostatic hypotension which was marked in six of 20 PD patients on selegiline, one of whom lost consciousness with unrecordable blood pressures. A lesser degree of orthostatic hypotension occurred with standing. Orthostatic hypotension was ameliorated 4 days after withdrawal of selegiline and totally abolished 7 days after discontinuation of the drug. Stopping selegiline also significantly reduced the supine systolic and diastolic blood pressures consistent with a previously undescribed supine pressor action. CONCLUSION: This study confirms our previous finding that selegiline in combination with L-dopa is associated with selective orthostatic hypotension. The possibilities that these cardiovascular findings might be the result of non-selective inhibition of monoamine oxidase or of amphetamine and metamphetamine are discussed.\n",
      "context_doc_id: 10091617\n",
      "type: Chemical\n",
      "label_id: MESH:D012642\n",
      "label: Selegiline ( CHEM : Deprenyl ; Eldepryl ; Selegiline Hydrochloride, (R)-Isomer ; Yumex ; E-250 ; Zelapar ; Selegiline Hydrochloride, (R,S)-Isomer ; Selegiline, (S)-Isomer ; Selegiline Hydrochloride, (S)-Isomer ; Selegiline, (R,S)-Isomer ; Emsam ; Selegyline ; L-Deprenyl ; Selegiline, (R)-Isomer ; Selegiline Hydrochloride ; Humex ; Jumex ; Deprenalin ; Deprenil ; Hydrochloride, Selegiline ; E 250 ; E250 ; Benzeneethanamine, N,alpha-dimethyl-N-2-propynyl-, (R)- ) [ A selective, irreversible inhibitor of Type B monoamine oxidase. It is used in newly diagnosed patients with Parkinson's disease. It may slow progression of the clinical disease and delay the requirement for levodopa therapy. It also may be given with levodopa upon onset of disability. (From AMA Drug Evaluations Annual, 1994, p385) The compound without isomeric designation is Deprenyl. ]\n",
      "label_title: Selegiline\n"
     ]
    }
   ],
   "source": [
    "ontology = \"MEDIC\"\n",
    "model = \"arboel\"\n",
    "# dataset = \"ncbi_disease\"\n",
    "dataset = \"bc5cdr\"\n",
    "abs_path = \"/home2/cye73/data_test2\"\n",
    "data_path = os.path.join(abs_path, model, dataset)\n",
    "\n",
    "mentions = []\n",
    "\n",
    "with open(os.path.join(data_path, \"train.jsonl\"), 'r')  as read_handle :\n",
    "    for line in read_handle:\n",
    "        mentions.append(json.loads(line))\n",
    "\n",
    "for i in range(1) :\n",
    "    print(\"------\") \n",
    "    for key, value in mentions[i].items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "mention: adenomatous polyposis coli tumour\n",
      "context_left: Identification of APC2, a homologue of the \n",
      "context_right:  suppressor.\n",
      "The adenomatous polyposis coli (APC) tumour-suppressor protein controls the Wnt signalling pathway by forming a complex with glycogen synthase kinase 3beta (GSK-3beta), axin/conductin and betacatenin. Complex formation induces the rapid degradation of betacatenin. In colon carcinoma cells, loss of APC leads to the accumulation of betacatenin in the nucleus, where it binds to and activates the Tcf-4 transcription factor (reviewed in [1] [2]). Here, we report the identification and genomic structure of APC homologues. Mammalian APC2, which closely resembles APC in overall domain structure, was functionally analyzed and shown to contain two SAMP domains, both of which are required for binding to conductin. Like APC, APC2 regulates the formation of active betacatenin-Tcf complexes, as demonstrated using transient transcriptional activation assays in APC -/- colon carcinoma cells. Human APC2 maps to chromosome 19p13. 3. APC and APC2 may therefore have comparable functions in development and cancer.\n",
      "type: Modifier\n",
      "label_id: MESH:D011125\n",
      "label_title: Adenomatous Polyposis Coli\n",
      "mention_id: 10021369.1\n",
      "context_doc_id: 10021369\n",
      "label: None\n"
     ]
    }
   ],
   "source": [
    "ontology = \"MEDIC\"\n",
    "model = \"arboel\"\n",
    "dataset = \"ncbi_disease\"\n",
    "#dataset = \"bc5cdr\"\n",
    "abs_path = \"/home2/cye73/arboEL2/data\"\n",
    "data_path = os.path.join(abs_path, model, dataset)\n",
    "\n",
    "mentions = []\n",
    "\n",
    "with open(os.path.join(data_path, \"train.jsonl\"), 'r')  as read_handle :\n",
    "    for line in read_handle:\n",
    "        mentions.append(json.loads(line))\n",
    "\n",
    "for i in range(1) :\n",
    "    print(\"------\") \n",
    "    for key, value in mentions[i].items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict :\n",
      " {'mention_id': '10091617.1', 'mention_name': 'Selegiline', 'context': {'tokens': ['[CLS]', '[unused1]', 'se', '##leg', '##ili', '##ne', '[unused2]', '-', 'induced', 'post', '##ural', 'h', '##y', '##pot', '##ens', '##ion', 'in', 'park', '##ins', '##on', \"'\", 's', 'disease', ':', 'a', 'longitudinal', 'study', 'on', 'the', 'effects', 'of', 'drug', 'withdrawal', '.', 'objectives', ':', 'the', 'united', 'kingdom', 'park', '##ins', '##on', \"'\", 's', 'disease', 'research', 'group', '(', 'uk', '##p', '##dr', '##g', ')', 'trial', 'found', 'an', 'increased', 'mortality', 'in', 'patients', 'with', 'park', '##ins', '##on', \"'\", 's', 'disease', '(', 'p', '##d', ')', 'random', '##ized', 'to', 'receive', '10', 'mg', 'se', '##leg', '##ili', '##ne', 'per', 'day', 'and', 'l', '-', 'do', '##pa', 'compared', 'with', 'those', 'taking', 'l', '-', 'do', '##pa', 'alone', '.', 'recently', ',', 'we', 'found', 'that', 'therapy', 'with', 'se', '##leg', '##ili', '##ne', 'and', 'l', '-', 'do', '##pa', 'was', 'associated', 'with', 'selective', 's', '##ys', '##to', '##lic', 'or', '##th', '##ost', '##atic', 'h', '[SEP]'], 'ids': [101, 1, 14516, 27412, 18575, 1673, 2, 118, 10645, 2112, 12602, 177, 1183, 11439, 5026, 1988, 1107, 2493, 4935, 1320, 112, 188, 3653, 131, 170, 23191, 2025, 1113, 1103, 3154, 1104, 3850, 10602, 119, 11350, 131, 1103, 10280, 6139, 2493, 4935, 1320, 112, 188, 3653, 1844, 1372, 113, 26006, 1643, 23632, 1403, 114, 3443, 1276, 1126, 2569, 14471, 1107, 4420, 1114, 2493, 4935, 1320, 112, 188, 3653, 113, 185, 1181, 114, 7091, 2200, 1106, 3531, 1275, 17713, 14516, 27412, 18575, 1673, 1679, 1285, 1105, 181, 118, 1202, 4163, 3402, 1114, 1343, 1781, 181, 118, 1202, 4163, 2041, 119, 3055, 117, 1195, 1276, 1115, 7606, 1114, 14516, 27412, 18575, 1673, 1105, 181, 118, 1202, 4163, 1108, 2628, 1114, 14930, 188, 6834, 2430, 8031, 1137, 1582, 15540, 7698, 177, 102]}, 'n_labels': 1, 'label_idxs': [252736, -1, -1, -1, -1, -1, -1, -1], 'label_cuis': ['MESH:D012642'], 'type': 'Chemical'}\n",
      "dict2 :\n",
      " {'mention_id': '10091617.1', 'mention_name': 'Selegiline', 'context': {'tokens': ['[CLS]', '[unused1]', 'Se', '##leg', '##ili', '##ne', '[unused2]', '-', 'induced', 'post', '##ural', 'h', '##y', '##pot', '##ens', '##ion', 'in', 'Parkinson', \"'\", 's', 'disease', ':', 'a', 'longitudinal', 'study', 'on', 'the', 'effects', 'of', 'drug', 'withdrawal', '.', 'O', '##B', '##J', '##EC', '##TI', '##VE', '##S', ':', 'The', 'United', 'Kingdom', 'Parkinson', \"'\", 's', 'Disease', 'Research', 'Group', '(', 'UK', '##PD', '##R', '##G', ')', 'trial', 'found', 'an', 'increased', 'mortality', 'in', 'patients', 'with', 'Parkinson', \"'\", 's', 'disease', '(', 'PD', ')', 'random', '##ized', 'to', 'receive', '10', 'mg', 'se', '##leg', '##ili', '##ne', 'per', 'day', 'and', 'L', '-', 'do', '##pa', 'compared', 'with', 'those', 'taking', 'L', '-', 'do', '##pa', 'alone', '.', 'Recently', ',', 'we', 'found', 'that', 'therapy', 'with', 'se', '##leg', '##ili', '##ne', 'and', 'L', '-', 'do', '##pa', 'was', 'associated', 'with', 'selective', 's', '##ys', '##to', '##lic', 'or', '##th', '##ost', '##atic', 'h', '##y', '[SEP]'], 'ids': [101, 1, 22087, 27412, 18575, 1673, 2, 118, 10645, 2112, 12602, 177, 1183, 11439, 5026, 1988, 1107, 22195, 112, 188, 3653, 131, 170, 23191, 2025, 1113, 1103, 3154, 1104, 3850, 10602, 119, 152, 2064, 4538, 8231, 21669, 17145, 1708, 131, 1109, 1244, 2325, 22195, 112, 188, 20012, 2713, 1990, 113, 1993, 15481, 2069, 2349, 114, 3443, 1276, 1126, 2569, 14471, 1107, 4420, 1114, 22195, 112, 188, 3653, 113, 27802, 114, 7091, 2200, 1106, 3531, 1275, 17713, 14516, 27412, 18575, 1673, 1679, 1285, 1105, 149, 118, 1202, 4163, 3402, 1114, 1343, 1781, 149, 118, 1202, 4163, 2041, 119, 15088, 117, 1195, 1276, 1115, 7606, 1114, 14516, 27412, 18575, 1673, 1105, 149, 118, 1202, 4163, 1108, 2628, 1114, 14930, 188, 6834, 2430, 8031, 1137, 1582, 15540, 7698, 177, 1183, 102]}, 'n_labels': 1, 'label_idxs': [252736, -1, -1, -1, -1, -1, -1, -1, -1, -1], 'label_cuis': ['MESH:D012642'], 'type': 'Chemical'}\n"
     ]
    }
   ],
   "source": [
    "path_train_processed_data = f'/home2/cye73/data_test2/arboel/{dataset}/train_processed_data.pickle'\n",
    "path_train_processed_data2 = f'/home2/cye73/arboEL2/data/arboel/{dataset}/train_processed_data.pickle'\n",
    "with open(path_train_processed_data, 'rb') as read_handle:\n",
    "    train_processed_data = pickle.load(read_handle)\n",
    "with open(path_train_processed_data2, 'rb') as read_handle:\n",
    "    train_processed_data2 = pickle.load(read_handle)\n",
    "\n",
    "print(\"dict :\\n\", train_processed_data[0])\n",
    "print(\"dict2 :\\n\", train_processed_data2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict :\n",
      " (tensor([  101,     1, 14516, 27412, 18575,  1673,     2,   118, 10645,  2112,\n",
      "        12602,   177,  1183, 11439,  5026,  1988,  1107,  2493,  4935,  1320,\n",
      "          112,   188,  3653,   131,   170, 23191,  2025,  1113,  1103,  3154,\n",
      "         1104,  3850, 10602,   119, 11350,   131,  1103, 10280,  6139,  2493,\n",
      "         4935,  1320,   112,   188,  3653,  1844,  1372,   113, 26006,  1643,\n",
      "        23632,  1403,   114,  3443,  1276,  1126,  2569, 14471,  1107,  4420,\n",
      "         1114,  2493,  4935,  1320,   112,   188,  3653,   113,   185,  1181,\n",
      "          114,  7091,  2200,  1106,  3531,  1275, 17713, 14516, 27412, 18575,\n",
      "         1673,  1679,  1285,  1105,   181,   118,  1202,  4163,  3402,  1114,\n",
      "         1343,  1781,   181,   118,  1202,  4163,  2041,   119,  3055,   117,\n",
      "         1195,  1276,  1115,  7606,  1114, 14516, 27412, 18575,  1673,  1105,\n",
      "          181,   118,  1202,  4163,  1108,  2628,  1114, 14930,   188,  6834,\n",
      "         2430,  8031,  1137,  1582, 15540,  7698,   177,   102]), tensor([252736,     -1,     -1,     -1,     -1,     -1,     -1,     -1]), tensor(1, dtype=torch.int32), tensor(0))\n",
      "dict2 :\n",
      " (tensor([  101,     1, 22087, 27412, 18575,  1673,     2,   118, 10645,  2112,\n",
      "        12602,   177,  1183, 11439,  5026,  1988,  1107, 22195,   112,   188,\n",
      "         3653,   131,   170, 23191,  2025,  1113,  1103,  3154,  1104,  3850,\n",
      "        10602,   119,   152,  2064,  4538,  8231, 21669, 17145,  1708,   131,\n",
      "         1109,  1244,  2325, 22195,   112,   188, 20012,  2713,  1990,   113,\n",
      "         1993, 15481,  2069,  2349,   114,  3443,  1276,  1126,  2569, 14471,\n",
      "         1107,  4420,  1114, 22195,   112,   188,  3653,   113, 27802,   114,\n",
      "         7091,  2200,  1106,  3531,  1275, 17713, 14516, 27412, 18575,  1673,\n",
      "         1679,  1285,  1105,   149,   118,  1202,  4163,  3402,  1114,  1343,\n",
      "         1781,   149,   118,  1202,  4163,  2041,   119, 15088,   117,  1195,\n",
      "         1276,  1115,  7606,  1114, 14516, 27412, 18575,  1673,  1105,   149,\n",
      "          118,  1202,  4163,  1108,  2628,  1114, 14930,   188,  6834,  2430,\n",
      "         8031,  1137,  1582, 15540,  7698,   177,  1183,   102]), tensor([252736,     -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,\n",
      "            -1]), tensor(1, dtype=torch.int32), tensor(0))\n"
     ]
    }
   ],
   "source": [
    "path_train_tensor_data = f'/home2/cye73/data_test2/arboel/{dataset}/train_tensor_data.pickle'\n",
    "path_train_tensor_data2 = f'/home2/cye73/arboEL2/data/arboel/{dataset}/train_tensor_data.pickle'\n",
    "with open(path_train_tensor_data, 'rb') as read_handle:\n",
    "    train_tensor_data = pickle.load(read_handle)\n",
    "with open(path_train_tensor_data2, 'rb') as read_handle:\n",
    "    train_tensor_data2 = pickle.load(read_handle)\n",
    "\n",
    "print(\"dict :\\n\", train_tensor_data[0])\n",
    "print(\"dict2 :\\n\", train_tensor_data2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity_dict :\n",
      " {'cui': 'MESH:C000002', 'title': 'bevonium', 'type': 'CHEM', 'description': 'bevonium ( CHEM : 2-(hydroxymethyl)-N,N-dimethylpiperidinium benzilate ; piribenzil methyl sulfate ; bevonium methylsulfate ; bevonium metilsulfate ; CG 201 ; Acabel ; bevonium sulfate (1:1) ; bevonium methyl sulfate )', 'tokens': ['[CLS]', 'be', '##von', '##ium', '[unused3]', '(', 'ch', '##em', ':', '2', '-', '(', 'h', '##ydro', '##xy', '##met', '##hyl', ')', '-', 'n', ',', 'n', '-', 'dim', '##eth', '##yl', '##pipe', '##rid', '##ini', '##um', 'ben', '##zi', '##late', ';', 'p', '##iri', '##ben', '##zi', '##l', 'met', '##hyl', 'su', '##lf', '##ate', ';', 'be', '##von', '##ium', 'met', '##hyl', '##sul', '##fa', '##te', ';', 'be', '##von', '##ium', 'met', '##ils', '##ulf', '##ate', ';', 'c', '##g', '201', ';', 'a', '##ca', '##bel', ';', 'be', '##von', '##ium', 'su', '##lf', '##ate', '(', '1', ':', '1', ')', ';', 'be', '##von', '##ium', 'met', '##hyl', 'su', '##lf', '##ate', ')', '[SEP]'], 'ids': [101, 1129, 19988, 3656, 3, 113, 22572, 5521, 131, 123, 118, 113, 177, 19694, 16844, 11006, 18873, 114, 118, 183, 117, 183, 118, 12563, 8767, 7777, 27871, 10132, 4729, 1818, 26181, 5303, 8052, 132, 185, 17262, 9672, 5303, 1233, 1899, 18873, 28117, 9654, 2193, 132, 1129, 19988, 3656, 1899, 18873, 24661, 8057, 1566, 132, 1129, 19988, 3656, 1899, 8825, 19284, 2193, 132, 172, 1403, 17365, 132, 170, 2599, 8511, 132, 1129, 19988, 3656, 28117, 9654, 2193, 113, 122, 131, 122, 114, 132, 1129, 19988, 3656, 1899, 18873, 28117, 9654, 2193, 114, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "entity_dict2 :\n",
      " {'cui': 'MESH:C000002', 'title': 'bevonium', 'type': 'CHEM', 'description': 'bevonium ( CHEM : 2-(hydroxymethyl)-N,N-dimethylpiperidinium benzilate ; piribenzil methyl sulfate ; bevonium methylsulfate ; bevonium metilsulfate ; CG 201 ; Acabel ; bevonium sulfate (1:1) ; bevonium methyl sulfate )', 'tokens': ['[CLS]', 'be', '##von', '##ium', '[unused3]', '(', 'CH', '##EM', ':', '2', '-', '(', 'h', '##ydro', '##xy', '##met', '##hyl', ')', '-', 'N', ',', 'N', '-', 'dim', '##eth', '##yl', '##pipe', '##rid', '##ini', '##um', 'ben', '##zi', '##late', ';', 'p', '##iri', '##ben', '##zi', '##l', 'met', '##hyl', 'su', '##lf', '##ate', ';', 'be', '##von', '##ium', 'met', '##hyl', '##sul', '##fa', '##te', ';', 'be', '##von', '##ium', 'met', '##ils', '##ulf', '##ate', ';', 'C', '##G', '201', ';', 'A', '##ca', '##bel', ';', 'be', '##von', '##ium', 'su', '##lf', '##ate', '(', '1', ':', '1', ')', ';', 'be', '##von', '##ium', 'met', '##hyl', 'su', '##lf', '##ate', ')', '[SEP]'], 'ids': [101, 1129, 19988, 3656, 3, 113, 24890, 15577, 131, 123, 118, 113, 177, 19694, 16844, 11006, 18873, 114, 118, 151, 117, 151, 118, 12563, 8767, 7777, 27871, 10132, 4729, 1818, 26181, 5303, 8052, 132, 185, 17262, 9672, 5303, 1233, 1899, 18873, 28117, 9654, 2193, 132, 1129, 19988, 3656, 1899, 18873, 24661, 8057, 1566, 132, 1129, 19988, 3656, 1899, 8825, 19284, 2193, 132, 140, 2349, 17365, 132, 138, 2599, 8511, 132, 1129, 19988, 3656, 28117, 9654, 2193, 113, 122, 131, 122, 114, 132, 1129, 19988, 3656, 1899, 18873, 28117, 9654, 2193, 114, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "path_entity_dictionary= f'/home2/cye73/data_test2/arboel/{dataset}/entity_dictionary.pickle'\n",
    "path_entity2_dictionary = f'/home2/cye73/arboEL2/data/arboel/{dataset}/entity_dictionary.pickle'\n",
    "with open(path_entity_dictionary, 'rb') as read_handle:\n",
    "    entity_dict = pickle.load(read_handle)\n",
    "with open(path_entity2_dictionary, 'rb') as read_handle:\n",
    "    entity_dict2 = pickle.load(read_handle)\n",
    "\n",
    "print(\"entity_dict :\\n\", entity_dict[0])\n",
    "print(\"entity_dict2 :\\n\", entity_dict2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/cye73/biomedical-entity-linking')\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional, Union\n",
    "from umls_utils import UmlsMappings\n",
    "\n",
    "@dataclass\n",
    "class BiomedicalEntity:\n",
    "    \"\"\"\n",
    "    Class for keeping track of all relevant fields in an ontology\n",
    "    \"\"\"\n",
    "    cui: str\n",
    "    name: str\n",
    "    types: List[str]\n",
    "    aliases: List[str]\n",
    "    definition: Optional[str]\n",
    "    equivalant_cuis: Optional[List[str]] = None\n",
    "    taxonomy: Optional[str] = None\n",
    "    extra_data: Optional[dict] = None\n",
    "\n",
    "@dataclass\n",
    "class BiomedicalOntology:\n",
    "    name: str\n",
    "    abbrev: Optional[str] = None                                           # Abbreviated name of ontology if different than name\n",
    "    types: List[str] = field(default_factory=list)                                          # List of all types in the ontology                                        \n",
    "    entities: List[BiomedicalEntity] = field(default_factory=list)                          # List Containing all Biomedical Entity Objects\n",
    "    mappings: dict = field(default_factory=dict)                                            # Dict mapping a cui to the index in entities\n",
    "\n",
    "    def get_aliases(self, cui=None):\n",
    "        '''\n",
    "        Get aliases for a particular CUI.  If cui=None, provide a mapping of {cui: [aliases]}\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def get_entities_with_alias(self, alias=None):\n",
    "        '''\n",
    "        Get all entities sharing a particular alias.  If alias=None, return a mapping of {alias: [cuis]}\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def get_definitions(self, cui):\n",
    "        pass\n",
    "\n",
    "    def from_obo(self, filepath=None):\n",
    "        pass\n",
    "\n",
    "    def load_umls(self, path = None, api_key = \"\"):\n",
    "        umls = UmlsMappings(umls_dir = path, umls_api_key=api_key)\n",
    "\n",
    "        # Get the Canonial Names\n",
    "        lowercase = False\n",
    "        umls_to_name = umls.get_canonical_name(\n",
    "            ontologies_to_include=\"all\",\n",
    "            use_umls_curies=True,\n",
    "            lowercase=lowercase,\n",
    "        )\n",
    "\n",
    "        # Group by the canonical names to group the alias and types \n",
    "        all_umls_df = umls.umls.query('lang == \"ENG\"').groupby('cui').agg({'alias': lambda x: list(set(x)), 'tui':'first', 'group': 'first', 'def':'first'}).reset_index()\n",
    "        all_umls_df['name'] = all_umls_df.cui.map(umls_to_name)\n",
    "        all_umls_df['alias'] = all_umls_df[['name','alias']].apply(lambda x: list(set(x[1]) - set([x[0]])) , axis=1)\n",
    "        all_umls_df['cui'] = all_umls_df['cui'].map(lambda x: 'UMLS' + x)\n",
    "        all_umls_df['has_definition'] = all_umls_df['def'].map(lambda x: x is not None)\n",
    "        all_umls_df['num_aliases'] = all_umls_df['alias'].map(lambda x: len(x))\n",
    "\n",
    "        for index, row in all_umls_df.iterrows():\n",
    "            entity = BiomedicalEntity(\n",
    "                cui = row['cui'],\n",
    "                name = row['name'],\n",
    "                types = row['tui'],\n",
    "                aliases = row['alias'],\n",
    "                definition = row['def'],\n",
    "                extra_data = {\n",
    "                    'group': row['group'],\n",
    "                }\n",
    "            )\n",
    "            self.entities.append(entity)\n",
    "            self.mappings[row['cui']] = index \n",
    "            \n",
    "    def load_medic(self, path):\n",
    "        '''\n",
    "        path : str\n",
    "        Path to medic.tsv dataset\n",
    "        '''\n",
    "\n",
    "        key_dict = [\n",
    "            \"DiseaseName\",\n",
    "            \"DiseaseID\",\n",
    "            \"AltDiseaseIDs\",\n",
    "            \"Definition\",\n",
    "            \"ParentIDs\",\n",
    "            \"TreeNumbers\",\n",
    "            \"ParentTreeNumbers\",\n",
    "            \"Synonyms\",\n",
    "            \"SlimMappings\",\n",
    "        ]\n",
    "        \n",
    "        # Open the TSV file\n",
    "        with open(path, newline=\"\") as tsvfile:\n",
    "            # Create a CSV reader specifying the delimiter as a tab character\n",
    "            reader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "\n",
    "            # Initialize a counter\n",
    "            counter = 0\n",
    "\n",
    "            ontology = []\n",
    "            # Iterate over the rows in the file\n",
    "            for row in reader:\n",
    "                dict = {}\n",
    "                # Print the current row\n",
    "                if counter > 28 :\n",
    "                    for i, elements in enumerate(row) :\n",
    "                        dict[key_dict[i]] = elements\n",
    "                    \n",
    "                    ontology.append(dict)\n",
    "                # Increment the counter\n",
    "                counter += 1\n",
    "                \n",
    "        for element in ontology : \n",
    "            equivalant_cuis = [element['DiseaseID']]\n",
    "            alt_ids = element['AltDiseaseIDs'].split('|') if element['AltDiseaseIDs'] else []\n",
    "            for alt_id in alt_ids:\n",
    "                if alt_id not in equivalant_cuis and alt_id[:2] != \"DO\":\n",
    "                    equivalant_cuis.append(alt_id)\n",
    "            entity = BiomedicalEntity(\n",
    "                cui = element['DiseaseID'],\n",
    "                name = element['DiseaseName'],\n",
    "                types = \"Disease\",\n",
    "                aliases = element['Synonyms'],\n",
    "                definition = element['Definition'],\n",
    "                equivalant_cuis = equivalant_cuis\n",
    "            )\n",
    "            self.entities.append(entity)\n",
    "\n",
    "\n",
    "    def from_mesh(self):\n",
    "        pass\n",
    "\n",
    "    def from_ncbi_taxon(self):\n",
    "        pass\n",
    "\n",
    "    def from_csv(self):\n",
    "        pass\n",
    "\n",
    "    def from_json(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     ontology = BiomedicalOntology(name=\"UMLS\")\n",
    "#     ontology.load_umls(path=\"/mitchell/entity-linking/2017AA/META/\", api_key=\"\")\n",
    "#     print(ontology.entities[0])\n",
    "#     print(ontology.entities[0].__dict__)\n",
    "#     print(ontology.entities[0].cui)\n",
    "#     print(ontology.mappings[ontology.entities[0].cui])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "ontology = BiomedicalOntology(name=\"UMLS\")\n",
    "ontology.load_medic(path=\"/mitchell/entity-linking/kbs/medic.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ontology.entities[0] : BiomedicalEntity(cui='MESH:C538288', name='10p Deletion Syndrome (Partial)', types='Disease', aliases='Chromosome 10, 10p- Partial|Chromosome 10, monosomy 10p|Chromosome 10, Partial Deletion (short arm)|Monosomy 10p', definition='', equivalant_cuis=['MESH:C538288'], taxonomy=None, extra_data=None)\n",
      "ontology.entities[0].cui : MESH:C538288\n"
     ]
    }
   ],
   "source": [
    "print('ontology.entities[0] :', ontology.entities[0])\n",
    "print('ontology.entities[0].cui :', ontology.entities[0].cui)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arboel_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
