{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17/Mar/2024 11:22:47] INFO - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "[17/Mar/2024 11:22:47] INFO - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "[17/Mar/2024 11:22:47] INFO - Loading faiss with AVX2 support.\n",
      "[17/Mar/2024 11:22:47] INFO - Successfully loaded faiss with AVX2 support.\n",
      "[17/Mar/2024 11:22:47] INFO - This is an info message.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../..')\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "import logger\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim, nn, utils, Tensor\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import lightning as L\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_transformers.tokenization_bert import BertTokenizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datetime import datetime\n",
    "from typing import Optional, Union\n",
    "\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler) \n",
    "from pytorch_transformers.optimization import WarmupLinearSchedule \n",
    "from scipy.sparse.csgraph import minimum_spanning_tree \n",
    "# csgraph = compressed sparse graph\n",
    "from scipy.sparse import csr_matrix\n",
    "# csr_matrix = compressed sparse row matrices\n",
    "from collections import Counter \n",
    "\n",
    "import blink.biencoder.data_process_mult as data_process\n",
    "import blink.candidate_ranking.utils as utils\n",
    "from blink.biencoder.biencoder import BiEncoderRanker\n",
    "from blink.common.optimizer import get_bert_optimizer\n",
    "from blink.common.params import BlinkParser\n",
    "\n",
    "from IPython import embed \n",
    "from tqdm import tqdm\n",
    "\n",
    "import logging\n",
    "\n",
    "# Configure the logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Now you can use logger.info()\n",
    "logger.info(\"This is an info message.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd /home2/cye73/arboEL/blink/biencoder/special_partition/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python setup.py build_ext --inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import blink.biencoder.eval_cluster_linking as eval_cluster_linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(split, params, logger):\n",
    "    '''\n",
    "    Description \n",
    "    -----------\n",
    "    Loads dataset samples from a specified path\n",
    "    Optionally filters out samples without labels\n",
    "    Checks if the dataset supports multiple labels per sample\n",
    "    \"has_mult_labels\" : bool\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    split : str\n",
    "        Indicates the portion of the dataset to load (\"train\", \"test\", \"valid\"), used by utils.read_dataset to determine which data to read.\n",
    "    params : dict(str)\n",
    "        Contains configuration options\n",
    "    logger : \n",
    "        An object used for logging messages about the process, such as the number of samples read.\n",
    "    '''\n",
    "    samples = utils.read_dataset(split, params[\"data_path\"]) #DD21\n",
    "    # Check if dataset has multiple ground-truth labels\n",
    "    has_mult_labels = \"labels\" in samples[0].keys()\n",
    "    if params[\"filter_unlabeled\"]:\n",
    "        # Filter samples without gold entities\n",
    "        samples = list(\n",
    "            filter(lambda sample: (len(sample[\"labels\"]) > 0) if has_mult_labels else (sample[\"label\"] is not None),\n",
    "                   samples))\n",
    "    logger.info(f\"Read %d {split} samples.\" % len(samples))\n",
    "    return samples, has_mult_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed_data = None\n",
    "valid_processed_data = None\n",
    "test_processed_data = None\n",
    "train_tensor_data = None\n",
    "valid_tensor_data = None\n",
    "test_tensor_data = None\n",
    "train_samples = None\n",
    "valid_samples = None\n",
    "test_samples = None\n",
    "entity_dict_vecs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home2/cye73/data/arboel/medmentions_st21pv\n",
      "[15/Mar/2024 15:14:07] INFO - Read 122174 train samples.\n"
     ]
    }
   ],
   "source": [
    "ontology = \"UMLS\"\n",
    "model = \"arboel\"\n",
    "dataset = \"medmentions_st21pv\"\n",
    "abs_path = \"/home2/cye73/data\"\n",
    "data_path = os.path.join(abs_path, model, dataset)\n",
    "print(data_path)\n",
    "abs_path2 = \"/home2/cye73/results\"\n",
    "model_output_path = os.path.join(abs_path2, model, dataset)\n",
    "\n",
    "ontology_type = \"umls\"\n",
    "umls_dir=\"/mitchell/entity-linking/2017AA/META/\"\n",
    "\n",
    "params_test = {\"data_path\" : data_path, \n",
    "               \"model_output_path\": model_output_path,\n",
    "               \"batch_size\" : 64,\n",
    "               \"max_context_length\": 64 ,\n",
    "               \"max_cand_length\" : 64 ,\n",
    "               \"context_key\" : \"context\",\n",
    "               \"debug\" : False,\n",
    "               \"knn\" : 4,\n",
    "               \"bert_model\": 'michiyasunaga/BioLinkBERT-base',\n",
    "               \"out_dim\": 768 ,\n",
    "               \"pull_from_layer\":11,\n",
    "               \"add_linear\":True,\n",
    "               \"use_types\" : True,\n",
    "               \"force_exact_search\" : True,\n",
    "               \"probe_mult_factor\" : 1,\n",
    "               \"embed_batch_size\" : 768,\n",
    "               \"drop_entities\" : False,\n",
    "               \"within_doc\" : True,\n",
    "               \"filter_unlabeled\" : False,\n",
    "               }\n",
    "\n",
    "train_samples, train_mult_labels = read_data(\"train\", params_test, logger)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter_by_context_doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '£' (U+00A3) (458944430.py, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [8], line 30\u001b[0;36m\u001b[0m\n\u001b[0;31m    mention_idxs = £mention_idxs[mask] # possible only if mention_idxs is an array, not a list\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '£' (U+00A3)\n"
     ]
    }
   ],
   "source": [
    "def filter_by_context_doc_id(mention_idxs, doc_id, doc_id_list, return_numpy=False):\n",
    "    '''\n",
    "    Description \n",
    "    -----------\n",
    "    Filters and returns mention indices that belong to a specific document identified by \"doc_id\".\n",
    "    Ensures that the analysis are constrained within the context of that particular document.\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    - mention_idxs : ndarray(int) of dim = (number of mentions)\n",
    "    Represents the indices of mentions\n",
    "    - doc_id : int \n",
    "    Indice of the target document\n",
    "    - doc_id_list : ndarray(int) of dim = (number of mentions)\n",
    "    Array of integers, where each element is a document ID associated with the corresponding mention in mention_idxs. \n",
    "    The length of doc_id_list should match the total number of mentions referenced in mention_idxs.\n",
    "    - return_numpy : bool\n",
    "    A flag indicating whether to return the filtered list of mention indices as a NumPy array. \n",
    "    If True, the function returns a NumPy array; otherwise, it returns a list\n",
    "    -------\n",
    "    Outputs: \n",
    "    - mask : ndarray(bool) of dim = (number of mentions)\n",
    "    Mask indicating where each mention's document ID (from doc_id_list) matches the target doc_id\n",
    "    - mention_idxs : \n",
    "    Only contains mention indices that belong to the target document (=doc_id).\n",
    "    '''\n",
    "    mask = [doc_id_list[i] == doc_id for i in mention_idxs]\n",
    "    if isinstance(mention_idxs, list): # Test if mention_idxs = list. Return a bool\n",
    "        mention_idxs = np.array(mention_idxs) \n",
    "    mention_idxs = mention_idxs[mask] # possible only if mention_idxs is an array, not a list\n",
    "    if not return_numpy:\n",
    "        mention_idxs = list(mention_idxs)\n",
    "    return mention_idxs, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### entity dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_dictionary_pkl_path = os.path.join(params_test[\"data_path\"], 'entity_dictionary.pickle')\n",
    "\n",
    "if os.path.isfile(entity_dictionary_pkl_path) :\n",
    "    print(\"Loading stored processed entity dictionary...\")\n",
    "    with open(entity_dictionary_pkl_path, 'rb') as read_handle: #CC7 'rb' = binary read mode\n",
    "        entity_dictionary = pickle.load(read_handle)\n",
    "\n",
    "if not os.path.isfile(entity_dictionary_pkl_path) : \n",
    "    with open(entity_dictionary_pkl_path, 'wb') as write_handle:\n",
    "        pickle.dump(entity_dictionary, write_handle,\n",
    "                    protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "entity_dict_vecs = torch.tensor(list(map(lambda x: x['ids'], entity_dictionary)), dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training mention data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='michiyasunaga/BioLinkBERT-base', vocab_size=28895, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('michiyasunaga/BioLinkBERT-base')\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing dictionary:   0%|          | 0/3465007 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing dictionary: 100%|██████████| 3465007/3465007 [00:02<00:00, 1159101.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/Mar/2024 23:54:07] INFO - ====Processed samples: ====\n",
      "[12/Mar/2024 23:54:07] INFO - Context tokens : [CLS] [unused1] dct ##n4 [unused2] as a modifier of chronic pseudomonas aeruginosa infection in cystic fibrosis pseudomonas aeruginosa ( pa ) infection in cystic fibrosis ( cf ) patients is associated with worse long - term pulmonary disease and shorter survival , and chronic pa infection ( cpa ) is associated with reduced lung function , faster rate of lung decline , increased [SEP]\n",
      "[12/Mar/2024 23:54:07] INFO - Context ids : 2 1 28493 22629 1 1732 42 22597 1685 2961 9628 9476 2527 1682 8111 5834 9628 9476 11 3627 12 2527 1682 8111 5834 11 5097 12 1808 1744 2138 1715 8583 2476 16 2384 3744 2174 1690 6865 2909 15 1690 2961 3627 2527 11 19287 12 1744 2138 1715 2716 3162 2108 15 8050 2366 1685 3162 6309 15 2165 3\n",
      "[12/Mar/2024 23:54:07] INFO - Label 3456251 tokens : [CLS] dct ##n4 protein , human [unused3] ( t1 ##16 : dyn ##actin subunit 4 , human ; dyn ##actin 4 ( p ##62 ) protein , human ; dyn ##actin p ##62 subunit , human ) [SEP]\n",
      "[12/Mar/2024 23:54:07] INFO - Label 3456251 ids : 2 28493 22629 2031 15 2323 1 11 6528 5250 29 8812 8729 5981 23 15 2323 30 8812 8729 23 11 57 13669 12 2031 15 2323 30 8812 8729 57 13669 5981 15 2323 12 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[12/Mar/2024 23:54:07] INFO - Context tokens : [CLS] dct ##n4 as a modifier of [unused1] chronic pseudomonas aeruginosa infection [unused2] in cystic fibrosis pseudomonas aeruginosa ( pa ) infection in cystic fibrosis ( cf ) patients is associated with worse long - term pulmonary disease and shorter survival , and chronic pa infection ( cpa ) is associated with reduced lung function , faster rate of lung decline , increased [SEP]\n",
      "[12/Mar/2024 23:54:07] INFO - Context ids : 2 28493 22629 1732 42 22597 1685 1 2961 9628 9476 2527 1 1682 8111 5834 9628 9476 11 3627 12 2527 1682 8111 5834 11 5097 12 1808 1744 2138 1715 8583 2476 16 2384 3744 2174 1690 6865 2909 15 1690 2961 3627 2527 11 19287 12 1744 2138 1715 2716 3162 2108 15 8050 2366 1685 3162 6309 15 2165 3\n",
      "[12/Mar/2024 23:54:07] INFO - Label 574449 tokens : [CLS] pseudomonas aeruginosa infection [unused3] ( t0 ##47 : aeruginosa infections pseudomonas ; aeruginosa infections pseudo ##mon ##a ; pseudomonas aeruginosa infection ; aeruginosa ; pseudomonas ; pseudomonas ; aeruginosa ; infection pseudomonas aeruginosa ; pseudomonas aeruginosa infection nos ) [SEP]\n",
      "[12/Mar/2024 23:54:07] INFO - Label 574449 ids : 2 9628 9476 2527 1 11 21544 11658 29 9476 4127 9628 30 9476 4127 10108 2258 1012 30 9628 9476 2527 30 9476 30 9628 30 9628 30 9476 30 2527 9628 9476 30 9628 9476 2527 8410 12 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[12/Mar/2024 23:54:07] INFO - Context tokens : [CLS] dct ##n4 as a modifier of chronic pseudomonas aeruginosa infection in [unused1] cystic fibrosis [unused2] pseudomonas aeruginosa ( pa ) infection in cystic fibrosis ( cf ) patients is associated with worse long - term pulmonary disease and shorter survival , and chronic pa infection ( cpa ) is associated with reduced lung function , faster rate of lung decline , increased [SEP]\n",
      "[12/Mar/2024 23:54:07] INFO - Context ids : 2 28493 22629 1732 42 22597 1685 2961 9628 9476 2527 1682 1 8111 5834 1 9628 9476 11 3627 12 2527 1682 8111 5834 11 5097 12 1808 1744 2138 1715 8583 2476 16 2384 3744 2174 1690 6865 2909 15 1690 2961 3627 2527 11 19287 12 1744 2138 1715 2716 3162 2108 15 8050 2366 1685 3162 6309 15 2165 3\n",
      "[12/Mar/2024 23:54:07] INFO - Label 5410 tokens : [CLS] cystic fibrosis [unused3] ( t0 ##47 : muco ##vis ##ci ##d ##osis ; fibrosis , cystic ; cystic fibrosis ; cystic fibrosis nos ; cystic fibrosis ( disorder ) ; cystic fibrosis nos ( disorder ) ; cystic fibrosis [ disease / finding ] ; cf ; fibro ##cys ##tic disease ; cf - cystic fibrosis ; muco ##vis ##ci ##d ##osis [SEP]\n",
      "[12/Mar/2024 23:54:07] INFO - Label 5410 ids : 2 8111 5834 1 11 21544 11658 29 26492 8944 9002 1022 2061 30 5834 15 8111 30 8111 5834 30 8111 5834 8410 30 8111 5834 11 3870 12 30 8111 5834 8410 11 3870 12 30 8111 5834 36 2174 18 2492 38 30 5097 30 15098 18487 1751 2174 30 5097 16 8111 5834 30 26492 8944 9002 1022 2061 3\n",
      "[12/Mar/2024 23:54:07] INFO - Context tokens : [CLS] dct ##n4 as a modifier of chronic pseudomonas aeruginosa infection in cystic fibrosis [unused1] pseudomonas aeruginosa ( pa ) infection [unused2] in cystic fibrosis ( cf ) patients is associated with worse long - term pulmonary disease and shorter survival , and chronic pa infection ( cpa ) is associated with reduced lung function , faster rate of lung decline , increased [SEP]\n",
      "[12/Mar/2024 23:54:07] INFO - Context ids : 2 28493 22629 1732 42 22597 1685 2961 9628 9476 2527 1682 8111 5834 1 9628 9476 11 3627 12 2527 1 1682 8111 5834 11 5097 12 1808 1744 2138 1715 8583 2476 16 2384 3744 2174 1690 6865 2909 15 1690 2961 3627 2527 11 19287 12 1744 2138 1715 2716 3162 2108 15 8050 2366 1685 3162 6309 15 2165 3\n",
      "[12/Mar/2024 23:54:07] INFO - Label 574449 tokens : [CLS] pseudomonas aeruginosa infection [unused3] ( t0 ##47 : aeruginosa infections pseudomonas ; aeruginosa infections pseudo ##mon ##a ; pseudomonas aeruginosa infection ; aeruginosa ; pseudomonas ; pseudomonas ; aeruginosa ; infection pseudomonas aeruginosa ; pseudomonas aeruginosa infection nos ) [SEP]\n",
      "[12/Mar/2024 23:54:07] INFO - Label 574449 ids : 2 9628 9476 2527 1 11 21544 11658 29 9476 4127 9628 30 9476 4127 10108 2258 1012 30 9628 9476 2527 30 9476 30 9628 30 9628 30 9476 30 2527 9628 9476 30 9628 9476 2527 8410 12 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[12/Mar/2024 23:54:07] INFO - Context tokens : [CLS] dct ##n4 as a modifier of chronic pseudomonas aeruginosa infection in cystic fibrosis pseudomonas aeruginosa ( pa ) infection in [unused1] cystic fibrosis [unused2] ( cf ) patients is associated with worse long - term pulmonary disease and shorter survival , and chronic pa infection ( cpa ) is associated with reduced lung function , faster rate of lung decline , increased [SEP]\n",
      "[12/Mar/2024 23:54:07] INFO - Context ids : 2 28493 22629 1732 42 22597 1685 2961 9628 9476 2527 1682 8111 5834 9628 9476 11 3627 12 2527 1682 1 8111 5834 1 11 5097 12 1808 1744 2138 1715 8583 2476 16 2384 3744 2174 1690 6865 2909 15 1690 2961 3627 2527 11 19287 12 1744 2138 1715 2716 3162 2108 15 8050 2366 1685 3162 6309 15 2165 3\n",
      "[12/Mar/2024 23:54:07] INFO - Label 5410 tokens : [CLS] cystic fibrosis [unused3] ( t0 ##47 : muco ##vis ##ci ##d ##osis ; fibrosis , cystic ; cystic fibrosis ; cystic fibrosis nos ; cystic fibrosis ( disorder ) ; cystic fibrosis nos ( disorder ) ; cystic fibrosis [ disease / finding ] ; cf ; fibro ##cys ##tic disease ; cf - cystic fibrosis ; muco ##vis ##ci ##d ##osis [SEP]\n",
      "[12/Mar/2024 23:54:07] INFO - Label 5410 ids : 2 8111 5834 1 11 21544 11658 29 26492 8944 9002 1022 2061 30 5834 15 8111 30 8111 5834 30 8111 5834 8410 30 8111 5834 11 3870 12 30 8111 5834 8410 11 3870 12 30 8111 5834 36 2174 18 2492 38 30 5097 30 15098 18487 1751 2174 30 5097 16 8111 5834 30 26492 8944 9002 1022 2061 3\n"
     ]
    }
   ],
   "source": [
    "entity_dictionary_loaded = True\n",
    "# train_processed_data, entity_dictionary, train_tensor_data = data_process.process_mention_data(\n",
    "#         train_samples,\n",
    "#         entity_dictionary,\n",
    "#         tokenizer,\n",
    "#         params_test[\"max_context_length\"],\n",
    "#         params_test[\"max_cand_length\"],\n",
    "#         context_key=params_test[\"context_key\"],\n",
    "#         multi_label_key=\"labels\" if train_mult_labels else None,\n",
    "#         # silent=self.hparams[\"silent\"], \n",
    "#         logger=logger,\n",
    "#         debug=params_test[\"debug\"], \n",
    "#         knn=params_test[\"knn\"],\n",
    "#         dictionary_processed=entity_dictionary_loaded\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stored processed train data...\n"
     ]
    }
   ],
   "source": [
    "train_tensor_data_pkl_path = os.path.join(params_test[\"data_path\"], 'train_tensor_data.pickle')\n",
    "train_processed_data_pkl_path = os.path.join(params_test[\"data_path\"], 'train_processed_data.pickle')\n",
    "        \n",
    "if os.path.isfile(train_tensor_data_pkl_path) and os.path.isfile(train_processed_data_pkl_path):\n",
    "    print(\"Loading stored processed train data...\")\n",
    "    with open(train_tensor_data_pkl_path, 'rb') as read_handle: #CC7 'rb' = binary read mode\n",
    "        train_tensor_data = pickle.load(read_handle)\n",
    "    with open(train_processed_data_pkl_path, 'rb') as read_handle:\n",
    "        train_processed_data = pickle.load(read_handle)\n",
    "    \n",
    "if not os.path.isfile(train_tensor_data_pkl_path) : \n",
    "    with open(train_tensor_data_pkl_path, 'wb') as write_handle:\n",
    "        pickle.dump(train_tensor_data, write_handle,\n",
    "                    protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(train_processed_data_pkl_path, 'wb') as write_handle:\n",
    "        pickle.dump(train_processed_data, write_handle,\n",
    "                    protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "train_men_vecs = train_tensor_data[:][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation mention data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_samples, valid_mult_labels = read_data(\"valid\", params_test, logger)\n",
    "# valid_processed_data, entity_dictionary, valid_tensor_data = data_process.process_mention_data(\n",
    "#         valid_samples,\n",
    "#         entity_dictionary,\n",
    "#         tokenizer,\n",
    "#         params_test[\"max_context_length\"],\n",
    "#         params_test[\"max_cand_length\"],\n",
    "#         context_key=params_test[\"context_key\"],\n",
    "#         multi_label_key=\"labels\" if valid_mult_labels else None,\n",
    "#         # silent= self.hparams[\"silent\"], \n",
    "#         logger=logger,\n",
    "#         debug=params_test[\"debug\"], \n",
    "#         knn=params_test[\"knn\"],\n",
    "#         dictionary_processed=True\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stored processed valid data...\n"
     ]
    }
   ],
   "source": [
    "valid_tensor_data_pkl_path = os.path.join(params_test[\"data_path\"], 'valid_tensor_data.pickle')\n",
    "valid_processed_data_pkl_path = os.path.join(params_test[\"data_path\"], 'valid_processed_data.pickle')\n",
    "\n",
    "if os.path.isfile(valid_tensor_data_pkl_path) and os.path.isfile(valid_processed_data_pkl_path):\n",
    "    print(\"Loading stored processed valid data...\")\n",
    "    with open(valid_tensor_data_pkl_path, 'rb') as read_handle: #CC7 'rb' = binary read mode\n",
    "        valid_tensor_data = pickle.load(read_handle)\n",
    "    with open(valid_processed_data_pkl_path, 'rb') as read_handle:\n",
    "        valid_processed_data = pickle.load(read_handle)\n",
    "\n",
    "if not os.path.isfile(valid_tensor_data_pkl_path) : \n",
    "    with open(valid_tensor_data_pkl_path, 'wb') as write_handle:\n",
    "        pickle.dump(valid_tensor_data, write_handle,\n",
    "                    protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(valid_processed_data_pkl_path, 'wb') as write_handle:\n",
    "        pickle.dump(valid_processed_data, write_handle,\n",
    "                    protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "valid_men_vecs = valid_tensor_data[:][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test mention data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# test_samples, test_mult_labels = read_data(\"test\", params_test, logger)\n",
    "# test_processed_data, entity_dictionary, test_tensor_data = data_process.process_mention_data(\n",
    "#         test_samples,\n",
    "#         entity_dictionary,\n",
    "#         tokenizer,\n",
    "#         params_test[\"max_context_length\"],\n",
    "#         params_test[\"max_cand_length\"],\n",
    "#         context_key=params_test[\"context_key\"],\n",
    "#         multi_label_key=\"labels\" if test_mult_labels else None,\n",
    "#         # silent=self.hparams[\"silent\"], \n",
    "#         logger=logger,\n",
    "#         debug=params_test[\"debug\"], \n",
    "#         knn=params_test[\"knn\"],\n",
    "#         dictionary_processed=True\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stored processed test data...\n"
     ]
    }
   ],
   "source": [
    "test_tensor_data_pkl_path = os.path.join(params_test[\"data_path\"], 'test_tensor_data.pickle')\n",
    "test_processed_data_pkl_path = os.path.join(params_test[\"data_path\"], 'test_processed_data.pickle')\n",
    "\n",
    "if os.path.isfile(test_tensor_data_pkl_path) and os.path.isfile(test_processed_data_pkl_path):\n",
    "    print(\"Loading stored processed test data...\")\n",
    "    with open(test_tensor_data_pkl_path, 'rb') as read_handle: #CC7 'rb' = binary read mode\n",
    "        test_tensor_data = pickle.load(read_handle)\n",
    "    with open(test_processed_data_pkl_path, 'rb') as read_handle:\n",
    "        test_processed_data = pickle.load(read_handle)\n",
    "        \n",
    "if not os.path.isfile(test_tensor_data_pkl_path) : \n",
    "    with open(test_tensor_data_pkl_path, 'wb') as write_handle:\n",
    "        pickle.dump(test_tensor_data, write_handle,\n",
    "                    protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(test_processed_data_pkl_path, 'wb') as write_handle:\n",
    "        pickle.dump(test_processed_data, write_handle,\n",
    "                    protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        \n",
    "test_men_vecs = test_tensor_data[:][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Within doc consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13/Mar/2024 10:31:03] INFO - Read 40864 valid samples.\n",
      "[13/Mar/2024 10:31:04] INFO - Read 40864 valid samples.\n"
     ]
    }
   ],
   "source": [
    "train_context_doc_ids = valid_context_doc_ids = test_context_doc_ids = None\n",
    "if params_test[\"within_doc\"]: \n",
    "    # RR9 : If path exist, train_samples, valid_samples, test_samples haven't been defined yet\n",
    "    # Store the context_doc_id for every mention in the train and valid sets\n",
    "    if train_samples is None:\n",
    "        train_samples, _ = read_data(\"train\", params_test, logger)\n",
    "    train_context_doc_ids = [s['context_doc_id'] for s in train_samples]\n",
    "    if valid_samples is None:\n",
    "        valid_samples, _ = read_data(\"valid\", params_test, logger)\n",
    "    valid_context_doc_ids = [s['context_doc_id'] for s in valid_samples]\n",
    "    if test_samples is None:\n",
    "        test_samples, _ = read_data(\"valid\", params_test, logger)\n",
    "    test_context_doc_ids = [s['context_doc_id'] for s in test_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gold_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get clusters of mentions that map to a gold entity\n",
    "train_gold_clusters = data_process.compute_gold_clusters(train_processed_data)\n",
    "# Maximum length of clusters inside gold_cluster\n",
    "max_gold_cluster_len = 0\n",
    "index = -1\n",
    "for i, ent in enumerate(train_gold_clusters):\n",
    "    if len(train_gold_clusters[ent]) > max_gold_cluster_len:\n",
    "        max_gold_cluster_len = len(train_gold_clusters[ent])\n",
    "        index = i \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_labels: 1\n",
      "\n",
      "label_idxs: [3456251, -1, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "print('n_labels:',train_processed_data[0]['n_labels'])\n",
    "print('\\nlabel_idxs:',train_processed_data[0]['label_idxs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_gold_clusters: [[0, 13, 14, 19, 30, 38, 45, 52, 60], [1, 3, 7, 8, 15, 21, 22, 23, 37, 41, 43, 44, 47, 50, 51, 55, 58, 64, 42625], [2, 4, 5, 16, 25, 32, 40, 42, 49, 54, 57, 65, 59139, 59140, 59152, 59153, 59161, 59179, 59182, 118599], [6, 21378, 21379, 21380, 21382, 21383, 21388, 21391, 21393, 21396, 25610, 112982, 112983], [9], [10, 24429, 24447, 41935, 69261, 79740, 88613, 94178], [11, 940, 951, 976, 980, 44640, 44650, 44664, 46981, 47003, 47016, 89193, 91766, 105478, 105493], [12, 20, 31, 39, 46, 48, 53, 61, 62, 5624, 5642, 7875, 31651, 31666, 45673, 64354, 71070, 71084, 71109, 94417, 94477, 97442, 97451, 99217, 105937, 110864], [17, 20339, 20353, 48587, 80542, 90933, 90938, 90950, 90955, 90963, 105213, 109813, 119202], [18, 80, 263, 425, 447, 474, 600, 760, 817, 821, 950, 1051, 1356, 1358, 1374, 1552, 1582, 1643, 1847, 1881, 1907, 1963, 2098, 2109, 2459, 2576, 2659, 2667, 3046, 3133, 3208, 3216, 3434, 3551, 3609, 3935, 3949, 4005, 4046, 4221, 4273, 4420, 5250, 5378, 5702, 5705, 5758, 5760, 6038, 6157, 6169, 6472, 6479, 6485, 6487, 6492, 6513, 6519, 6554, 6661, 6701, 7060, 7215, 7217, 7348, 7409, 7417, 7452, 7457, 7471, 7600, 7965, 7974, 8028, 8256, 8289, 8382, 8403, 8612, 8613, 8799, 9104, 9185, 9307, 9309, 9385, 9395, 9402, 9407, 9465, 9504, 9536, 9546, 9705, 9723, 9743, 9875, 9950, 9970, 10026, 10093, 10150, 10159, 10311, 10334, 10360, 10387, 10773, 10791, 10795, 10843, 10965, 10993, 11227, 11273, 11341, 11705, 11714, 12143, 12169, 12312, 12499, 12563, 12645, 12688, 12806, 12819, 12858, 12941, 13000, 13001, 13141, 13340, 13345, 13346, 13347, 13386, 13545, 13694, 13771, 13827, 13830, 13968, 13995, 14037, 14191, 14234, 14308, 14341, 14368, 14860, 14918, 14950, 15036, 15271, 15378, 15417, 15424, 15430, 15436, 15464, 15487, 15508, 15622, 15652, 15898, 15899, 15957, 15990, 16089, 16096, 16113, 16147, 16180, 16345, 16457, 16611, 16667, 16685, 16743, 16922, 16932, 16943, 17620, 17644, 17757, 17796, 17806, 17808, 17821, 17872, 18071, 18092, 18158, 18231, 18247, 18271, 18567, 18568, 18583, 18710, 18930, 18970, 19249, 19323, 19565, 19661, 19785, 19912, 19927, 19932, 19975, 20144, 20156, 20415, 20527, 20626, 20637, 21563, 21564, 21568, 21574, 22128, 22345, 22408, 22512, 22713, 22771, 23100, 23221, 23433, 23482, 23977, 24100, 24198, 24211, 24215, 24311, 24354, 24391, 24398, 24921, 24930, 25063, 25067, 25255, 25412, 25482, 25484, 25553, 25621, 25693, 25786, 25863, 25885, 26148, 26160, 26191, 26339, 26444, 26496, 26757, 26790, 26792, 26800, 26839, 26941, 27009, 27017, 27020, 27021, 27024, 27026, 27029, 27036, 27043, 27044, 27052, 27053, 27569, 27747, 27780, 28088, 28098, 28134, 28306, 28315, 28345, 28349, 28472, 28506, 28518, 28807, 28818, 28823, 28825, 28849, 29094, 29104, 29113, 29131, 29163, 29324, 29328, 29556, 29564, 29569, 29576, 29639, 29643, 29648, 29789, 30147, 30203, 30215, 30222, 30313, 30350, 30383, 30400, 30413, 31113, 31368, 31373, 31415, 31439, 31453, 31528, 31546, 32062, 32164, 32179, 32196, 32337, 32342, 32363, 32406, 32461, 32548, 32565, 32611, 32625, 32629, 32678, 32813, 32819, 32832, 32905, 32920, 32944, 33063, 33208, 33453, 33474, 33497, 33606, 33649, 33664, 33688, 33698, 33709, 33714, 33717, 33718, 33739, 34090, 34485, 34622, 34635, 34648, 34712, 34894, 35126, 35210, 35215, 35717, 35783, 35862, 35864, 35869, 35871, 35900, 36143, 36188, 36351, 36455, 36480, 36484, 36526, 36530, 36543, 36551, 36570, 36689, 36736, 37139, 37344, 37363, 37616, 37926, 37933, 38036, 38308, 38315, 38324, 38354, 38385, 38578, 38594, 38624, 38630, 38635, 38790, 38916, 39052, 39165, 39172, 39221, 39259, 39389, 39401, 39485, 39558, 39702, 39743, 39818, 39837, 39847, 39849, 39858, 39873, 40130, 40289, 40321, 40328, 40330, 40429, 40594, 40646, 40704, 40745, 40845, 41133, 41194, 41207, 41325, 41479, 41492, 41542, 41545, 41551, 41644, 41686, 41720, 41759, 41766, 41782, 41787, 41803, 41946, 41951, 41974, 42038, 42048, 42086, 42134, 42699, 42710, 42721, 42727, 42825, 42985, 43121, 43520, 43765, 43790, 43919, 43920, 43925, 43939, 44058, 44310, 44311, 44315, 44328, 44338, 44353, 44366, 44374, 44545, 44617, 44691, 44847, 44878, 44891, 44898, 45040, 45065, 45093, 45265, 45462, 45465, 45480, 45997, 46029, 46362, 46424, 46484, 46641, 46774, 46780, 46807, 46819, 47015, 47047, 47123, 47132, 47207, 47209, 47224, 47409, 47735, 47858, 47897, 47976, 47983, 48098, 48109, 48147, 48150, 48402, 48405, 48524, 48542, 48547, 48558, 48725, 48732, 48875, 49038, 49155, 49169, 49219, 49289, 49334, 49401, 49435, 49516, 49716, 49730, 49753, 49756, 49758, 49768, 49778, 49794, 49795, 49841, 50055, 50112, 50136, 50235, 50311, 50319, 50331, 50639, 50645, 50836, 50883, 50973, 51000, 51078, 51083, 51108, 51272, 51362, 51785, 51789, 51833, 51854, 52019, 52544, 52634, 52640, 52735, 52751, 52868, 52876, 52902, 53037, 53094, 53198, 53279, 53362, 53419, 53540, 53737, 53769, 53772, 53822, 53986, 54126, 54309, 54386, 54395, 54733, 54920, 54948, 54969, 55011, 55017, 55050, 55218, 55254, 55439, 55502, 55510, 55600, 55605, 55647, 55685, 55798, 55803, 56105, 56151, 57323, 57352, 57365, 57512, 57519, 57669, 57679, 57696, 58003, 58040, 58118, 58123, 58648, 58703, 58806, 58812, 58852, 58955, 58984, 59003, 59034, 59655, 59901, 59964, 59977, 60142, 60148, 60156, 60163, 60183, 60199, 60229, 60249, 60325, 60448, 60700, 61022, 61061, 61863, 62112, 62115, 62159, 62239, 62244, 62255, 62631, 62731, 62999, 63090, 63115, 63286, 63401, 63452, 63514, 63600, 63647, 63745, 63786, 63842, 64085, 64116, 64147, 64149, 64157, 64242, 64400, 64494, 64497, 64567, 64724, 64772, 64800, 64816, 65415, 65462, 65553, 65597, 65634, 65931, 65945, 66098, 66101, 66334, 66361, 66485, 66513, 66526, 66613, 66621, 66783, 66993, 67012, 67015, 67134, 67276, 67379, 67381, 67518, 67533, 67588, 67694, 68026, 68158, 68167, 68170, 68179, 68181, 68440, 68443, 68564, 68572, 68729, 69377, 69381, 69404, 69488, 69783, 69790, 69810, 69908, 70076, 70247, 70387, 70591, 70623, 70629, 70772, 71015, 71337, 71445, 71678, 71732, 71968, 72031, 72086, 72241, 72242, 72749, 73024, 73155, 73280, 73294, 73339, 73347, 73353, 73363, 73442, 73451, 73580, 73601, 73624, 73823, 73837, 73879, 73898, 73937, 74283, 74693, 75043, 75057, 75072, 75095, 75110, 75121, 75244, 75294, 75336, 75454, 75703, 75709, 75737, 75869, 75926, 76062, 76063, 76073, 76154, 76163, 76353, 76381, 76439, 76548, 76555, 76556, 76557, 76558, 76712, 76914, 76972, 76973, 76974, 76978, 76979, 77085, 77099, 77161, 77352, 77390, 77444, 77538, 77700, 77763, 77772, 77797, 77834, 77925, 77933, 78018, 78070, 78091, 78414, 78543, 78572, 78615, 78647, 78661, 78663, 78997, 79009, 79157, 79165, 79200, 79206, 79219, 79314, 79759, 79991, 80201, 80538, 80566, 80577, 80675, 80870, 80879, 80904, 80910, 80947, 80951, 80956, 81007, 81014, 81200, 81516, 81609, 81867, 81889, 82184, 82188, 82213, 82317, 82349, 82359, 82723, 83039, 83093, 83110, 83180, 83348, 83400, 83434, 83525, 83533, 83891, 83940, 83989, 84318, 84332, 84339, 84699, 84794, 84881, 84927, 85051, 85121, 85175, 85389, 85460, 85466, 85553, 85579, 85610, 85611, 85612, 85629, 85742, 85773, 85790, 85896, 86106, 86169, 86177, 86211, 86255, 86297, 86329, 86509, 86628, 86670, 86732, 86757, 86932, 86951, 86956, 86976, 86979, 87019, 87190, 87345, 87492, 87845, 87946, 87949, 87956, 88082, 88208, 89129, 89246, 89329, 89568, 89700, 89729, 89742, 89880, 89957, 90002, 90072, 90428, 90450, 90499, 90515, 90570, 90905, 91255, 91389, 91599, 91768, 91785, 91834, 91872, 91898, 91904, 91908, 91912, 91915, 91970, 91994, 92070, 92849, 93037, 93141, 93191, 93195, 93252, 93281, 93402, 93487, 93516, 93658, 93690, 93693, 93756, 93794, 93800, 93862, 93948, 93961, 93963, 93976, 94047, 94071, 94100, 94250, 94715, 94795, 94881, 94888, 94897, 94942, 94955, 95318, 95319, 95391, 95435, 95456, 95479, 95729, 95999, 96586, 96653, 96718, 96731, 96912, 97072, 97247, 97262, 97670, 98104, 98181, 98527, 98627, 98631, 98705, 98745, 98763, 98826, 99021, 99236, 99354, 99724, 99896, 99917, 99939, 100074, 100447, 100457, 100546, 100756, 100775, 100817, 100853, 100898, 100931, 101008, 101301, 101350, 101376, 101623, 101680, 101805, 101872, 102085, 102388, 102395, 102460, 102464, 102610, 102614, 102671, 102724, 103069, 103073, 103097, 103255, 103303, 103306, 103325, 103328, 103356, 103460, 103470, 103493, 103506, 103576, 103758, 103819, 103852, 103893, 104023, 104130, 104342, 104356, 104421, 104498, 104582, 104709, 104740, 105105, 105201, 105204, 105519, 105546, 106099, 106121, 106557, 106659, 106777, 107030, 107042, 107044, 107584, 107812, 107851, 107986, 108104, 108111, 108179, 108186, 108447, 108451, 108534, 108711, 108782, 108864, 108920, 108943, 108944, 109020, 109023, 109103, 109320, 109364, 109375, 109411, 109477, 109483, 109493, 109525, 109568, 109639, 109822, 109856, 110212, 110321, 110509, 110667, 111158, 111165, 111252, 111416, 111419, 111625, 111640, 111749, 112015, 112041, 112055, 112123, 112143, 112260, 112267, 112271, 112274, 112434, 112454, 112648, 112676, 112733, 112807, 113243, 113489, 113494, 113538, 113684, 113696, 113877, 113970, 114121, 114186, 114214, 114494, 114512, 114617, 114632, 114665, 114747, 115134, 115140, 115141, 115159, 115169, 115172, 115174, 115352, 115414, 115451, 115453, 115460, 115522, 115661, 116045, 116094, 116161, 116254, 116393, 116546, 116552, 116661, 116694, 116717, 116745, 116758, 116790, 117021, 117027, 117152, 117187, 117294, 117384, 117435, 117633, 117711, 118118, 118124, 118200, 118499, 118518, 118643, 118873, 118989, 118997, 119017, 119032, 119037, 119120, 119129, 119268, 119291, 119451, 119456, 119460, 119486, 119623, 119959, 120003, 120102, 120225, 120258, 120316, 120701, 120959, 120978, 121257, 121274, 121311, 121324, 121348, 121478, 121499, 121897, 121908, 121938]]\n",
      "max_gold_cluster_len : 1256\n",
      "max index: 9\n",
      "number of entities 18519\n"
     ]
    }
   ],
   "source": [
    "print('train_gold_clusters:', [train_gold_clusters[key] for key in train_gold_clusters.keys()][:10])\n",
    "print('max_gold_cluster_len :', max_gold_cluster_len)\n",
    "print(\"max index:\", index)\n",
    "print('number of entities',len(train_gold_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_tensor_data, batch_size=params_test[\"batch_size\"])\n",
    "test_dataloader = DataLoader(test_tensor_data, batch_size=params_test[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    first_batch = batch\n",
    "    break  # Exit the loop after the first iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_context_inputs, candidate_idxs, n_gold, mention_idxs = first_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_context_inputs: Indices of the 64 tokens of the first element (mention + context)\n",
      " tensor([[    2,     1, 28493, 22629,     1,  1732,    42, 22597,  1685,  2961,\n",
      "          9628,  9476,  2527,  1682,  8111,  5834,  9628,  9476,    11,  3627,\n",
      "            12,  2527,  1682,  8111,  5834,    11,  5097,    12,  1808,  1744,\n",
      "          2138,  1715,  8583,  2476,    16,  2384,  3744,  2174,  1690,  6865,\n",
      "          2909,    15,  1690,  2961,  3627,  2527,    11, 19287,    12,  1744,\n",
      "          2138,  1715,  2716,  3162,  2108,    15,  8050,  2366,  1685,  3162,\n",
      "          6309,    15,  2165,     3],\n",
      "        [    2, 28493, 22629,  1732,    42, 22597,  1685,     1,  2961,  9628,\n",
      "          9476,  2527,     1,  1682,  8111,  5834,  9628,  9476,    11,  3627,\n",
      "            12,  2527,  1682,  8111,  5834,    11,  5097,    12,  1808,  1744,\n",
      "          2138,  1715,  8583,  2476,    16,  2384,  3744,  2174,  1690,  6865,\n",
      "          2909,    15,  1690,  2961,  3627,  2527,    11, 19287,    12,  1744,\n",
      "          2138,  1715,  2716,  3162,  2108,    15,  8050,  2366,  1685,  3162,\n",
      "          6309,    15,  2165,     3]])\n",
      "\n",
      "candidate_idxs: Indices of the correct_entity\n",
      " tensor([[3456251,      -1,      -1,      -1],\n",
      "        [ 574449,      -1,      -1,      -1],\n",
      "        [   5410,      -1,      -1,      -1]])\n",
      "\n",
      "n_gold: Number of correct entity\n",
      " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "\n",
      "mention_idxs: Unique mention identifier id\n",
      " tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "print(\"batch_context_inputs: Indices of the 64 tokens of the first element (mention + context)\\n\", batch_context_inputs[:2])\n",
    "print(\"\\ncandidate_idxs: Indices of the correct_entity\\n\", candidate_idxs[:3])\n",
    "print(\"\\nn_gold: Number of correct entity\\n\", n_gold)\n",
    "print(\"\\nmention_idxs: Unique mention identifier id\\n\", mention_idxs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LitArboel\n",
    "\n",
    "### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiEncoderModule(\n",
       "  (context_encoder): BertEncoder(\n",
       "    (bert_model): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(28895, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (additional_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (cand_encoder): BertEncoder(\n",
       "    (bert_model): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(28895, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (additional_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranker = BiEncoderRanker(params_test)\n",
    "model = reranker.model\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### on_train_epoch_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_process_mult import build_index\n",
    "from tqdm import tqdm\n",
    "\n",
    "def embed_and_index(\n",
    "    model,\n",
    "    token_id_vecs,\n",
    "    encoder_type,\n",
    "    batch_size=768,\n",
    "    n_gpu=1,\n",
    "    only_embed=False,\n",
    "    corpus=None,\n",
    "    force_exact_search=False,\n",
    "    probe_mult_factor=1):  \n",
    "    '''\n",
    "    Description\n",
    "    -----------\n",
    "    Designed for embedding the token ID vectors AND INDEXING them for efficient search (tldr : embedding + indexing)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : \n",
    "        Object representing a ML model\n",
    "    token_id_vecs : tensor\n",
    "        Tensors representing tokenized text data\n",
    "    encoder_type : str\n",
    "        Determines the type of encoder to be used from the model. It must be either \"context\" or \"candidate\".\n",
    "    batch_size : int\n",
    "        The number of token vectors to process in each batch.\n",
    "    n_gpu : int\n",
    "        The number of GPUs to be used for processing.\n",
    "    only_embed : bool\n",
    "        If set to True, the function returns only the embeddings, skipping the indexing part.\n",
    "    corpus : list of dict\n",
    "        Collection of entities, which is used to build type-specific search indexes if provided.\n",
    "    force_exact_search : bool\n",
    "        Determine whether to use exact search methods or approximate methods while building the search index.\n",
    "    probe_mult_factor : int\n",
    "        A multiplier factor used in index building for probing in case of approximate search\n",
    "    -------\n",
    "    Returns :\n",
    "        The embeddings of the token ID vectors with dim = (len(token_id_vecs), embedding_dimension)\n",
    "        If encoder_type is \"context\", embeds contains the embeddings of : [CLS] c_left [START] mention [END] c_right [SEP]\n",
    "        If encoder_type is \"candidate\", embeds contains the embeddings of [CLS] e_title [TITLE] e_desc [SEP]\n",
    "        \n",
    "        And optionally : Builds and returns a search index based on the embeddings.\n",
    "    '''\n",
    "    \n",
    "    # Selects the appropriate encoder\n",
    "    if encoder_type == \"context\": # mention\n",
    "        encoder = model.encode_context\n",
    "    elif encoder_type == \"candidate\": # entity\n",
    "        encoder = model.encode_candidate\n",
    "    else:\n",
    "        raise ValueError(\"Invalid encoder_type: expected context or candidate\")\n",
    "\n",
    "    # Compute embeddings\n",
    "    embeds = None\n",
    "    sampler = SequentialSampler(token_id_vecs) # Type of sampler (other one is RandomSampler)\n",
    "    dataloader = DataLoader(\n",
    "        token_id_vecs, sampler=sampler, batch_size=batch_size\n",
    "    )\n",
    "    iter_ = tqdm(dataloader, desc=\"Embedding in batches\")\n",
    "    for step, batch in enumerate(iter_):\n",
    "        batch_embeds = encoder(batch) # batch is being transferred to a GPU (if available) for encoder processing.\n",
    "        embeds = ( #D Concatenate all embeddings into a single array\n",
    "            batch_embeds\n",
    "            if embeds is None\n",
    "            else np.concatenate((embeds, batch_embeds), axis=0) #DD10\n",
    "        )\n",
    "            \n",
    "    if only_embed:\n",
    "        return embeds\n",
    "\n",
    "    if corpus is None: #DD11\n",
    "        # When \"use_types\" is False\n",
    "        index = build_index(\n",
    "            embeds, force_exact_search, probe_mult_factor=probe_mult_factor\n",
    "        )\n",
    "        return embeds, index\n",
    "\n",
    "    # Build type-specific search indexes\n",
    "    search_indexes = {} # Dictionary that will store search indexes (!= indices)for each unique entity type found in the corpus\n",
    "    corpus_idxs = {} # Dictionary to store indices of the corpus elements, grouped by their entity type.\n",
    "    for i, e in enumerate(corpus):\n",
    "        ent_type = e[\"type\"]\n",
    "        if ent_type not in corpus_idxs:\n",
    "            corpus_idxs[ent_type] = []\n",
    "        corpus_idxs[ent_type].append(i)\n",
    "    for ent_type in corpus_idxs:\n",
    "        search_indexes[ent_type] = build_index(\n",
    "            embeds[corpus_idxs[ent_type]],\n",
    "            force_exact_search,\n",
    "            probe_mult_factor=probe_mult_factor,\n",
    "        )\n",
    "        corpus_idxs[ent_type] = np.array(corpus_idxs[ent_type])\n",
    "    return embeds, search_indexes, corpus_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_men_vecs = train_men_vecs[:5000]\n",
    "entity_dict_vecs = entity_dict_vecs[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding in batches: 100%|██████████| 7/7 [01:02<00:00,  8.88s/it]\n",
      "Embedding in batches: 100%|██████████| 7/7 [00:54<00:00,  7.80s/it]\n"
     ]
    }
   ],
   "source": [
    "tr_loss = 0\n",
    "n_gpu = 1\n",
    "\n",
    "'IV.1) Compute mention and entity embeddings and indexes at the start of each epoch'\n",
    "# Compute mention and entity embeddings and indexes at the start of each epoch\n",
    "if params_test['use_types']: # type-specific indexes \n",
    "    train_dict_embeddings, train_dict_indexes, dict_idxs_by_type = embed_and_index(\n",
    "        reranker, \n",
    "        entity_dict_vecs, \n",
    "        encoder_type=\"candidate\", \n",
    "        # n_gpu=n_gpu, \n",
    "        corpus= entity_dictionary, \n",
    "        force_exact_search= params_test[\"force_exact_search\"], \n",
    "        batch_size= params_test[\"embed_batch_size\"], \n",
    "        probe_mult_factor= params_test[\"probe_mult_factor\"]) #D11\n",
    "    train_men_embeddings, train_men_indexes, men_idxs_by_type = embed_and_index(\n",
    "        reranker, \n",
    "        train_men_vecs, \n",
    "        encoder_type=\"context\", \n",
    "        # n_gpu=n_gpu, \n",
    "        corpus= train_processed_data, \n",
    "        force_exact_search= params_test[\"force_exact_search\"], \n",
    "        batch_size= params_test[\"embed_batch_size\"], \n",
    "        probe_mult_factor= params_test[\"probe_mult_factor\"])\n",
    "else: # general indexes\n",
    "    train_dict_embeddings, train_dict_index = embed_and_index(\n",
    "        reranker, \n",
    "        entity_dict_vecs, \n",
    "        encoder_type=\"candidate\", \n",
    "        # n_gpu=n_gpu, \n",
    "        force_exact_search= params_test[\"force_exact_search\"], \n",
    "        batch_size= params_test[\"embed_batch_size\"], \n",
    "        probe_mult_factor= params_test[\"probe_mult_factor\"])\n",
    "    train_men_embeddings, train_men_index = embed_and_index(\n",
    "        reranker, \n",
    "        train_men_vecs, \n",
    "        encoder_type=\"context\", \n",
    "        # n_gpu=n_gpu, \n",
    "        force_exact_search= params_test[\"force_exact_search\"], \n",
    "        batch_size= params_test[\"embed_batch_size\"], \n",
    "        probe_mult_factor= params_test[\"probe_mult_factor\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13/Mar/2024 10:33:01] INFO - Starting KNN search...\n",
      "[13/Mar/2024 10:33:02] INFO - Search finished\n"
     ]
    }
   ],
   "source": [
    "# Number of entities and mentions\n",
    "n_entities = len(entity_dictionary)\n",
    "n_mentions = len(train_processed_data)\n",
    "\n",
    "# Store golden MST links\n",
    "gold_links = {}\n",
    "# Calculate the number of negative entities and mentions to fetch # Divides the k-nn evenly between entities and mentions\n",
    "knn_dict = params_test[\"knn\"]//2\n",
    "knn_men = params_test[\"knn\"] - knn_dict\n",
    "\n",
    "'IV.3) knn search : indice and distance of k closest mentions and entities'\n",
    "logger.info(\"Starting KNN search...\")\n",
    "# INFO: Fetching all sorted mentions to be able to filter to within-doc later=\n",
    "n_men_to_fetch = len(train_men_embeddings) if params_test[\"use_types\"] else knn_men + max_gold_cluster_len\n",
    "n_ent_to_fetch = knn_dict + 1 # +1 accounts for the possibility of self-reference\n",
    "if not params_test[\"use_types\"]:\n",
    "    _, dict_nns = train_dict_index.search(train_men_embeddings, n_ent_to_fetch)\n",
    "    _, men_nns = train_men_index.search(train_men_embeddings, n_men_to_fetch)\n",
    "else:\n",
    "    dict_nns = -1 * np.ones((len(train_men_embeddings), n_ent_to_fetch))\n",
    "    men_nns = -1 * np.ones((len(train_men_embeddings), n_men_to_fetch))\n",
    "    for entity_type in train_men_indexes:\n",
    "        men_embeds_by_type = train_men_embeddings[men_idxs_by_type[entity_type]]\n",
    "        _, dict_nns_by_type = train_dict_indexes[entity_type].search(men_embeds_by_type, n_ent_to_fetch)\n",
    "        _, men_nns_by_type = train_men_indexes[entity_type].search(men_embeds_by_type, min(n_men_to_fetch, len(men_embeds_by_type)))\n",
    "        dict_nns_idxs = np.array(list(map(lambda x: dict_idxs_by_type[entity_type][x], dict_nns_by_type)))\n",
    "        men_nns_idxs = np.array(list(map(lambda x: men_idxs_by_type[entity_type][x], men_nns_by_type)))\n",
    "        for i, idx in enumerate(men_idxs_by_type[entity_type]):\n",
    "            dict_nns[idx] = dict_nns_idxs[i]\n",
    "            men_nns[idx][:len(men_nns_idxs[i])] = men_nns_idxs[i]\n",
    "logger.info(\"Search finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(batch_context_inputs, candidate_idxs, n_gold, mention_idxs):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----------\n",
    "    Processes a batch of input data to generate embeddings, and identifies positive and negative examples for training. \n",
    "    It handles the construction of mention-entity graphs, computes nearest neighbors, and organizes the data for subsequent loss calculation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    - “batch_context_inputs” : Tensor\n",
    "        Tensor containing IDs of (mention + surrounding context) tokens. Shape: (batch_size, context_length) \n",
    "    - “candidate_idxs” : Tensor\n",
    "        Tensor with indices pointing to the entities in the entity dictionary that are considered correct labels for the mention. Shape: (batch_size, candidate_count)\n",
    "    - “n_gold” : Tensor\n",
    "        Number of labels (=entities) associated with the mention. Shape: (batch_size,)\n",
    "    - “mention_idx” : Tensor\n",
    "        Tensor containing a sequence of integers from 0 to N-1 (N = number of mentions in the dataset) serbing as a unique identifier for each mention.\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    - label_inputs : Tensor\n",
    "        Tensor of binary labels indicating the correct candidates. Shape: (batch_size, 1 + knn_dict + knn_men), where 1 represents the positive example and the rest are negative examples.\n",
    "    - context_inputs : Tensor\n",
    "        Processed batch context inputs, filtered to remove mentions with no negative examples. Shape: (filtered_batch_size, context_length).\n",
    "    - negative_men_inputs : Tensor\n",
    "        Tensor of negative mention inputs. Shape: (filtered_batch_size * knn_men,)\n",
    "    - negative_dict_inputs : Tensor\n",
    "        Tensor of negative dictionary (entity) inputs. Shape: (filtered_batch_size * knn_dict,)\n",
    "    - positive_embeds : Tensor\n",
    "        Tensor of embeddings for the positive examples. Shape: (filtered_batch_size, embedding_dim)\n",
    "    - skipped : int \n",
    "        The number of mentions skipped due to lack of valid negative examples.\n",
    "    - skipped_positive_idxs : list(int)\n",
    "        List of indices for positive examples that were skipped.\n",
    "    - skipped_negative_dict_inputs :\n",
    "        Tensor of negative dictionary inputs for skipped examples. Shape may vary based on the number of skipped examples and available negative dictionary entries.\n",
    "    - context_inputs_mask : list(bool)\n",
    "        Mask indicating which entries in batch_context_inputs were retained after filtering out mentions with no negative examples.\n",
    "    \"\"\"\n",
    "    \n",
    "    # mentions within the batch\n",
    "    mention_embeddings = train_men_embeddings[mention_idxs.cpu()]\n",
    "    if len(mention_embeddings.shape) == 1:\n",
    "        mention_embeddings = np.expand_dims(mention_embeddings, axis=0)\n",
    "\n",
    "    positive_idxs = []\n",
    "    negative_dict_inputs = []\n",
    "    negative_men_inputs = []\n",
    "\n",
    "    skipped_positive_idxs = []\n",
    "    skipped_negative_dict_inputs = []\n",
    "\n",
    "    min_neg_mens = float('inf')\n",
    "    skipped = 0\n",
    "    context_inputs_mask = [True]*len(batch_context_inputs)\n",
    "    \n",
    "    'IV.4.B) For each mention within the batch'\n",
    "    # For each mention within the batch\n",
    "    for m_embed_idx, m_embed in enumerate(mention_embeddings):\n",
    "        mention_idx = int(mention_idxs[m_embed_idx])\n",
    "        #CC11 ground truth entities of the mention \"mention_idx\"\n",
    "        gold_idxs = set(train_processed_data[mention_idx]['label_idxs'][:n_gold[m_embed_idx]])\n",
    "        \n",
    "        # TEMPORARY: Assuming that there is only 1 gold label, TODO: Incorporate multiple case\n",
    "        assert n_gold[m_embed_idx] == 1\n",
    "\n",
    "        if mention_idx in gold_links:\n",
    "            gold_link_idx = gold_links[mention_idx]\n",
    "        else:\n",
    "            'IV.4.B.a) Create the graph with positive edges'\n",
    "            # This block creates all the positive edges of the mention in this iteration\n",
    "            # Run MST on mention clusters of all the gold entities of the current query mention to find its positive edge\n",
    "            rows, cols, data, shape = [], [], [], (n_entities+n_mentions, n_entities+n_mentions)\n",
    "            seen = set()\n",
    "\n",
    "            # Set whether the gold edge should be the nearest or the farthest neighbor\n",
    "            sim_order = 1 if params_test[\"farthest_neighbor\"] else -1 #A26\n",
    "\n",
    "            for cluster_ent in gold_idxs:\n",
    "                #CC12 IDs of all the mentions inside the gold cluster with entity id = \"cluster_ent\"\n",
    "                cluster_mens = train_gold_clusters[cluster_ent]\n",
    "\n",
    "                if params_test[\"within_doc\"]:\n",
    "                    # Filter the gold cluster to within-doc\n",
    "                    cluster_mens, _ = filter_by_context_doc_id(cluster_mens,\n",
    "                                                                train_context_doc_ids[mention_idx],\n",
    "                                                                train_context_doc_ids)\n",
    "                \n",
    "                # weights for all the mention-entity links inside the cluster of the current mention\n",
    "                to_ent_data = train_men_embeddings[cluster_mens] @ train_dict_embeddings[cluster_ent].T\n",
    "\n",
    "                # weights for all the mention-mention links inside the cluster of the current mention\n",
    "                to_men_data = train_men_embeddings[cluster_mens] @ train_men_embeddings[cluster_mens].T\n",
    "\n",
    "                if params_test['gold_arbo_knn'] is not None:\n",
    "                    # Descending order of similarity if nearest-neighbor, else ascending order\n",
    "                    sorti = np.argsort(sim_order * to_men_data, axis=1)\n",
    "                    sortv = np.take_along_axis(to_men_data, sorti, axis=1)\n",
    "                    if params_test[\"rand_gold_arbo\"]:\n",
    "                        randperm = np.random.permutation(sorti.shape[1])\n",
    "                        sortv, sorti = sortv[:, randperm], sorti[:, randperm]\n",
    "\n",
    "                for i in range(len(cluster_mens)):\n",
    "                    from_node = n_entities + cluster_mens[i]\n",
    "                    to_node = cluster_ent\n",
    "                    # Add mention-entity link\n",
    "                    rows.append(from_node)\n",
    "                    cols.append(to_node)\n",
    "                    data.append(-1 * to_ent_data[i])\n",
    "                    if params_test['gold_arbo_knn'] is None:\n",
    "                        # Add forward and reverse mention-mention links over the entire MST\n",
    "                        for j in range(i+1, len(cluster_mens)):\n",
    "                            to_node = n_entities + cluster_mens[j]\n",
    "                            if (from_node, to_node) not in seen:\n",
    "                                score = to_men_data[i,j]\n",
    "                                rows.append(from_node)\n",
    "                                cols.append(to_node)\n",
    "                                data.append(-1 * score) # Negatives needed for SciPy's Minimum Spanning Tree computation\n",
    "                                seen.add((from_node, to_node))\n",
    "                                seen.add((to_node, from_node))\n",
    "                    else:\n",
    "                        # Approximate the MST using <gold_arbo_knn> nearest mentions from the gold cluster\n",
    "                        added = 0\n",
    "                        approx_k = min(params_test['gold_arbo_knn']+1, len(cluster_mens))\n",
    "                        for j in range(approx_k):\n",
    "                            if added == approx_k - 1:\n",
    "                                break\n",
    "                            to_node = n_entities + cluster_mens[sorti[i, j]]\n",
    "                            if to_node == from_node:\n",
    "                                continue\n",
    "                            added += 1\n",
    "                            if (from_node, to_node) not in seen:\n",
    "                                score = sortv[i, j]\n",
    "                                rows.append(from_node)\n",
    "                                cols.append(to_node)\n",
    "                                data.append(\n",
    "                                    -1 * score)  # Negatives needed for SciPy's Minimum Spanning Tree computation\n",
    "                                seen.add((from_node, to_node))\n",
    "\n",
    "            'IV.4.B.b) Fine tuning with inference procedure to get a mst'\n",
    "            # Creates MST with entity constraint (inference procedure)\n",
    "            csr = csr_matrix((-sim_order * np.array(data), (rows, cols)), shape=shape)\n",
    "            # Note: minimum_spanning_tree expects distances as edge weights\n",
    "            mst = minimum_spanning_tree(csr).tocoo()\n",
    "            # Note: cluster_linking_partition expects similarities as edge weights # Convert directed to undirected graph\n",
    "            rows, cols, data = cluster_linking_partition(np.concatenate((mst.row, mst.col)), # cluster_linking_partition is imported from eval_cluster_linking\n",
    "                                                            np.concatenate((mst.col, mst.row)),\n",
    "                                                            np.concatenate((sim_order * mst.data, sim_order * mst.data)),\n",
    "                                                            n_entities,\n",
    "                                                            directed=True,\n",
    "                                                            silent=True)\n",
    "            assert np.array_equal(rows - n_entities, cluster_mens)\n",
    "            \n",
    "            for i in range(len(rows)):\n",
    "                men_idx = rows[i] - n_entities\n",
    "                if men_idx in gold_links:\n",
    "                    continue\n",
    "                assert men_idx >= 0\n",
    "                add_link = True\n",
    "                # Store the computed positive edges for the mentions in the clusters only if they have the same gold entities as the query mention\n",
    "                for l in train_processed_data[men_idx]['label_idxs'][:train_processed_data[men_idx]['n_labels']]:\n",
    "                    if l not in gold_idxs:\n",
    "                        add_link = False\n",
    "                        break\n",
    "                if add_link:\n",
    "                    gold_links[men_idx] = cols[i]\n",
    "            gold_link_idx = gold_links[mention_idx]\n",
    "            \n",
    "        'IV.4.B.c) Retrieve the pre-computed nearest neighbors'\n",
    "        knn_dict_idxs = dict_nns[mention_idx]\n",
    "        knn_dict_idxs = knn_dict_idxs.astype(np.int64).flatten()\n",
    "        knn_men_idxs = men_nns[mention_idx][men_nns[mention_idx] != -1]\n",
    "        knn_men_idxs = knn_men_idxs.astype(np.int64).flatten()\n",
    "        if params_test['within_doc']:\n",
    "            knn_men_idxs, _ = filter_by_context_doc_id(knn_men_idxs,\n",
    "                                                    train_context_doc_ids[mention_idx],\n",
    "                                                    train_context_doc_ids, return_numpy=True)\n",
    "        'IV.4.B.d) Add negative examples'\n",
    "        neg_mens = list(knn_men_idxs[~np.isin(knn_men_idxs, np.concatenate([train_gold_clusters[gi] for gi in gold_idxs]))][:knn_men])\n",
    "        # Track queries with no valid mention negatives\n",
    "        if len(neg_mens) == 0:\n",
    "            context_inputs_mask[m_embed_idx] = False\n",
    "            skipped_negative_dict_inputs += list(knn_dict_idxs[~np.isin(knn_dict_idxs, list(gold_idxs))][:knn_dict])\n",
    "            skipped_positive_idxs.append(gold_link_idx)\n",
    "            skipped += 1\n",
    "            continue\n",
    "        else:\n",
    "            min_neg_mens = min(min_neg_mens, len(neg_mens))\n",
    "        negative_men_inputs.append(knn_men_idxs[~np.isin(knn_men_idxs, np.concatenate([train_gold_clusters[gi] for gi in gold_idxs]))][:knn_men])\n",
    "        negative_dict_inputs += list(knn_dict_idxs[~np.isin(knn_dict_idxs, list(gold_idxs))][:knn_dict])\n",
    "        # Add the positive example\n",
    "        positive_idxs.append(gold_link_idx)\n",
    "\n",
    "    \n",
    "    'IV.4.C) Skip this iteration if no suitable negative examples found'\n",
    "    if len(negative_men_inputs) == 0 :\n",
    "        return None #DD8 instead of continue\n",
    "    \n",
    "    # Sets the minimum number of negative mentions found across all processed mentions in the current batch\n",
    "    knn_men = min_neg_mens\n",
    "    \n",
    "    # This step ensures that each mention is compared against a uniform number of negative mentions\n",
    "    filtered_negative_men_inputs = []\n",
    "    for row in negative_men_inputs:\n",
    "        filtered_negative_men_inputs += list(row[:knn_men])\n",
    "    negative_men_inputs = filtered_negative_men_inputs\n",
    "\n",
    "    # Assertions for Data Integrity\n",
    "    assert len(negative_dict_inputs) == (len(mention_embeddings) - skipped) * knn_dict\n",
    "    assert len(negative_men_inputs) == (len(mention_embeddings) - skipped) * knn_men\n",
    "\n",
    "    total_skipped += skipped\n",
    "    total_knn_men_negs += knn_men\n",
    "\n",
    "    # Convert to tensors\n",
    "    negative_dict_inputs = torch.tensor(list(map(lambda x: entity_dict_vecs[x].numpy(), negative_dict_inputs)))\n",
    "    negative_men_inputs = torch.tensor(list(map(lambda x: train_men_vecs[x].numpy(), negative_men_inputs)))\n",
    "    \n",
    "    # Labels indicating the correct candidates. Used for computing loss.\n",
    "    positive_embeds = []\n",
    "    for pos_idx in positive_idxs:\n",
    "        if pos_idx < n_entities:\n",
    "            pos_embed = reranker.encode_candidate(entity_dict_vecs[pos_idx:pos_idx + 1], requires_grad=True)\n",
    "        else:\n",
    "            pos_embed = reranker.encode_context(train_men_vecs[pos_idx - n_entities:pos_idx - n_entities + 1], requires_grad=True)\n",
    "        positive_embeds.append(pos_embed)\n",
    "    positive_embeds = torch.cat(positive_embeds)\n",
    "    \n",
    "    # Remove mentions with no negative examples\n",
    "    context_inputs = batch_context_inputs[context_inputs_mask]\n",
    "    context_inputs = context_inputs\n",
    "    \n",
    "    # Tensor containing binary values that act as indicator variables in the paper:\n",
    "    # Contains Indicator variable such that I_{u,m_i} = 1 if(u,mi) ∈ E'_{m_i} and I{u,m_i} = 0 otherwise.\n",
    "    label_inputs = torch.tensor([[1]+[0]*(knn_dict+knn_men)]*len(context_inputs), dtype=torch.float32)\n",
    "    \n",
    "    return {'label_inputs':label_inputs, 'context_inputs' : context_inputs, \"negative_men_inputs\" : negative_men_inputs,\n",
    "            'negative_dict_inputs' : negative_dict_inputs, 'positive_embeds' : positive_embeds, 'skipped' : skipped,\n",
    "            'skipped_positive_idxs' : skipped_positive_idxs, 'skipped_negative_dict_inputs' : skipped_negative_dict_inputs,\n",
    "            'context_inputs_mask' : context_inputs_mask\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 3456251 is out of bounds for axis 0 with size 5000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_context_inputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_context_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcandidate_idxs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcandidate_idxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_gold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_gold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmention_idxs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmention_idxs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [29], line 90\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(batch_context_inputs, candidate_idxs, n_gold, mention_idxs)\u001b[0m\n\u001b[1;32m     85\u001b[0m     cluster_mens, _ \u001b[38;5;241m=\u001b[39m filter_by_context_doc_id(cluster_mens,\n\u001b[1;32m     86\u001b[0m                                                 train_context_doc_ids[mention_idx],\n\u001b[1;32m     87\u001b[0m                                                 train_context_doc_ids)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# weights for all the mention-entity links inside the cluster of the current mention\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m to_ent_data \u001b[38;5;241m=\u001b[39m train_men_embeddings[cluster_mens] \u001b[38;5;241m@\u001b[39m \u001b[43mtrain_dict_embeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcluster_ent\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# weights for all the mention-mention links inside the cluster of the current mention\u001b[39;00m\n\u001b[1;32m     93\u001b[0m to_men_data \u001b[38;5;241m=\u001b[39m train_men_embeddings[cluster_mens] \u001b[38;5;241m@\u001b[39m train_men_embeddings[cluster_mens]\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3456251 is out of bounds for axis 0 with size 5000"
     ]
    }
   ],
   "source": [
    "f = forward(batch_context_inputs = batch_context_inputs, \n",
    "            candidate_idxs = candidate_idxs, \n",
    "            n_gold = n_gold, \n",
    "            mention_idxs = mention_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISSUE : Can't reduce the dataset.\n",
    "# So Let's do the test on the class directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/Mar/2024 16:13:19] INFO - PyTorch version 2.2.0 available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for an_em contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/an_em/an_em.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for anat_em contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/anat_em/anat_em.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for ask_a_patient contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/ask_a_patient/ask_a_patient.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bc5cdr contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bc5cdr/bc5cdr.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bc7_litcovid contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bc7_litcovid/bc7_litcovid.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bio_sim_verb contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bio_sim_verb/bio_sim_verb.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bio_simlex contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bio_simlex/bio_simlex.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bioasq_2021_mesinesp contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bioasq_2021_mesinesp/bioasq_2021_mesinesp.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bioasq_task_b contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bioasq_task_b/bioasq_task_b.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bioasq_task_c_2017 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bioasq_task_c_2017/bioasq_task_c_2017.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bioid contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bioid/bioid.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bioinfer contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bioinfer/bioinfer.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for biology_how_why_corpus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/biology_how_why_corpus/biology_how_why_corpus.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for biomrc contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/biomrc/biomrc.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bionlp_shared_task_2009 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bionlp_shared_task_2009/bionlp_shared_task_2009.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bionlp_st_2011_epi contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bionlp_st_2011_epi/bionlp_st_2011_epi.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bionlp_st_2011_ge contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bionlp_st_2011_ge/bionlp_st_2011_ge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bionlp_st_2011_id contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bionlp_st_2011_id/bionlp_st_2011_id.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bionlp_st_2011_rel contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bionlp_st_2011_rel/bionlp_st_2011_rel.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bionlp_st_2013_cg contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bionlp_st_2013_cg/bionlp_st_2013_cg.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bionlp_st_2013_ge contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bionlp_st_2013_ge/bionlp_st_2013_ge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bionlp_st_2013_gro contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bionlp_st_2013_gro/bionlp_st_2013_gro.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bionlp_st_2013_pc contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bionlp_st_2013_pc/bionlp_st_2013_pc.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bionlp_st_2019_bb contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bionlp_st_2019_bb/bionlp_st_2019_bb.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for biored contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/biored/biored.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for biorelex contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/biorelex/biorelex.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bioscope contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bioscope/bioscope.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for biosses contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/biosses/biosses.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for blurb contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/blurb/blurb.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bronco contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bronco/bronco.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for cantemist contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/cantemist/cantemist.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for cardiode contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/cardiode/cardiode.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for cas contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/cas/cas.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for cellfinder contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/cellfinder/cellfinder.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for chebi_nactem contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/chebi_nactem/chebi_nactem.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for chemdner contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/chemdner/chemdner.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for chemprot contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/chemprot/chemprot.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for chia contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/chia/chia.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for citation_gia_test_collection contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/citation_gia_test_collection/citation_gia_test_collection.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for codiesp contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/codiesp/codiesp.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for cpi contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/cpi/cpi.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for ctebmsp contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/ctebmsp/ctebmsp.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for czi_drsm contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/czi_drsm/czi_drsm.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for ddi_corpus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/ddi_corpus/ddi_corpus.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for distemist contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/distemist/distemist.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for drugprot contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/drugprot/drugprot.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for ebm_pico contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/ebm_pico/ebm_pico.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for ehr_rel contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/ehr_rel/ehr_rel.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for essai contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/essai/essai.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for euadr contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/euadr/euadr.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for evidence_inference contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/evidence_inference/evidence_inference.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for gad contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/gad/gad.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for genetag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/genetag/genetag.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for genia_ptm_event_corpus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/genia_ptm_event_corpus/genia_ptm_event_corpus.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for genia_relation_corpus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/genia_relation_corpus/genia_relation_corpus.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for genia_term_corpus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/genia_term_corpus/genia_term_corpus.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for geokhoj_v1 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/geokhoj_v1/geokhoj_v1.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for ggponc2 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/ggponc2/ggponc2.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for gnormplus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/gnormplus/gnormplus.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for hallmarks_of_cancer contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/hallmarks_of_cancer/hallmarks_of_cancer.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for hprd50 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/hprd50/hprd50.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for iepa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/iepa/iepa.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for jnlpba contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/jnlpba/jnlpba.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for linnaeus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/linnaeus/linnaeus.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for lll contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/lll/lll.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for mayosrs contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/mayosrs/mayosrs.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for med_qa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/med_qa/med_qa.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for medal contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/medal/medal.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for meddialog contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/meddialog/meddialog.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for meddocan contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/meddocan/meddocan.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for medhop contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/medhop/medhop.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for medical_data contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/medical_data/medical_data.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for mediqa_nli contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/mediqa_nli/mediqa_nli.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for mediqa_qa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/mediqa_qa/mediqa_qa.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for mediqa_rqe contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/mediqa_rqe/mediqa_rqe.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for medmentions contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/medmentions/medmentions.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for mednli contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/mednli/mednli.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for meqsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/meqsum/meqsum.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for minimayosrs contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/minimayosrs/minimayosrs.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for mirna contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/mirna/mirna.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for mlee contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/mlee/mlee.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for mqp contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/mqp/mqp.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for msh_wsd contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/msh_wsd/msh_wsd.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for muchmore contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/muchmore/muchmore.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for multi_xscience contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/multi_xscience/multi_xscience.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for n2c2_2006_deid contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/n2c2_2006_deid/n2c2_2006_deid.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for n2c2_2006_smokers contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/n2c2_2006_smokers/n2c2_2006_smokers.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for n2c2_2008 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/n2c2_2008/n2c2_2008.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for n2c2_2009 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/n2c2_2009/n2c2_2009.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for n2c2_2010 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/n2c2_2010/n2c2_2010.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for n2c2_2011 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/n2c2_2011/n2c2_2011.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for n2c2_2014_deid contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/n2c2_2014_deid/n2c2_2014_deid.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for n2c2_2018_track1 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/n2c2_2018_track1/n2c2_2018_track1.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for n2c2_2018_track2 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/n2c2_2018_track2/n2c2_2018_track2.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for ncbi_disease contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/ncbi_disease/ncbi_disease.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for nlm_gene contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/nlm_gene/nlm_gene.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for nlm_wsd contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/nlm_wsd/nlm_wsd.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for nlmchem contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/nlmchem/nlmchem.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for ntcir_13_medweb contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/ntcir_13_medweb/ntcir_13_medweb.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for osiris contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/osiris/osiris.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for paramed contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/paramed/paramed.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for pcr contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/pcr/pcr.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for pdr contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/pdr/pdr.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for pharmaconer contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/pharmaconer/pharmaconer.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for pico_extraction contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/pico_extraction/pico_extraction.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for pmc_patients contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/pmc_patients/pmc_patients.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for progene contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/progene/progene.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for psytar contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/psytar/psytar.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for pubhealth contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/pubhealth/pubhealth.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for pubmed_qa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/pubmed_qa/pubmed_qa.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for pubtator_central contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/pubtator_central/pubtator_central.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for quaero contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/quaero/quaero.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for scai_chemical contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/scai_chemical/scai_chemical.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for scai_disease contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/scai_disease/scai_disease.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for scicite contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/scicite/scicite.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for scielo contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/scielo/scielo.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for scifact contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/scifact/scifact.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for sciq contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/sciq/sciq.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for scitail contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/scitail/scitail.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for sem_eval_2024_task_2 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/sem_eval_2024_task_2/sem_eval_2024_task_2.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for seth_corpus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/seth_corpus/seth_corpus.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for spl_adr_200db contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/spl_adr_200db/spl_adr_200db.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for swedish_medical_ner contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/swedish_medical_ner/swedish_medical_ner.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for tmvar_v1 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/tmvar_v1/tmvar_v1.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for tmvar_v2 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/tmvar_v2/tmvar_v2.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for tmvar_v3 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/tmvar_v3/tmvar_v3.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for twadrl contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/twadrl/twadrl.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for umnsrs contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/umnsrs/umnsrs.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for verspoor_2013 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/verspoor_2013/verspoor_2013.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from Lightning_datamodule import ArboelDataModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate / filter_by_context_doc_id / read_data / loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    reranker,\n",
    "    valid_dict_vecs, \n",
    "    valid_men_vecs, \n",
    "    # device, #no longer used\n",
    "    logger, \n",
    "    # knn,  #not even used\n",
    "    # n_gpu, \n",
    "    entity_data, \n",
    "    query_data,\n",
    "    silent=False, \n",
    "    use_types=False, \n",
    "    embed_batch_size=768, \n",
    "    force_exact_search=False, \n",
    "    probe_mult_factor=1,\n",
    "    within_doc=False, \n",
    "    context_doc_ids=None ):\n",
    "    '''\n",
    "    Description \n",
    "    -----------\n",
    "    1) Computes embeddings and indexes for entities and mentions. \n",
    "    2) Performs k-nearest neighbors (k-NN) search to establish relationships between them.\n",
    "    3) Constructs graphs based on these relationships.\n",
    "    4) Evaluates the model's accuracy by analyzing how effectively the model can link mentions to the correct entities.\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    reranker : BiEncoderRanker\n",
    "        NN-based ranking model\n",
    "    valid_dict_vec : list or ndarray\n",
    "        Ground truth dataset containing the entities\n",
    "    valid_men_vecs : list or ndarray\n",
    "        Dataset containing mentions\n",
    "    device : str\n",
    "        cpu or gpu\n",
    "    logger : 'Logger' object\n",
    "        Logging object used to record messages\n",
    "    knn : int\n",
    "        Number of neighbors\n",
    "    n_gpu : int\n",
    "        Number of gpu\n",
    "    entity_data : list or dict\n",
    "        Entities from the data\n",
    "    query_data : list or dict\n",
    "        Queries / mentions against which the entities are evaluated\n",
    "    silent=False : bool \n",
    "        When set to \"True\", likely suppresses the output or logging of progress updates to keep the console output clean.\n",
    "    use_types=False : bool\n",
    "        A boolean flag that indicates whether or not to use type-specific indexes for entities and mentions\n",
    "    embed_batch_size=128 : int\n",
    "        The batch size to use when processing embeddings.\n",
    "    force_exact_search=False : bool\n",
    "        force the embedding process to use exact search methods rather than approximate methods.\n",
    "    probe_mult_factor=1 : int\n",
    "        A multiplier factor used in index building for probing in case of approximate search (bigger = better but slower)\n",
    "    within_doc=False : bool\n",
    "        Boolean flag that indicates whether the evaluation should be constrained to within-document contexts\n",
    "    context_doc_ids=None : bool\n",
    "        This would be used in conjunction with within_doc to limit evaluations within the same document.\n",
    "    '''\n",
    "    torch.cuda.empty_cache() # Empty the CUDA cache to free up GPU memory\n",
    "    \n",
    "    n_entities = len(valid_dict_vecs) # total number of entities\n",
    "    n_mentions = len(valid_men_vecs) # total number of mentions\n",
    "    max_knn = 8 # max number of neighbors\n",
    "    \n",
    "    joint_graphs = {} # Store results of the NN search and distance between entities and mentions\n",
    "    \n",
    "    for k in [0, 1, 2, 4, 8]:\n",
    "        joint_graphs[k] = { #DD3\n",
    "            \"rows\": np.array([]),\n",
    "            \"cols\": np.array([]),\n",
    "            \"data\": np.array([]),\n",
    "            \"shape\": (n_entities + n_mentions, n_entities + n_mentions),\n",
    "        }\n",
    "        \n",
    "    \n",
    "    '1) Computes embeddings and indexes for entities and mentions. '\n",
    "    '''\n",
    "    This block is preparing the data for evaluation by transforming raw vectors into a format that can be efficiently used for retrieval and comparison operations\n",
    "    '''\n",
    "    if use_types: # corpus = entity data\n",
    "                  # corpus is a collection of entities, which is used to build type-specific search indexes if provided.\n",
    "        '''\n",
    "        With a Corpus : Multiple type-specific indexes are created, allowing for more targeted and efficient searches within specific categories of entities.\n",
    "        'dict_embeds' and 'men_embeds': The resulting entity and mention embeddings.\n",
    "        'dict_indexes' and 'men_indexes': Dictionary that will store search indexes (!= indices)for each unique entity type found in the corpus\n",
    "        'dict_idxs_by_type' and 'men_idxs_by_type': Dictionary to store indices of the corpus elements, grouped by their entity type.\n",
    "        !!! idxs = indices / indexes = indexes !!!\n",
    "        '''\n",
    "        logger.info(\"Eval: Dictionary: Embedding and building index\") # For entities\n",
    "        dict_embeds, dict_indexes, dict_idxs_by_type = data_process.embed_and_index(\n",
    "            reranker,\n",
    "            valid_dict_vecs,\n",
    "            encoder_type=\"candidate\",\n",
    "            # n_gpu=n_gpu,\n",
    "            corpus=entity_data, \n",
    "            force_exact_search=force_exact_search, \n",
    "            batch_size=embed_batch_size, \n",
    "            probe_mult_factor=probe_mult_factor, \n",
    "            )\n",
    "        logger.info(\"Eval: Queries: Embedding and building index\") # For mentions\n",
    "        men_embeds, men_indexes, men_idxs_by_type = data_process.embed_and_index(\n",
    "            reranker,\n",
    "            valid_men_vecs,\n",
    "            encoder_type=\"context\",\n",
    "            # n_gpu=n_gpu,\n",
    "            corpus=query_data,\n",
    "            force_exact_search=force_exact_search,\n",
    "            batch_size=embed_batch_size,\n",
    "            probe_mult_factor=probe_mult_factor,\n",
    "        )\n",
    "    else: # corpus = None\n",
    "        '''\n",
    "        Without a Corpus: A single, general index is created for all embeddings, suitable for broad searches across the entire dataset.\n",
    "        'dict_embeds' and 'men_embeds': The resulting entity and mention embeddings.\n",
    "        'dict_index' and 'men_index': Dictionary that will store search index\n",
    "        '''\n",
    "        logger.info(\"Eval: Dictionary: Embedding and building index\")\n",
    "        dict_embeds, dict_index = data_process.embed_and_index(\n",
    "            reranker,\n",
    "            valid_dict_vecs,\n",
    "            \"candidate\",\n",
    "            # n_gpu=n_gpu,\n",
    "            force_exact_search=force_exact_search,\n",
    "            batch_size=embed_batch_size,\n",
    "            probe_mult_factor=probe_mult_factor,\n",
    "        )\n",
    "        logger.info(\"Eval: Queries: Embedding and building index\")\n",
    "        men_embeds, men_index = data_process.embed_and_index(\n",
    "            reranker,\n",
    "            valid_men_vecs,\n",
    "            \"context\",\n",
    "            # n_gpu=n_gpu,\n",
    "            force_exact_search=force_exact_search,\n",
    "            batch_size=embed_batch_size,\n",
    "            probe_mult_factor=probe_mult_factor,\n",
    "        )\n",
    "\n",
    "\n",
    "    '2) Performs k-nearest neighbors (k-NN) search to establish relationships between mentions and entities.'\n",
    "    logger.info(\"Eval: Starting KNN search...\") # An informational message is logged to indicate that the k-NN search is starting.\n",
    "    # Fetch recall_k (default 16) knn entities for all mentions\n",
    "    # Fetch (k+1) NN mention candidates; fetching all mentions for within_doc to filter down later\n",
    "    n_men_to_fetch = len(men_embeds) if within_doc else max_knn + 1 # Number of mentions to fetch\n",
    "    if not use_types: # Only one index so only need one search\n",
    "        nn_ent_dists, nn_ent_idxs = dict_index.search(men_embeds, 1) #DD4/DD5 #return the distance and the indice of the closest entity for all mentions in men_embeds\n",
    "        nn_men_dists, nn_men_idxs = men_index.search(men_embeds, n_men_to_fetch) # return the distances and the indices of the k closest mentions for all mentions in men_embeds\n",
    "    else: #C Several indexes corresponding to the different entities in entity_data so we can use the specific search index\n",
    "        # DD6\n",
    "        # DD7\n",
    "        nn_ent_idxs = -1 * np.ones((len(men_embeds), 1), dtype=int) # Indice of the closest entity for all mentions in men_embeds\n",
    "        nn_ent_dists = -1 * np.ones((len(men_embeds), 1), dtype=\"float64\") # Distance of the closest entity for all mentions in men_embeds\n",
    "        nn_men_idxs = -1 * np.ones((len(men_embeds), n_men_to_fetch), dtype=int) # Indice of k closest mentions for all mentions in men_embeds\n",
    "        nn_men_dists = -1 * np.ones((len(men_embeds), n_men_to_fetch), dtype=\"float64\") # Distance of the k closest mentions for all mentions in men_embeds\n",
    "        for entity_type in men_indexes:\n",
    "            #CC3 Creates a new list only containing the mentions for which type = entity_types\n",
    "            men_embeds_by_type = men_embeds[men_idxs_by_type[entity_type]] # Only want to search the mentions that belongs to a specific type of entity.\n",
    "            # Returns the distance and the indice of the closest entity for all mentions in men_embeds by entity type\n",
    "            nn_ent_dists_by_type, nn_ent_idxs_by_type = dict_indexes[entity_type].search(men_embeds_by_type, 1) \n",
    "            nn_ent_idxs_by_type = np.array( #CC4 DD8\n",
    "                list( #DD9\n",
    "                    map( # lambda x : acts as a function\n",
    "                        lambda x: dict_idxs_by_type[entity_type][x], nn_ent_idxs_by_type\n",
    "                    ) # nn_ent_idxs_by_type is the iterable being processed by the map function\n",
    "                    # Each element within nn_ent_idxs_by_type is passed to the lambda function as x.\n",
    "                ) # map alone would return an object, that's why need a list\n",
    "            )\n",
    "            # Returns the distance and the indice of the k closest mentions for all mention in men_embeds by entity type\n",
    "            # Note that here we may not necessarily have k mentions in each entity type which is why we use min(k,len(men_embeds_by_type))\n",
    "            nn_men_dists_by_type, nn_men_idxs_by_type = men_indexes[entity_type].search(\n",
    "                men_embeds_by_type, min(n_men_to_fetch, len(men_embeds_by_type))\n",
    "            )\n",
    "            nn_men_idxs_by_type = np.array(\n",
    "                list(\n",
    "                    map(lambda x: men_idxs_by_type[entity_type][x], nn_men_idxs_by_type)\n",
    "                )\n",
    "            )\n",
    "            for i, idx in enumerate(men_idxs_by_type[entity_type]): #CC5\n",
    "                nn_ent_idxs[idx] = nn_ent_idxs_by_type[i]\n",
    "                nn_ent_dists[idx] = nn_ent_dists_by_type[i]\n",
    "                nn_men_idxs[idx][: len(nn_men_idxs_by_type[i])] = nn_men_idxs_by_type[i]\n",
    "                nn_men_dists[idx][: len(nn_men_dists_by_type[i])] = nn_men_dists_by_type[i]\n",
    "    logger.info(\"Eval: Search finished\") # An informational message is logged to indicate that the k-NN search is finished\n",
    "\n",
    "    '3) Constructs graphs based on these relationships.'\n",
    "    '''\n",
    "    nn_ent_dists contain information about distance of the closest entity\n",
    "    nn_ent_idxs contain information about indice of the closest entity\n",
    "    nn_men_dists contain information about distance of the k nearest mentions\n",
    "    nn_men_idxs contain information about indice of the k nearest mentions\n",
    "    - We can fill in the \"rows\" part (=start nodes) of the graph in the order of the mentions\n",
    "    - We can fill in the \"cols\" part (=end nodes) of the graph with nn_ent_idxs and nn_men_idxs\n",
    "    - We can fill in the \"data\" part (=weights) of the graph with nn_ent_dists and nn_men_dists\n",
    "    '''\n",
    "    logger.info(\"Eval: Building graphs\")\n",
    "    for men_query_idx, men_embed in enumerate(\n",
    "        tqdm(men_embeds, total=len(men_embeds), desc=\"Eval: Building graphs\")\n",
    "    ):\n",
    "        # Get nearest entity candidate\n",
    "        dict_cand_idx = nn_ent_idxs[men_query_idx][0] # Use of [0] to retrieve a scalar and not an 1D array\n",
    "        dict_cand_score = nn_ent_dists[men_query_idx][0]\n",
    "\n",
    "        # Filter candidates to remove -1s, mention query, within doc (if reqd.), and keep only the top k candidates\n",
    "        filter_mask_neg1 = nn_men_idxs[men_query_idx] != -1 # bool ndarray. Ex : np.array([True, False, True, False])\n",
    "        men_cand_idxs = nn_men_idxs[men_query_idx][filter_mask_neg1] # Only keep the elements != -1\n",
    "        men_cand_scores = nn_men_dists[men_query_idx][filter_mask_neg1]\n",
    "\n",
    "        if within_doc:\n",
    "            men_cand_idxs, wd_mask = filter_by_context_doc_id(\n",
    "                men_cand_idxs,\n",
    "                context_doc_ids[men_query_idx],\n",
    "                context_doc_ids,\n",
    "                return_numpy=True,\n",
    "            )\n",
    "            men_cand_scores = men_cand_scores[wd_mask]\n",
    "        \n",
    "        # Filter self-reference + limits the number of candidate to 'max_knn'\n",
    "        filter_mask = men_cand_idxs != men_query_idx\n",
    "        men_cand_idxs, men_cand_scores = (\n",
    "            men_cand_idxs[filter_mask][:max_knn],\n",
    "            men_cand_scores[filter_mask][:max_knn],\n",
    "        )\n",
    "\n",
    "        # Add edges to the graphs\n",
    "        for k in joint_graphs:\n",
    "            joint_graph = joint_graphs[k] # There is no \"s\" in \"joint_graph\", it's not the same ! \n",
    "            # Add mention-entity edge\n",
    "            joint_graph[\"rows\"] = np.append( # Mentions are offset by the total number of entities to differentiate mention nodes from entity nodes\n",
    "                joint_graph[\"rows\"], [n_entities + men_query_idx]\n",
    "            )  \n",
    "            joint_graph[\"cols\"] = np.append(joint_graph[\"cols\"], dict_cand_idx)\n",
    "            joint_graph[\"data\"] = np.append(joint_graph[\"data\"], dict_cand_score)\n",
    "            if k > 0:\n",
    "                # Add mention-mention edges\n",
    "                joint_graph[\"rows\"] = np.append(\n",
    "                    joint_graph[\"rows\"],\n",
    "                    [n_entities + men_query_idx] * len(men_cand_idxs[:k]), # creates an array where the starting node (current mention) is repeated len(men_cand_idxs[:k]) times\n",
    "                ) \n",
    "                joint_graph[\"cols\"] = np.append(\n",
    "                    joint_graph[\"cols\"], n_entities + men_cand_idxs[:k]\n",
    "                )\n",
    "                joint_graph[\"data\"] = np.append(\n",
    "                    joint_graph[\"data\"], men_cand_scores[:k]\n",
    "                )\n",
    "    \n",
    "    \"4) Evaluates the model's accuracy by analyzing how effectively the model can link mentions to the correct entities.\"    \n",
    "    \n",
    "    best_result = {'accuracy': 0}\n",
    "    \n",
    "    dict_acc = {}\n",
    "    max_eval_acc = -1.\n",
    "    for k in joint_graphs:\n",
    "        logger.info(f\"\\nEval: Graph (k={k}):\")\n",
    "        # Partition graph based on cluster-linking constraints (inference procedure)\n",
    "        partitioned_graph, clusters = eval_cluster_linking.partition_graph(\n",
    "            joint_graphs[k], n_entities, directed=True, return_clusters=True)\n",
    "        # Infer predictions from clusters\n",
    "        result = eval_cluster_linking.analyzeClusters(clusters, entity_data, query_data, k)\n",
    "        acc = float(result['accuracy'].split(' ')[0])\n",
    "        best_result['accuracy'] = acc if acc >= best_result['accuracy'] else best_result\n",
    "        dict_acc[f'k{k}'] = acc\n",
    "        max_eval_acc = max(acc, max_eval_acc)\n",
    "        logger.info(f\"Eval: accuracy for graph@k={k}: {acc}%\")\n",
    "    logger.info(f\"Eval: Best accuracy: {max_eval_acc}%\")\n",
    "    embed_and_index_dict = {'dict_embeds': dict_embeds, 'dict_indexes': dict_indexes, 'dict_idxs_by_type': dict_idxs_by_type} if use_types else {'dict_embeds': dict_embeds, 'dict_index': dict_index}\n",
    "    return max_eval_acc, dict_acc, embed_and_index_dict\n",
    "\n",
    "\n",
    "\n",
    "def filter_by_context_doc_id(mention_idxs, doc_id, doc_id_list, return_numpy=False):\n",
    "    '''\n",
    "    Description \n",
    "    -----------\n",
    "    Filters and returns mention indices that belong to a specific document identified by \"doc_id\".\n",
    "    Ensures that the analysis are constrained within the context of that particular document.\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    - mention_idxs : ndarray(int) of dim = (number of mentions)\n",
    "    Represents the indices of mentions\n",
    "    - doc_id : int \n",
    "    Indice of the target document\n",
    "    - doc_id_list : ndarray(int) of dim = (number of mentions)\n",
    "    Array of integers, where each element is a document ID associated with the corresponding mention in mention_idxs. \n",
    "    The length of doc_id_list should match the total number of mentions referenced in mention_idxs.\n",
    "    - return_numpy : bool\n",
    "    A flag indicating whether to return the filtered list of mention indices as a NumPy array. \n",
    "    If True, the function returns a NumPy array; otherwise, it returns a list\n",
    "    -------\n",
    "    Outputs: \n",
    "    - mask : ndarray(bool) of dim = (number of mentions)\n",
    "    Mask indicating where each mention's document ID (from doc_id_list) matches the target doc_id\n",
    "    - mention_idxs : \n",
    "    Only contains mention indices that belong to the target document (=doc_id).\n",
    "    '''\n",
    "    mask = [doc_id_list[i] == doc_id for i in mention_idxs]\n",
    "    if isinstance(mention_idxs, list): # Test if mention_idxs = list. Return a bool\n",
    "        mention_idxs = np.array(mention_idxs) \n",
    "    mention_idxs = mention_idxs[mask] # possible only if mention_idxs is an array, not a list\n",
    "    if not return_numpy:\n",
    "        mention_idxs = list(mention_idxs)\n",
    "    return mention_idxs, mask\n",
    "\n",
    "\n",
    "\n",
    "def read_data(split, params, logger):\n",
    "    '''\n",
    "    Description \n",
    "    -----------\n",
    "    Loads dataset samples from a specified path\n",
    "    Optionally filters out samples without labels\n",
    "    Checks if the dataset supports multiple labels per sample\n",
    "    \"has_mult_labels\" : bool\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    split : str\n",
    "        Indicates the portion of the dataset to load (\"train\", \"test\", \"valid\"), used by utils.read_dataset to determine which data to read.\n",
    "    params : dict(str)\n",
    "        Contains configuration options\n",
    "    logger : \n",
    "        An object used for logging messages about the process, such as the number of samples read.\n",
    "    '''\n",
    "    samples = utils.read_dataset(split, params[\"data_path\"]) #DD21\n",
    "    # Check if dataset has multiple ground-truth labels\n",
    "    has_mult_labels = \"labels\" in samples[0].keys()\n",
    "    if params[\"filter_unlabeled\"]:\n",
    "        # Filter samples without gold entities\n",
    "        samples = list(\n",
    "            filter(lambda sample: (len(sample[\"labels\"]) > 0) if has_mult_labels else (sample[\"label\"] is not None),\n",
    "                   samples))\n",
    "    logger.info(f\"Read %d {split} samples.\" % len(samples))\n",
    "    return samples, has_mult_labels\n",
    "\n",
    "\n",
    "\n",
    "def loss_function(reranker, \n",
    "    params, \n",
    "    forward_output, \n",
    "    data_module, \n",
    "    n_entities, \n",
    "    knn_dict, \n",
    "    batch_context_inputs, \n",
    "    accumulate_grad_batches\n",
    "    ):\n",
    "    '''\n",
    "    Compute the loss function during the training.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    - reranker : BiEncoderRanker\n",
    "    NN-based ranking model\n",
    "    - params : dict\n",
    "    Contains most of the relevant keys for training (embed_batch_size, train_batch_size, n_gpu, force_exact_search etc...)\n",
    "    - forward_output : dict\n",
    "    Output of the forward() method\n",
    "    - data_module : Instance of ArboelDataModule class\n",
    "    - n_entities : int\n",
    "    Total number of entities\n",
    "    - knn_dict : int (self.knn_dict = self.hparams[\"knn\"]//2)\n",
    "    number of negative entities to fetch. It divides the k-nn evenly between entities and mentions \n",
    "    - accumulate_grad_batches : int\n",
    "    Number of steps to accumulate gradients\n",
    "    '''\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss_dual_negs = loss_ent_negs = 0\n",
    "    # loss of a batch includes both negative mention and entity inputs (alongside positive examples ofc)\n",
    "    loss_dual_negs, _ = reranker(forward_output['context_inputs'], label_input=forward_output['label_inputs'], mst_data={\n",
    "        'positive_embeds': forward_output['positive_embeds'],\n",
    "        'negative_dict_inputs': forward_output['negative_dict_inputs'],\n",
    "        'negative_men_inputs': forward_output['negative_men_inputs']\n",
    "    }, pos_neg_loss=params[\"pos_neg_loss\"]) #A27\n",
    "    skipped_context_inputs = []\n",
    "    if forward_output['skipped'] > 0 and not params[\"within_doc_skip_strategy\"]: #A28\n",
    "        skipped_negative_dict_inputs = torch.tensor(\n",
    "            list(map(lambda x: data_module.entity_dict_vecs[x].numpy(), skipped_negative_dict_inputs)))\n",
    "        skipped_positive_embeds = []\n",
    "        for pos_idx in forward_output['skipped_positive_idxs']:\n",
    "            if pos_idx < n_entities:\n",
    "                pos_embed = reranker.encode_candidate(data_module.entity_dict_vecs[pos_idx:pos_idx + 1],\n",
    "                                                        requires_grad=True)\n",
    "            else:\n",
    "                pos_embed = reranker.encode_context(\n",
    "                    data_module.train_men_vecs[pos_idx - n_entities:pos_idx - n_entities + 1], requires_grad=True)\n",
    "            skipped_positive_embeds.append(pos_embed)\n",
    "        skipped_positive_embeds = torch.cat(skipped_positive_embeds)\n",
    "        skipped_context_inputs = batch_context_inputs[~np.array(forward_output['context_inputs_mask'])]\n",
    "        skipped_context_inputs = skipped_context_inputs\n",
    "        skipped_label_inputs = torch.tensor([[1] + [0] * (knn_dict)] * len(skipped_context_inputs),\n",
    "                                    dtype=torch.float32)\n",
    "        #DD18 loss of a batch that only includes negative entity inputs.\n",
    "        loss_ent_negs, _ = reranker(skipped_context_inputs, label_input=skipped_label_inputs, mst_data={\n",
    "            'positive_embeds': skipped_positive_embeds,\n",
    "            'negative_dict_inputs': skipped_negative_dict_inputs,\n",
    "            'negative_men_inputs': None\n",
    "        }, pos_neg_loss=params[\"pos_neg_loss\"])\n",
    "            \n",
    "    # len(context_input) = Number of mentions in the batch that successfully found negative entities and mentions.\n",
    "    # len(skipped_context_inputs): Number of mentions in the batch that only found negative entities.\n",
    "    loss = ((loss_dual_negs * len(forward_output['context_inputs']) + loss_ent_negs * len(skipped_context_inputs)) / (len(forward_output['context_inputs']) + len(skipped_context_inputs))) / accumulate_grad_batches\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Model Training'\n",
    "class LitArboel(L.LightningModule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        params\n",
    "        ):\n",
    "        '''\n",
    "        - params : dict\n",
    "        Contains most of the relevant keys for training (embed_batch_size, train_batch_size, n_gpu, force_exact_search etc...)\n",
    "        - data_module : Instance of ArboelDataModule class\n",
    "        '''\n",
    "        super(LitArboel, self).__init__()\n",
    "        self.save_hyperparameters(params) #DD1\n",
    "        \n",
    "        self.reranker = BiEncoderRanker(params)\n",
    "        # self.tokenizer = self.reranker.tokenizer\n",
    "        self.model = self.reranker.model\n",
    "        \n",
    "        \n",
    "    def forward(self, batch_context_inputs, candidate_idxs, n_gold, mention_idxs):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        -----------\n",
    "        Processes a batch of input data to generate embeddings, and identifies positive and negative examples for training. \n",
    "        It handles the construction of mention-entity graphs, computes nearest neighbors, and organizes the data for subsequent loss calculation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        - “batch_context_inputs” : Tensor\n",
    "            Tensor containing IDs of (mention + surrounding context) tokens. Shape: (batch_size, context_length) \n",
    "        - “candidate_idxs” : Tensor\n",
    "            Tensor with indices pointing to the entities in the entity dictionary that are considered correct labels for the mention. Shape: (batch_size, candidate_count)\n",
    "        - “n_gold” : Tensor\n",
    "            Number of labels (=entities) associated with the mention. Shape: (batch_size,)\n",
    "        - “mention_idx” : Tensor\n",
    "            Tensor containing a sequence of integers from 0 to N-1 (N = number of mentions in the dataset) serbing as a unique identifier for each mention.\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        - label_inputs : Tensor\n",
    "            Tensor of binary labels indicating the correct candidates. Shape: (batch_size, 1 + knn_dict + knn_men), where 1 represents the positive example and the rest are negative examples.\n",
    "        - context_inputs : Tensor\n",
    "            Processed batch context inputs, filtered to remove mentions with no negative examples. Shape: (filtered_batch_size, context_length).\n",
    "        - negative_men_inputs : Tensor\n",
    "            Tensor of negative mention inputs. Shape: (filtered_batch_size * knn_men,)\n",
    "        - negative_dict_inputs : Tensor\n",
    "            Tensor of negative dictionary (entity) inputs. Shape: (filtered_batch_size * knn_dict,)\n",
    "        - positive_embeds : Tensor\n",
    "            Tensor of embeddings for the positive examples. Shape: (filtered_batch_size, embedding_dim)\n",
    "        - skipped : int \n",
    "            The number of mentions skipped due to lack of valid negative examples.\n",
    "        - skipped_positive_idxs : list(int)\n",
    "            List of indices for positive examples that were skipped.\n",
    "        - skipped_negative_dict_inputs :\n",
    "            Tensor of negative dictionary inputs for skipped examples. Shape may vary based on the number of skipped examples and available negative dictionary entries.\n",
    "        - context_inputs_mask : list(bool)\n",
    "            Mask indicating which entries in batch_context_inputs were retained after filtering out mentions with no negative examples.\n",
    "        \"\"\"\n",
    "        \n",
    "        # mentions within the batch\n",
    "        mention_embeddings = self.train_men_embeddings[mention_idxs.cpu()]\n",
    "        if len(mention_embeddings.shape) == 1:\n",
    "            mention_embeddings = np.expand_dims(mention_embeddings, axis=0)\n",
    "\n",
    "        positive_idxs = []\n",
    "        negative_dict_inputs = []\n",
    "        negative_men_inputs = []\n",
    "\n",
    "        skipped_positive_idxs = []\n",
    "        skipped_negative_dict_inputs = []\n",
    "\n",
    "        min_neg_mens = float('inf')\n",
    "        skipped = 0\n",
    "        context_inputs_mask = [True]*len(batch_context_inputs)\n",
    "        \n",
    "        'IV.4.B) For each mention within the batch'\n",
    "        # For each mention within the batch\n",
    "        for m_embed_idx, m_embed in enumerate(mention_embeddings):\n",
    "            mention_idx = int(mention_idxs[m_embed_idx])\n",
    "            #CC11 ground truth entities of the mention \"mention_idx\"\n",
    "            gold_idxs = set(self.trainer.datamodule.train_processed_data[mention_idx]['label_idxs'][:n_gold[m_embed_idx]])\n",
    "            \n",
    "            # TEMPORARY: Assuming that there is only 1 gold label, TODO: Incorporate multiple case\n",
    "            assert n_gold[m_embed_idx] == 1\n",
    "\n",
    "            if mention_idx in self.gold_links:\n",
    "                gold_link_idx = self.gold_links[mention_idx]\n",
    "            else:\n",
    "                'IV.4.B.a) Create the graph with positive edges'\n",
    "                # This block creates all the positive edges of the mention in this iteration\n",
    "                # Run MST on mention clusters of all the gold entities of the current query mention to find its positive edge\n",
    "                rows, cols, data, shape = [], [], [], (self.n_entities+self.n_mentions, self.n_entities+self.n_mentions)\n",
    "                seen = set()\n",
    "\n",
    "                # Set whether the gold edge should be the nearest or the farthest neighbor\n",
    "                sim_order = 1 if self.hparams[\"farthest_neighbor\"] else -1 #A26\n",
    "\n",
    "                for cluster_ent in gold_idxs:\n",
    "                    #CC12 IDs of all the mentions inside the gold cluster with entity id = \"cluster_ent\"\n",
    "                    cluster_mens = self.hparams.train_gold_clusters[cluster_ent]\n",
    "\n",
    "                    if self.hparams.param[\"within_doc\"]:\n",
    "                        # Filter the gold cluster to within-doc\n",
    "                        cluster_mens, _ = filter_by_context_doc_id(cluster_mens,\n",
    "                                                                    self.trainer.datamodule.train_context_doc_ids[mention_idx],\n",
    "                                                                    self.trainer.datamodule.train_context_doc_ids)\n",
    "                    \n",
    "                    # weights for all the mention-entity links inside the cluster of the current mention\n",
    "                    to_ent_data = self.train_men_embeddings[cluster_mens] @ self.train_dict_embeddings[cluster_ent].T\n",
    "\n",
    "                    # weights for all the mention-mention links inside the cluster of the current mention\n",
    "                    to_men_data = self.train_men_embeddings[cluster_mens] @ self.train_men_embeddings[cluster_mens].T\n",
    "\n",
    "                    if self.hparams['gold_arbo_knn'] is not None:\n",
    "                        # Descending order of similarity if nearest-neighbor, else ascending order\n",
    "                        sorti = np.argsort(sim_order * to_men_data, axis=1)\n",
    "                        sortv = np.take_along_axis(to_men_data, sorti, axis=1)\n",
    "                        if self.hparams[\"rand_gold_arbo\"]:\n",
    "                            randperm = np.random.permutation(sorti.shape[1])\n",
    "                            sortv, sorti = sortv[:, randperm], sorti[:, randperm]\n",
    "\n",
    "                    for i in range(len(cluster_mens)):\n",
    "                        from_node = self.n_entities + cluster_mens[i]\n",
    "                        to_node = cluster_ent\n",
    "                        # Add mention-entity link\n",
    "                        rows.append(from_node)\n",
    "                        cols.append(to_node)\n",
    "                        data.append(-1 * to_ent_data[i])\n",
    "                        if self.hparams['gold_arbo_knn'] is None:\n",
    "                            # Add forward and reverse mention-mention links over the entire MST\n",
    "                            for j in range(i+1, len(cluster_mens)):\n",
    "                                to_node = self.n_entities + cluster_mens[j]\n",
    "                                if (from_node, to_node) not in seen:\n",
    "                                    score = to_men_data[i,j]\n",
    "                                    rows.append(from_node)\n",
    "                                    cols.append(to_node)\n",
    "                                    data.append(-1 * score) # Negatives needed for SciPy's Minimum Spanning Tree computation\n",
    "                                    seen.add((from_node, to_node))\n",
    "                                    seen.add((to_node, from_node))\n",
    "                        else:\n",
    "                            # Approximate the MST using <gold_arbo_knn> nearest mentions from the gold cluster\n",
    "                            added = 0\n",
    "                            approx_k = min(self.hparams['gold_arbo_knn']+1, len(cluster_mens))\n",
    "                            for j in range(approx_k):\n",
    "                                if added == approx_k - 1:\n",
    "                                    break\n",
    "                                to_node = self.n_entities + cluster_mens[sorti[i, j]]\n",
    "                                if to_node == from_node:\n",
    "                                    continue\n",
    "                                added += 1\n",
    "                                if (from_node, to_node) not in seen:\n",
    "                                    score = sortv[i, j]\n",
    "                                    rows.append(from_node)\n",
    "                                    cols.append(to_node)\n",
    "                                    data.append(\n",
    "                                        -1 * score)  # Negatives needed for SciPy's Minimum Spanning Tree computation\n",
    "                                    seen.add((from_node, to_node))\n",
    "\n",
    "                'IV.4.B.b) Fine tuning with inference procedure to get a mst'\n",
    "                # Creates MST with entity constraint (inference procedure)\n",
    "                csr = csr_matrix((-sim_order * np.array(data), (rows, cols)), shape=shape)\n",
    "                # Note: minimum_spanning_tree expects distances as edge weights\n",
    "                mst = minimum_spanning_tree(csr).tocoo()\n",
    "                # Note: cluster_linking_partition expects similarities as edge weights # Convert directed to undirected graph\n",
    "                rows, cols, data = cluster_linking_partition(np.concatenate((mst.row, mst.col)), # cluster_linking_partition is imported from eval_cluster_linking\n",
    "                                                                np.concatenate((mst.col, mst.row)),\n",
    "                                                                np.concatenate((sim_order * mst.data, sim_order * mst.data)),\n",
    "                                                                self.n_entities,\n",
    "                                                                directed=True,\n",
    "                                                                silent=True)\n",
    "                assert np.array_equal(rows - self.n_entities, cluster_mens)\n",
    "                \n",
    "                for i in range(len(rows)):\n",
    "                    men_idx = rows[i] - self.n_entities\n",
    "                    if men_idx in self.gold_links:\n",
    "                        continue\n",
    "                    assert men_idx >= 0\n",
    "                    add_link = True\n",
    "                    # Store the computed positive edges for the mentions in the clusters only if they have the same gold entities as the query mention\n",
    "                    for l in self.trainer.datamodule.train_processed_data[men_idx]['label_idxs'][:self.trainer.datamodule.train_processed_data[men_idx]['n_labels']]:\n",
    "                        if l not in gold_idxs:\n",
    "                            add_link = False\n",
    "                            break\n",
    "                    if add_link:\n",
    "                        self.gold_links[men_idx] = cols[i]\n",
    "                gold_link_idx = self.gold_links[mention_idx]\n",
    "                \n",
    "            'IV.4.B.c) Retrieve the pre-computed nearest neighbors'\n",
    "            knn_dict_idxs = self.dict_nns[mention_idx]\n",
    "            knn_dict_idxs = knn_dict_idxs.astype(np.int64).flatten()\n",
    "            knn_men_idxs = self.men_nns[mention_idx][self.men_nns[mention_idx] != -1]\n",
    "            knn_men_idxs = knn_men_idxs.astype(np.int64).flatten()\n",
    "            if self.hparams['within_doc']:\n",
    "                knn_men_idxs, _ = filter_by_context_doc_id(knn_men_idxs,\n",
    "                                                        self.trainer.datamodule.train_context_doc_ids[mention_idx],\n",
    "                                                        self.trainer.datamodule.train_context_doc_ids, return_numpy=True)\n",
    "            'IV.4.B.d) Add negative examples'\n",
    "            neg_mens = list(knn_men_idxs[~np.isin(knn_men_idxs, np.concatenate([self.trainer.datamodule.train_gold_clusters[gi] for gi in gold_idxs]))][:self.knn_men])\n",
    "            # Track queries with no valid mention negatives\n",
    "            if len(neg_mens) == 0:\n",
    "                context_inputs_mask[m_embed_idx] = False\n",
    "                skipped_negative_dict_inputs += list(knn_dict_idxs[~np.isin(knn_dict_idxs, list(gold_idxs))][:self.knn_dict])\n",
    "                skipped_positive_idxs.append(gold_link_idx)\n",
    "                skipped += 1\n",
    "                continue\n",
    "            else:\n",
    "                min_neg_mens = min(min_neg_mens, len(neg_mens))\n",
    "            negative_men_inputs.append(knn_men_idxs[~np.isin(knn_men_idxs, np.concatenate([self.trainer.datamodule.train_gold_clusters[gi] for gi in gold_idxs]))][:self.knn_men])\n",
    "            negative_dict_inputs += list(knn_dict_idxs[~np.isin(knn_dict_idxs, list(gold_idxs))][:self.knn_dict])\n",
    "            # Add the positive example\n",
    "            positive_idxs.append(gold_link_idx)\n",
    "\n",
    "        \n",
    "        'IV.4.C) Skip this iteration if no suitable negative examples found'\n",
    "        if len(negative_men_inputs) == 0 :\n",
    "            return None #DD8 instead of continue\n",
    "        \n",
    "        # Sets the minimum number of negative mentions found across all processed mentions in the current batch\n",
    "        self.knn_men = min_neg_mens\n",
    "        \n",
    "        # This step ensures that each mention is compared against a uniform number of negative mentions\n",
    "        filtered_negative_men_inputs = []\n",
    "        for row in negative_men_inputs:\n",
    "            filtered_negative_men_inputs += list(row[:self.knn_men])\n",
    "        negative_men_inputs = filtered_negative_men_inputs\n",
    "\n",
    "        # Assertions for Data Integrity\n",
    "        assert len(negative_dict_inputs) == (len(mention_embeddings) - skipped) * self.knn_dict\n",
    "        assert len(negative_men_inputs) == (len(mention_embeddings) - skipped) * self.knn_men\n",
    "\n",
    "        self.total_skipped += skipped\n",
    "        self.total_knn_men_negs += self.knn_men\n",
    "\n",
    "        # Convert to tensors\n",
    "        negative_dict_inputs = torch.tensor(list(map(lambda x: self.trainer.datamodule.entity_dict_vecs[x].numpy(), negative_dict_inputs)))\n",
    "        negative_men_inputs = torch.tensor(list(map(lambda x: self.trainer.datamodule.train_men_vecs[x].numpy(), negative_men_inputs)))\n",
    "        \n",
    "        # Labels indicating the correct candidates. Used for computing loss.\n",
    "        positive_embeds = []\n",
    "        for pos_idx in positive_idxs:\n",
    "            if pos_idx < self.n_entities:\n",
    "                pos_embed = self.reranker.encode_candidate(self.trainer.datamodule.entity_dict_vecs[pos_idx:pos_idx + 1], requires_grad=True)\n",
    "            else:\n",
    "                pos_embed = self.reranker.encode_context(self.trainer.datamodule.train_men_vecs[pos_idx - self.n_entities:pos_idx - self.n_entities + 1], requires_grad=True)\n",
    "            positive_embeds.append(pos_embed)\n",
    "        positive_embeds = torch.cat(positive_embeds)\n",
    "        \n",
    "        # Remove mentions with no negative examples\n",
    "        context_inputs = batch_context_inputs[context_inputs_mask]\n",
    "        context_inputs = context_inputs\n",
    "        \n",
    "        # Tensor containing binary values that act as indicator variables in the paper:\n",
    "        # Contains Indicator variable such that I_{u,m_i} = 1 if(u,mi) ∈ E'_{m_i} and I{u,m_i} = 0 otherwise.\n",
    "        label_inputs = torch.tensor([[1]+[0]*(self.knn_dict+self.knn_men)]*len(context_inputs), dtype=torch.float32)\n",
    "        \n",
    "        return {'label_inputs':label_inputs, 'context_inputs' : context_inputs, \"negative_men_inputs\" : negative_men_inputs,\n",
    "                'negative_dict_inputs' : negative_dict_inputs, 'positive_embeds' : positive_embeds, 'skipped' : skipped,\n",
    "                'skipped_positive_idxs' : skipped_positive_idxs, 'skipped_negative_dict_inputs' : skipped_negative_dict_inputs,\n",
    "                'context_inputs_mask' : context_inputs_mask\n",
    "                }\n",
    "\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        # batch = tuple(t.to(device) for t in batch) : automated in pytorch #DD5\n",
    "        \n",
    "        # Initialize the parameters\n",
    "        # batch is a subsample from tensor_dataset\n",
    "        batch_context_inputs, candidate_idxs, n_gold, mention_idxs = batch\n",
    "        \n",
    "        f = self.forward(batch_context_inputs, candidate_idxs, n_gold, mention_idxs)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_function(self.reranker, \n",
    "            self.hparams, \n",
    "            f, \n",
    "            self.trainer.datamodule, \n",
    "            self.n_entities, \n",
    "            self.knn_dict, \n",
    "            batch_context_inputs, \n",
    "            self.trainer.accumulate_grad_batches\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def configure_optimizers(self): #DD2\n",
    "        \n",
    "        # Define optimizer\n",
    "        optimizer = get_bert_optimizer(\n",
    "        [self.model],\n",
    "        self.hparams[\"type_optimization\"],\n",
    "        self.hparams[\"learning_rate\"],\n",
    "        fp16=self.hparams.get(\"fp16\"),\n",
    "        )\n",
    "        \n",
    "        # Define scheduler\n",
    "        num_train_steps = int(len(self.trainer.datamodule.train_tensor_data) / self.hparams[\"train_batch_size\"] / self.trainer.accumulate_grad_batches) * self.trainer.max_epochs\n",
    "        num_warmup_steps = int(num_train_steps * self.hparams[\"warmup_proportion\"])\n",
    "\n",
    "        scheduler = WarmupLinearSchedule(\n",
    "            optimizer, warmup_steps=num_warmup_steps, t_total=num_train_steps,\n",
    "        )\n",
    "        logger.info(\" Num optimization steps = %d\" % num_train_steps)\n",
    "        logger.info(\" Num warmup steps = %d\", num_warmup_steps)\n",
    "        return [optimizer], [{'scheduler': scheduler, 'interval': 'step'}]\n",
    "\n",
    "\n",
    "\n",
    "    def on_train_start(self):\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    \n",
    "    # Don't necessarily need this\n",
    "    # def on_fit_start(self):\n",
    "    #     # Compute n_gpu once, at the start of training, and store it as an instance attribute\n",
    "    #     if self.trainer.devices is None:\n",
    "    #         self.n_gpu = 0\n",
    "    #     elif isinstance(self.trainer.devices, list):\n",
    "    #         self.n_gpu = len(self.trainer.devices)\n",
    "    #     elif isinstance(self.trainer.devices, int):\n",
    "    #         self.n_gpu = self.trainer.devices\n",
    "    #     else:\n",
    "    #         # For other configurations, such as auto-select or when specific GPUs are selected as a string\n",
    "    #         # It's safer to rely on the actual allocated devices by the trainer\n",
    "    #         self.n_gpu = len(self.trainer.accelerator_connector.parallel_devices)\n",
    "            \n",
    "            \n",
    "    def on_train_epoch_start(self):\n",
    "        # To do at the start of each epoch\n",
    "        self.tr_loss = 0\n",
    "        \n",
    "        'IV.1) Compute mention and entity embeddings and indexes at the start of each epoch'\n",
    "        # Compute mention and entity embeddings and indexes at the start of each epoch\n",
    "        if self.hparams['use_types']: # type-specific indexes \n",
    "            self.train_dict_embeddings, self.train_dict_indexes, self.dict_idxs_by_type = data_process.embed_and_index(\n",
    "                self.reranker, \n",
    "                self.trainer.datamodule.entity_dict_vecs, \n",
    "                encoder_type=\"candidate\", \n",
    "                # n_gpu=self.n_gpu, \n",
    "                corpus= self.trainer.datamodule.entity_dictionary, \n",
    "                force_exact_search= self.hparams['force_exact_search'], \n",
    "                batch_size= self.hparams['embed_batch_size'], \n",
    "                probe_mult_factor= self.hparams['probe_mult_factor']) #D11\n",
    "            self.train_men_embeddings, self.train_men_indexes, self.men_idxs_by_type = data_process.embed_and_index(\n",
    "                self.reranker, \n",
    "                self.trainer.datamodule.train_men_vecs, \n",
    "                encoder_type=\"context\", \n",
    "                #n_gpu=self.n_gpu, \n",
    "                corpus= self.trainer.datamodule.train_processed_data, \n",
    "                force_exact_search= self.hparams['force_exact_search'], \n",
    "                batch_size= self.hparams['embed_batch_size'], \n",
    "                probe_mult_factor= self.hparams['probe_mult_factor'])\n",
    "        \n",
    "        else: # general indexes\n",
    "            self.train_dict_embeddings, self.train_dict_index = data_process.embed_and_index(\n",
    "                self.reranker, \n",
    "                self.trainer.datamodule.entity_dict_vecs, \n",
    "                encoder_type=\"candidate\", \n",
    "                # n_gpu=self.n_gpu, \n",
    "                force_exact_search= \n",
    "                self.hparams['force_exact_search'], \n",
    "                batch_size= self.hparams['embed_batch_size'], \n",
    "                probe_mult_factor= self.hparams['probe_mult_factor'])\n",
    "            self.train_men_embeddings, self.train_men_index = data_process.embed_and_index(\n",
    "                self.reranker, \n",
    "                self.trainer.datamodule.train_men_vecs, \n",
    "                encoder_type=\"context\", \n",
    "                # n_gpu=self.n_gpu, \n",
    "                force_exact_search = self.hparams['force_exact_search'], \n",
    "                batch_size= self.hparams['embed_batch_size'], \n",
    "                probe_mult_factor= self.hparams['probe_mult_factor'])\n",
    "        \n",
    "        # Number of entities and mentions\n",
    "        self.n_entities = len(self.trainer.datamodule.entity_dictionary)\n",
    "        self.n_mentions = len(self.trainer.datamodule.train_processed_data)\n",
    "        \n",
    "        # Store golden MST links\n",
    "        self.gold_links = {}\n",
    "        # Calculate the number of negative entities and mentions to fetch # Divides the k-nn evenly between entities and mentions\n",
    "        self.knn_dict = self.hparams[\"knn\"]//2\n",
    "        self.knn_men = self.hparams[\"knn\"] - self.knn_dict\n",
    "        \n",
    "        'IV.3) knn search : indice and distance of k closest mentions and entities'\n",
    "        logger.info(\"Starting KNN search...\")\n",
    "        # INFO: Fetching all sorted mentions to be able to filter to within-doc later=\n",
    "        n_men_to_fetch = len(self.train_men_embeddings) if self.hparams[\"use_types\"] else self.knn_men + self.trainer.datamodule.max_gold_cluster_len\n",
    "        n_ent_to_fetch = self.knn_dict + 1 # +1 accounts for the possibility of self-reference\n",
    "        if not self.hparams[\"use_types\"]:\n",
    "            _, self.dict_nns = self.train_dict_index.search(self.train_men_embeddings, n_ent_to_fetch)\n",
    "            _, self.men_nns = self.train_men_index.search(self.train_men_embeddings, n_men_to_fetch)\n",
    "        else:\n",
    "            self.dict_nns = -1 * np.ones((len(self.train_men_embeddings), n_ent_to_fetch))\n",
    "            self.men_nns = -1 * np.ones((len(self.train_men_embeddings), n_men_to_fetch))\n",
    "            for entity_type in self.train_men_indexes:\n",
    "                self.men_embeds_by_type = self.train_men_embeddings[self.men_idxs_by_type[entity_type]]\n",
    "                _, self.dict_nns_by_type = self.train_dict_indexes[entity_type].search(self.men_embeds_by_type, n_ent_to_fetch)\n",
    "                _, self.men_nns_by_type = self.train_men_indexes[entity_type].search(self.men_embeds_by_type, min(n_men_to_fetch, len(self.men_embeds_by_type)))\n",
    "                self.dict_nns_idxs = np.array(list(map(lambda x: self.dict_idxs_by_type[entity_type][x], self.dict_nns_by_type)))\n",
    "                self.men_nns_idxs = np.array(list(map(lambda x: self.men_idxs_by_type[entity_type][x], self.men_nns_by_type)))\n",
    "                for i, idx in enumerate(self.men_idxs_by_type[entity_type]):\n",
    "                    self.dict_nns[idx] = self.dict_nns_idxs[i]\n",
    "                    self.men_nns[idx][:len(self.men_nns_idxs[i])] = self.men_nns_idxs[i]\n",
    "        logger.info(\"Search finished\")\n",
    "        \n",
    "        self.total_skipped = self.total_knn_men_negs = 0\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        # To do at the end of each epoch\n",
    "        # May not need it \n",
    "        pass\n",
    "    \n",
    "    def on_after_backward(self): #DD11\n",
    "        # After .backward()\n",
    "        if (self.trainer.global_step + 1) % self.trainer.accumulate_grad_batches == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                self.parameters(), self.trainer.grad_clip_norm\n",
    "            )\n",
    "            \n",
    "    # RR2 : Most of the info is already in the Trainer's callback        \n",
    "    # def on_train_batch_end(self, outputs, batch, batch_idx, dataloader_idx = 0):\n",
    "    #     # end of training batch\n",
    "    #     'IV.4.E) Information about the training (step, epoch, average_loss)'\n",
    "    #     n_print_iters = self.hparams[\"print_interval\"] * self.trainer.accumulate_grad_batches #29\n",
    "    #     if (batch_idx + 1) % n_print_iters == 0:\n",
    "    #         # DD13\n",
    "    #         self.log(\"train/average_loss\", self.tr_loss / n_print_iters, on_step=False, on_epoch=True, prog_bar=True)\n",
    "    #         if self.total_skipped > 0:\n",
    "    #             self.log(\"train/queries_without_negs\", self.total_skipped / n_print_iters, on_step=False, on_epoch=True)\n",
    "    #             self.log(\"train/negative_mentions_per_query\", self.total_knn_men_negs / n_print_iters, on_step=False, on_epoch=True)\n",
    "            \n",
    "    #         # Reset your tracking variables for the next interval\n",
    "    #         self.total_skipped = 0\n",
    "    #         self.total_knn_men_negs = 0\n",
    "    #         self.tr_loss = 0\n",
    "\n",
    "    #     # DD14\n",
    "    #     '''\n",
    "    #     # Regular checks on model performance against a validation dataset without interrupting the training more often than desired\n",
    "    #     if self.hparams[\"eval_interval\"] != -1: #A31\n",
    "    #         if (batch_idx + 1) % (self.hparams[\"eval_interval\"] * self.hparams[\"gradient_accumulation_steps\"]) == 0:\n",
    "    #             logger.info(\"Evaluation on the development dataset\")\n",
    "    #             evaluate(\n",
    "    #                 self.reranker, self.trainer.datamodule.entity_dict_vecs, self.trainer.datamodule.valid_men_vecs, device=self.device, logger=logger, knn=self.hparams[\"knn\"], n_gpu=self.n_gpu,\n",
    "    #                 entity_data=self.trainer.datamodule.entity_dictionary, query_data=self.valid_processed_data, silent=self.hparams[\"silent\"],\n",
    "    #                 use_types=self.hparams['use_types'] or self.hparams[\"use_types_for_eval\"], embed_batch_size=self.hparams[\"embed_batch_size\"],\n",
    "    #                 force_exact_search=self.hparams['use_types'] or self.hparams[\"use_types_for_eval\"] or self.hparams[\"force_exact_search\"],\n",
    "    #                 probe_mult_factor=self.hparams['probe_mult_factor'], within_doc=self.hparams['within_doc'],\n",
    "    #                 context_doc_ids=self.trainer.datamodule.valid_context_doc_ids\n",
    "    #             )\n",
    "    #             self.model.train()\n",
    "    #             logger.info(\"\\n\")\n",
    "    #     '''\n",
    "        \n",
    "    #     pass\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx): #DD14\n",
    "\n",
    "        max_acc, dict_acc, embed_and_index_dict = evaluate(\n",
    "            self.reranker, \n",
    "            self.trainer.datamodule.entity_dict_vecs, \n",
    "            self.trainer.datamodule.valid_men_vecs, \n",
    "            # device=self.device, \n",
    "            logger=logger, \n",
    "            # knn=self.hparams[\"knn\"], \n",
    "            # n_gpu=self.n_gpu,\n",
    "            entity_data=self.trainer.datamodule.entity_dictionary, \n",
    "            query_data=self.trainer.datamodule.valid_processed_data, \n",
    "            # silent=self.hparams[\"silent\"],\n",
    "            use_types=self.hparams['use_types'], #use_types=self.hparams['use_types'] or self.hparams[\"use_types_for_eval\"] \n",
    "            embed_batch_size=self.hparams[\"embed_batch_size\"],\n",
    "            force_exact_search=self.hparams['use_types'] or self.hparams[\"force_exact_search\"], #force_exact_search= self.hparams['use_types'] or use_types or self.hparams[\"use_types_for_eval\"] or self.hparams[\"force_exact_search\"]\n",
    "            probe_mult_factor=self.hparams['probe_mult_factor'], \n",
    "            within_doc=self.hparams['within_doc'],\n",
    "            context_doc_ids=self.trainer.datamodule.valid_context_doc_ids\n",
    "        )\n",
    "        self.log(\"max_acc\", max_acc, on_epoch=True)\n",
    "        for key, value in dict_acc.items():\n",
    "            self.log(f\"dict_acc_{key}\", value, on_epoch=True)\n",
    "        for key, value in dict_acc.items():\n",
    "            self.log(f\"embed_and_index_dict{key}\", value, on_epoch=True)\n",
    "        \n",
    "        \n",
    "    def test_step(self, batch, batch_idx): #DD14\n",
    "        max_acc, dict_acc, embed_and_index_dict = evaluate(\n",
    "            self.reranker, \n",
    "            self.trainer.datamodule.entity_dict_vecs, \n",
    "            self.trainer.datamodule.test_men_vecs, \n",
    "            # device=self.device, \n",
    "            logger=logger, \n",
    "            # knn=self.hparams[\"knn\"], \n",
    "            # n_gpu=self.n_gpu,\n",
    "            entity_data=self.trainer.datamodule.entity_dictionary, \n",
    "            query_data=self.trainer.datamodule.test_processed_data,\n",
    "            # silent=self.hparams[\"silent\"],\n",
    "            use_types=self.hparams['use_types'], #use_types=self.hparams['use_types'] or self.hparams[\"use_types_for_eval\"] \n",
    "            embed_batch_size=self.hparams[\"embed_batch_size\"],\n",
    "            force_exact_search=self.hparams['use_types'] or self.hparams[\"force_exact_search\"], #force_exact_search= self.hparams['use_types'] or use_types or self.hparams[\"use_types_for_eval\"] or self.hparams[\"force_exact_search\"]\n",
    "            probe_mult_factor=self.hparams['probe_mult_factor'], \n",
    "            within_doc=self.hparams['within_doc'],\n",
    "            context_doc_ids=self.trainer.datamodule.test_context_doc_ids\n",
    "        )\n",
    "        self.log(\"max_acc\", max_acc, on_epoch=True)\n",
    "        self.log(\"dict_acc\", dict_acc, on_epoch=True)\n",
    "        self.log(\"embedding_and_indexing\", embed_and_index_dict, on_epoch=True)\n",
    "        \n",
    "\n",
    "    def on_train_end(self):\n",
    "        execution_time = (time.time() - self.start_time) / 60\n",
    "        self.logger.info(f\"The training took {execution_time} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.trainer import Trainer\n",
    "from Lightning_datamodule import ArboelDataModule\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home2/cye73/data/arboel/ncbi_disease\n"
     ]
    }
   ],
   "source": [
    "ontology = \"medic\"\n",
    "model = \"arboel\"\n",
    "dataset = \"ncbi_disease\"\n",
    "abs_path = \"/home2/cye73/data\"\n",
    "data_path = os.path.join(abs_path, model, dataset)\n",
    "print(data_path)\n",
    "abs_path2 = \"/home2/cye73/results\"\n",
    "model_output_path = os.path.join(abs_path2, model, dataset)\n",
    "\n",
    "ontology_type = \"umls\"\n",
    "umls_dir=\"/mitchell/entity-linking/2017AA/META/\"\n",
    "\n",
    "params_test = {\"model_output_path\" : model_output_path,\n",
    "               \"data_path\" : data_path,  \n",
    "               \"knn\" : 4,\n",
    "               \"use_types\" : False,\n",
    "               \"max_context_length\": 64 ,\n",
    "               \"max_cand_length\" : 64 ,\n",
    "               \"context_key\" : \"context\", # to specify context_left or context_right\n",
    "               \"debug\" : True,\n",
    "               \"gold_arbo_knn\": 4,\n",
    "               \"within_doc\" : True,\n",
    "               \"within_doc_skip_strategy\" : False,\n",
    "               \"batch_size\" : 128,#batch_size = embed_batch_size\n",
    "               \"train_batch_size\" : 128,\n",
    "               \"filter_unlabeled\" : False,\n",
    "               \"type_optimization\" : \"all\",\n",
    "               # 'additional_layers', 'top_layer', 'top4_layers', 'all_encoder_layers', 'all'\n",
    "               \"learning_rate\" : 3e-5,\n",
    "               \"warmup_proportion\" : 464,\n",
    "               \"fp16\" : False,\n",
    "               \"embed_batch_size\" : 128,\n",
    "               \"force_exact_search\" : True,\n",
    "               \"probe_mult_factor\" : 1,\n",
    "               \"pos_neg_loss\" : True,\n",
    "               \"use_types_for_eval\" : True,\n",
    "               \"drop_entities\" : False,\n",
    "               \"drop_set\" : False,\n",
    "               \"farthest_neighbor\" : True,\n",
    "               \"rand_gold_arbo\" : True,\n",
    "               \"bert_model\": 'michiyasunaga/BioLinkBERT-base',\n",
    "               \"out_dim\": 768 ,\n",
    "               \"pull_from_layer\":11, #11 for base and 23 for large\n",
    "               \"add_linear\":True,\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = ArboelDataModule(params = params_test,\n",
    "                                ontology = ontology,\n",
    "                                dataset = dataset,\n",
    "                                ontology_type = ontology_type,\n",
    "                                umls_dir = umls_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitArboel(params = params_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint(\n",
    "    monitor='max_acc',  # Metric to monitor\n",
    "    dirpath=params_test[\"model_output_path\"],  # Directory to save the model\n",
    "    filename='{epoch}-{max_acc:.2f}',  # Saves the model with epoch and val_loss in the filename\n",
    "    save_top_k= 1,  # Number of best models to save; -1 means save all of them\n",
    "    mode='max',  # 'max' means the highest max_acc will be considered as the best model\n",
    "    verbose=True,  # Logs a message whenever a model checkpoint is saved\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/Mar/2024 16:13:27] INFO - GPU available: True (cuda), used: True\n",
      "[14/Mar/2024 16:13:27] INFO - TPU available: False, using: 0 TPU cores\n",
      "[14/Mar/2024 16:13:27] INFO - IPU available: False, using: 0 IPUs\n",
      "[14/Mar/2024 16:13:27] INFO - HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    devices=[0, 1, 2, 3],\n",
    "    accelerator=\"gpu\",\n",
    "    enable_progress_bar=True,\n",
    "    callbacks=[model_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/Mar/2024 16:13:29] INFO - Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/Mar/2024 16:13:29] INFO - Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/Mar/2024 16:13:29] INFO - Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/Mar/2024 16:13:29] INFO - Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "[14/Mar/2024 16:13:29] INFO - ----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 4 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:1454: FutureWarning: The repository for bigbio/ncbi_disease contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/bigbio/ncbi_disease\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stored processed entity dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13189/13189 [00:00<00:00, 3126585.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max labels on one doc: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating correct mention format for train dataset: 100%|██████████| 5065/5065 [00:00<00:00, 241391.59it/s]\n",
      "Creating correct mention format for validation dataset: 100%|██████████| 780/780 [00:00<00:00, 283521.72it/s]\n",
      "Creating correct mention format for test dataset: 100%|██████████| 960/960 [00:00<00:00, 297688.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stored processed entity dictionary...\n",
      "Loading stored processed entity dictionary...\n",
      "Loading stored processed entity dictionary...\n",
      "Loading stored processed entity dictionary...\n",
      "Loading stored processed train data...\n",
      "Loading stored processed train data...\n",
      "Loading stored processed valid data...Loading stored processed valid data...\n",
      "\n",
      "Loading stored processed train data...\n",
      "Loading stored processed valid data...\n",
      "[14/Mar/2024 16:13:32] INFO - within_doc\n",
      "[14/Mar/2024 16:13:32] INFO - within_doc\n",
      "[14/Mar/2024 16:13:32] INFO - within_doc\n",
      "[14/Mar/2024 16:13:32] INFO - Read 4782 train samples..\n",
      "[14/Mar/2024 16:13:32] INFO - Read 722 valid samples..\n",
      "[14/Mar/2024 16:13:32] INFO - Read 4782 train samples..\n",
      "[14/Mar/2024 16:13:32] INFO - Read 877 test samples..\n",
      "[14/Mar/2024 16:13:32] INFO - Read 4782 train samples..\n",
      "[14/Mar/2024 16:13:32] INFO - Read 722 valid samples..\n",
      "[14/Mar/2024 16:13:32] INFO - Read 722 valid samples..\n",
      "[14/Mar/2024 16:13:32] INFO - Read 877 test samples..\n",
      "[14/Mar/2024 16:13:32] INFO - Read 877 test samples..\n",
      "Loading stored processed train data...\n",
      "Loading stored processed valid data...\n",
      "[14/Mar/2024 16:13:33] INFO - within_doc\n",
      "[14/Mar/2024 16:13:33] INFO - Read 4782 train samples..\n",
      "[14/Mar/2024 16:13:33] INFO - Read 722 valid samples..\n",
      "[14/Mar/2024 16:13:33] INFO - Read 877 test samples..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/Mar/2024 16:13:33] INFO - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "[14/Mar/2024 16:13:33] INFO - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "[14/Mar/2024 16:13:33] INFO - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "[14/Mar/2024 16:13:33] INFO - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "The following parameters will be optimized WITH decay:The following parameters will be optimized WITH decay:The following parameters will be optimized WITH decay:\n",
      "\n",
      "\n",
      "The following parameters will be optimized WITH decay:context_encoder.bert_model.embeddings.word_embeddings.weight , context_encoder.bert_model.embeddings.position_embeddings.weight , context_encoder.bert_model.embeddings.token_type_embeddings.weight , context_encoder.bert_model.embeddings.LayerNorm.weight , context_encoder.bert_model.encoder.layer.0.attention.self.query.weight , ...and 197 morecontext_encoder.bert_model.embeddings.word_embeddings.weight , context_encoder.bert_model.embeddings.position_embeddings.weight , context_encoder.bert_model.embeddings.token_type_embeddings.weight , context_encoder.bert_model.embeddings.LayerNorm.weight , context_encoder.bert_model.encoder.layer.0.attention.self.query.weight , ...and 197 morecontext_encoder.bert_model.embeddings.word_embeddings.weight , context_encoder.bert_model.embeddings.position_embeddings.weight , context_encoder.bert_model.embeddings.token_type_embeddings.weight , context_encoder.bert_model.embeddings.LayerNorm.weight , context_encoder.bert_model.encoder.layer.0.attention.self.query.weight , ...and 197 more\n",
      "\n",
      "\n",
      "\n",
      "The following parameters will be optimized WITHOUT decay:The following parameters will be optimized WITHOUT decay:\n",
      "\n",
      "context_encoder.bert_model.embeddings.word_embeddings.weight , context_encoder.bert_model.embeddings.position_embeddings.weight , context_encoder.bert_model.embeddings.token_type_embeddings.weight , context_encoder.bert_model.embeddings.LayerNorm.weight , context_encoder.bert_model.encoder.layer.0.attention.self.query.weight , ...and 197 morecontext_encoder.bert_model.embeddings.LayerNorm.bias , context_encoder.bert_model.encoder.layer.0.attention.self.query.bias , context_encoder.bert_model.encoder.layer.0.attention.self.key.bias , context_encoder.bert_model.encoder.layer.0.attention.self.value.bias , context_encoder.bert_model.encoder.layer.0.attention.output.dense.bias , ...and 191 moreThe following parameters will be optimized WITHOUT decay:context_encoder.bert_model.embeddings.LayerNorm.bias , context_encoder.bert_model.encoder.layer.0.attention.self.query.bias , context_encoder.bert_model.encoder.layer.0.attention.self.key.bias , context_encoder.bert_model.encoder.layer.0.attention.self.value.bias , context_encoder.bert_model.encoder.layer.0.attention.output.dense.bias , ...and 191 more\n",
      "\n",
      "\n",
      "The following parameters will be optimized WITHOUT decay:\n",
      "\n",
      "context_encoder.bert_model.embeddings.LayerNorm.bias , context_encoder.bert_model.encoder.layer.0.attention.self.query.bias , context_encoder.bert_model.encoder.layer.0.attention.self.key.bias , context_encoder.bert_model.encoder.layer.0.attention.self.value.bias , context_encoder.bert_model.encoder.layer.0.attention.output.dense.bias , ...and 191 morecontext_encoder.bert_model.embeddings.LayerNorm.bias , context_encoder.bert_model.encoder.layer.0.attention.self.query.bias , context_encoder.bert_model.encoder.layer.0.attention.self.key.bias , context_encoder.bert_model.encoder.layer.0.attention.self.value.bias , context_encoder.bert_model.encoder.layer.0.attention.output.dense.bias , ...and 191 more\n",
      "\n",
      "[14/Mar/2024 16:13:33] INFO -  Num optimization steps = 10\n",
      "[14/Mar/2024 16:13:33] INFO -  Num optimization steps = 10\n",
      "[14/Mar/2024 16:13:33] INFO -  Num optimization steps = 10\n",
      "[14/Mar/2024 16:13:33] INFO -  Num warmup steps = 4640\n",
      "[14/Mar/2024 16:13:33] INFO -  Num optimization steps = 10\n",
      "[14/Mar/2024 16:13:33] INFO -  Num warmup steps = 4640\n",
      "[14/Mar/2024 16:13:33] INFO -  Num warmup steps = 4640\n",
      "[14/Mar/2024 16:13:33] INFO -  Num warmup steps = 4640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: \n",
      "  | Name     | Type            | Params\n",
      "---------------------------------------------\n",
      "0 | reranker | BiEncoderRanker | 217 M \n",
      "1 | model    | BiEncoderModule | 217 M \n",
      "---------------------------------------------\n",
      "217 M     Trainable params\n",
      "0         Non-trainable params\n",
      "217 M     Total params\n",
      "870.586   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/Mar/2024 16:13:33] INFO - \n",
      "  | Name     | Type            | Params\n",
      "---------------------------------------------\n",
      "0 | reranker | BiEncoderRanker | 217 M \n",
      "1 | model    | BiEncoderModule | 217 M \n",
      "---------------------------------------------\n",
      "217 M     Trainable params\n",
      "0         Non-trainable params\n",
      "217 M     Total params\n",
      "870.586   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d750e1dce34c2a9b17f980c8f90f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/Mar/2024 16:13:33] INFO - Eval: Dictionary: Embedding and building index\n",
      "[14/Mar/2024 16:13:33] INFO - Eval: Dictionary: Embedding and building index\n",
      "[14/Mar/2024 16:13:33] INFO - Eval: Dictionary: Embedding and building index\n",
      "[14/Mar/2024 16:13:33] INFO - Eval: Dictionary: Embedding and building index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding in batches: 100%|██████████| 104/104 [00:06<00:00, 16.32it/s]\n",
      "Embedding in batches: 100%|██████████| 104/104 [00:06<00:00, 15.39it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/Mar/2024 16:13:40] INFO - Eval: Queries: Embedding and building index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding in batches:   0%|          | 0/2 [00:00<?, ?it/s], 14.98it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/Mar/2024 16:13:40] INFO - Eval: Queries: Embedding and building index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding in batches: 100%|██████████| 2/2 [00:00<00:00, 25.42it/s]/s]\n",
      "Embedding in batches: 100%|██████████| 2/2 [00:00<00:00, 25.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/Mar/2024 16:13:40] INFO - Eval: Starting KNN search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/Mar/2024 16:13:40] INFO - Eval: Starting KNN search...\n",
      "[14/Mar/2024 16:13:40] INFO - Eval: Search finished\n",
      "[14/Mar/2024 16:13:40] INFO - Eval: Building graphs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: Building graphs:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/Mar/2024 16:13:40] INFO - Eval: Search finished\n",
      "[14/Mar/2024 16:13:40] INFO - Eval: Building graphs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: Building graphs: 100%|██████████| 200/200 [00:00<00:00, 4280.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/Mar/2024 16:13:40] INFO - \n",
      "Eval: Graph (k=0):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: Building graphs: 100%|██████████| 200/200 [00:00<00:00, 4215.30it/s]\n",
      "Paritioning joint graph:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/Mar/2024 16:13:40] INFO - \n",
      "Eval: Graph (k=0):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paritioning joint graph: 100%|██████████| 200/200 [00:00<00:00, 149502.91it/s]\n",
      "Paritioning joint graph: 100%|██████████| 200/200 [00:00<00:00, 102076.03it/s]\n",
      "Embedding in batches: 100%|██████████| 104/104 [00:06<00:00, 14.93it/s]\n",
      "Embedding in batches:  43%|████▎     | 45/104 [00:06<00:08,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing clusters...\n",
      "Accuracy = 0.0 %\n",
      "[14/Mar/2024 16:13:40] INFO - Eval: accuracy for graph@k=0: 0.0%\n",
      "[14/Mar/2024 16:13:40] INFO - \n",
      "Eval: Graph (k=1):\n",
      "[14/Mar/2024 16:13:40] INFO - Eval: Queries: Embedding and building index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paritioning joint graph: 100%|██████████| 400/400 [00:00<00:00, 71860.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing clusters...\n",
      "Accuracy = 0.0 %\n",
      "[14/Mar/2024 16:13:40] INFO - Eval: accuracy for graph@k=0: 0.0%\n",
      "[14/Mar/2024 16:13:40] INFO - \n",
      "Eval: Graph (k=1):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paritioning joint graph: 100%|██████████| 400/400 [00:00<00:00, 90492.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing clusters...\n",
      "Accuracy = 0.0 %\n",
      "[14/Mar/2024 16:13:40] INFO - Eval: accuracy for graph@k=1: 0.0%\n",
      "[14/Mar/2024 16:13:40] INFO - \n",
      "Eval: Graph (k=2):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paritioning joint graph: 100%|██████████| 600/600 [00:00<00:00, 97030.48it/s]\n",
      "Embedding in batches: 100%|██████████| 2/2 [00:00<00:00, 25.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/Mar/2024 16:13:40] INFO - Eval: Starting KNN search...\n",
      "Analyzing clusters...\n",
      "Accuracy = 0.0 %\n",
      "[14/Mar/2024 16:13:40] INFO - Eval: accuracy for graph@k=1: 0.0%\n",
      "[14/Mar/2024 16:13:40] INFO - \n",
      "Eval: Graph (k=2):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paritioning joint graph:   0%|          | 0/600 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/Mar/2024 16:13:41] INFO - Eval: Search finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paritioning joint graph: 100%|██████████| 600/600 [00:00<00:00, 122199.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/Mar/2024 16:13:41] INFO - Eval: Building graphs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding in batches:  44%|████▍     | 46/104 [00:07<00:08,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing clusters...\n",
      "Accuracy = 0.0 %\n",
      "[14/Mar/2024 16:13:41] INFO - Eval: accuracy for graph@k=2: 0.0%\n",
      "[14/Mar/2024 16:13:41] INFO - \n",
      "Eval: Graph (k=4):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paritioning joint graph:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing clusters...\n",
      "Accuracy = 0.0 %\n",
      "[14/Mar/2024 16:13:41] INFO - Eval: accuracy for graph@k=2: 0.0%\n",
      "[14/Mar/2024 16:13:41] INFO - \n",
      "Eval: Graph (k=4):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: Building graphs: 100%|██████████| 200/200 [00:00<00:00, 4235.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/Mar/2024 16:13:41] INFO - \n",
      "Eval: Graph (k=0):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Paritioning joint graph: 100%|██████████| 200/200 [00:00<00:00, 61881.14it/s]s]\n",
      "Paritioning joint graph: 100%|██████████| 1000/1000 [00:00<00:00, 44832.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing clusters...\n",
      "Accuracy = 0.0 %\n",
      "[14/Mar/2024 16:13:41] INFO - Eval: accuracy for graph@k=4: 0.0%\n",
      "[14/Mar/2024 16:13:41] INFO - \n",
      "Eval: Graph (k=8):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paritioning joint graph:   0%|          | 0/1800 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing clusters...\n",
      "Accuracy = 0.0 %\n",
      "[14/Mar/2024 16:13:41] INFO - Eval: accuracy for graph@k=0: 0.0%\n",
      "[14/Mar/2024 16:13:41] INFO - \n",
      "Eval: Graph (k=1):\n",
      "Analyzing clusters..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paritioning joint graph:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.0 %\n",
      "[14/Mar/2024 16:13:41] INFO - Eval: accuracy for graph@k=4: 0.0%\n",
      "[14/Mar/2024 16:13:41] INFO - \n",
      "Eval: Graph (k=8):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paritioning joint graph: 100%|██████████| 400/400 [00:00<00:00, 134099.72it/s]\n",
      "Paritioning joint graph: 100%|██████████| 1800/1800 [00:00<00:00, 51849.81it/s]\n",
      "Paritioning joint graph: 100%|██████████| 1800/1800 [00:00<00:00, 51430.90it/s]\n",
      "Embedding in batches:  45%|████▌     | 47/104 [00:07<00:08,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing clusters...\n",
      "Accuracy = 0.0 %\n",
      "[14/Mar/2024 16:13:41] INFO - Eval: accuracy for graph@k=1: 0.0%\n",
      "[14/Mar/2024 16:13:41] INFO - \n",
      "Eval: Graph (k=2):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing clusters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paritioning joint graph: 100%|██████████| 600/600 [00:00<00:00, 82730.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.0 %"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[14/Mar/2024 16:13:41] INFO - Eval: accuracy for graph@k=8: 0.0%\n",
      "[14/Mar/2024 16:13:41] INFO - Eval: Best accuracy: 0.0%\n",
      "Analyzing clusters...\n",
      "Accuracy = 0.0 %\n",
      "[14/Mar/2024 16:13:41] INFO - Eval: accuracy for graph@k=8: 0.0%\n",
      "[14/Mar/2024 16:13:41] INFO - Eval: Best accuracy: 0.0%\n",
      "Analyzing clusters...\n",
      "Accuracy = 0.0 %\n",
      "[14/Mar/2024 16:13:41] INFO - Eval: accuracy for graph@k=2: 0.0%\n",
      "[14/Mar/2024 16:13:41] INFO - \n",
      "Eval: Graph (k=4):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paritioning joint graph: 100%|██████████| 1000/1000 [00:00<00:00, 68473.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing clusters...\n",
      "Accuracy = 0.0 %\n",
      "[14/Mar/2024 16:13:41] INFO - Eval: accuracy for graph@k=4: 0.0%\n",
      "[14/Mar/2024 16:13:41] INFO - \n",
      "Eval: Graph (k=8):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paritioning joint graph: 100%|██████████| 1800/1800 [00:00<00:00, 51529.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing clusters...\n",
      "Accuracy = 0.0 %\n",
      "[14/Mar/2024 16:13:41] INFO - Eval: accuracy for graph@k=8: 0.0%\n",
      "[14/Mar/2024 16:13:41] INFO - Eval: Best accuracy: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('max_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('dict_acc_k0', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('dict_acc_k1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('dict_acc_k2', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('dict_acc_k4', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('dict_acc_k8', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('embed_and_index_dictk0', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('embed_and_index_dictk1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('embed_and_index_dictk2', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('embed_and_index_dictk4', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('embed_and_index_dictk8', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "Embedding in batches: 100%|██████████| 104/104 [00:15<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/Mar/2024 16:13:49] INFO - Eval: Queries: Embedding and building index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding in batches: 100%|██████████| 2/2 [00:00<00:00,  9.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/Mar/2024 16:13:49] INFO - Eval: Starting KNN search...\n",
      "[14/Mar/2024 16:13:49] INFO - Eval: Search finished\n",
      "[14/Mar/2024 16:13:49] INFO - Eval: Building graphs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: Building graphs: 100%|██████████| 200/200 [00:00<00:00, 4191.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/Mar/2024 16:13:49] INFO - \n",
      "Eval: Graph (k=0):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paritioning joint graph: 100%|██████████| 200/200 [00:00<00:00, 63806.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing clusters...\n",
      "Accuracy = 0.0 %\n",
      "[14/Mar/2024 16:13:49] INFO - Eval: accuracy for graph@k=0: 0.0%\n",
      "[14/Mar/2024 16:13:49] INFO - \n",
      "Eval: Graph (k=1):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paritioning joint graph: 100%|██████████| 400/400 [00:00<00:00, 98371.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing clusters...\n",
      "Accuracy = 0.0 %\n",
      "[14/Mar/2024 16:13:49] INFO - Eval: accuracy for graph@k=1: 0.0%\n",
      "[14/Mar/2024 16:13:49] INFO - \n",
      "Eval: Graph (k=2):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paritioning joint graph: 100%|██████████| 600/600 [00:00<00:00, 118852.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing clusters...\n",
      "Accuracy = 0.0 %\n",
      "[14/Mar/2024 16:13:49] INFO - Eval: accuracy for graph@k=2: 0.0%\n",
      "[14/Mar/2024 16:13:49] INFO - \n",
      "Eval: Graph (k=4):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paritioning joint graph: 100%|██████████| 1000/1000 [00:00<00:00, 40753.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing clusters...\n",
      "Accuracy = 0.0 %\n",
      "[14/Mar/2024 16:13:49] INFO - Eval: accuracy for graph@k=4: 0.0%\n",
      "[14/Mar/2024 16:13:49] INFO - \n",
      "Eval: Graph (k=8):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paritioning joint graph: 100%|██████████| 1800/1800 [00:00<00:00, 53812.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing clusters...\n",
      "Accuracy = 0.0 %\n",
      "[14/Mar/2024 16:13:50] INFO - Eval: accuracy for graph@k=8: 0.0%\n",
      "[14/Mar/2024 16:13:50] INFO - Eval: Best accuracy: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "Embedding in batches:   0%|          | 0/104 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba2f82c984a4f81b9ad0fe5854f29e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding in batches: 100%|██████████| 104/104 [00:05<00:00, 18.42it/s]\n",
      "Embedding in batches: 100%|██████████| 104/104 [00:05<00:00, 18.28it/s]\n",
      "Embedding in batches: 100%|██████████| 2/2 [00:00<00:00, 25.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/Mar/2024 16:13:55] INFO - Starting KNN search...\n",
      "[14/Mar/2024 16:13:55] INFO - Search finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding in batches: 100%|██████████| 2/2 [00:00<00:00, 25.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/Mar/2024 16:13:55] INFO - Starting KNN search...\n",
      "[14/Mar/2024 16:13:55] INFO - Search finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding in batches: 100%|██████████| 104/104 [00:05<00:00, 17.87it/s]\n",
      "Embedding in batches: 100%|██████████| 2/2 [00:00<00:00, 25.33it/s]/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/Mar/2024 16:13:56] INFO - Starting KNN search...\n",
      "[14/Mar/2024 16:13:56] INFO - Search finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding in batches:  43%|████▎     | 45/104 [00:06<00:08,  7.26it/s]INFO: [rank: 3] Received SIGTERM: 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/Mar/2024 16:13:56] INFO - [rank: 3] Received SIGTERM: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding in batches: 100%|██████████| 104/104 [00:14<00:00,  7.24it/s]\n",
      "Embedding in batches: 100%|██████████| 2/2 [00:00<00:00,  9.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/Mar/2024 16:14:04] INFO - Starting KNN search...\n",
      "[14/Mar/2024 16:14:04] INFO - Search finished\n"
     ]
    },
    {
     "ename": "ProcessRaisedException",
     "evalue": "\n\n-- Process 1 terminated with the following error:\nTraceback (most recent call last):\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/fabric/utilities/data.py\", line 487, in __getattr__\n    return self[key]\nKeyError: 'train_gold_clusters'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 68, in _wrap\n    fn(i, *args)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/multiprocessing.py\", line 173, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 579, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 986, in _run\n    results = self._run_stage()\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 1032, in _run_stage\n    self.fit_loop.run()\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py\", line 205, in run\n    self.advance()\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py\", line 363, in advance\n    self.epoch_loop.run(self._data_fetcher)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 138, in run\n    self.advance(data_fetcher)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 242, in advance\n    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 191, in run\n    self._optimizer_step(batch_idx, closure)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 269, in _optimizer_step\n    call._call_lightning_module_hook(\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py\", line 157, in _call_lightning_module_hook\n    output = fn(*args, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/core/module.py\", line 1303, in optimizer_step\n    optimizer.step(closure=optimizer_closure)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/core/optimizer.py\", line 152, in step\n    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/strategies/ddp.py\", line 270, in optimizer_step\n    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py\", line 239, in optimizer_step\n    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/plugins/precision/precision.py\", line 122, in optimizer_step\n    return optimizer.step(closure=closure, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/torch/optim/lr_scheduler.py\", line 75, in wrapper\n    return wrapped(*args, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/torch/optim/optimizer.py\", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/pytorch_transformers/optimization.py\", line 139, in step\n    loss = closure()\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/plugins/precision/precision.py\", line 108, in _wrap_closure\n    closure_result = closure()\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 144, in __call__\n    self._result = self.closure(*args, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 129, in closure\n    step_output = self._step_fn()\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 319, in _training_step\n    training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py\", line 309, in _call_strategy_hook\n    output = fn(*args, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py\", line 390, in training_step\n    return self._forward_redirection(self.model, self.lightning_module, \"training_step\", *args, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py\", line 642, in __call__\n    wrapper_output = wrapper_module(*args, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1523, in forward\n    else self._run_ddp_forward(*inputs, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1359, in _run_ddp_forward\n    return self.module(*inputs, **kwargs)  # type: ignore[index]\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py\", line 635, in wrapped_forward\n    out = method(*_args, **_kwargs)\n  File \"/tmp/ipykernel_490235/3504015694.py\", line 272, in training_step\n    f = self.forward(batch_context_inputs, candidate_idxs, n_gold, mention_idxs)\n  File \"/tmp/ipykernel_490235/3504015694.py\", line 100, in forward\n    cluster_mens = self.hparams.train_gold_clusters[cluster_ent]\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/fabric/utilities/data.py\", line 489, in __getattr__\n    raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{key}'\") from e\nAttributeError: 'AttributeDict' object has no attribute 'train_gold_clusters'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_module\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:543\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlauncher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/multiprocessing.py:144\u001b[0m, in \u001b[0;36m_MultiProcessingLauncher.launch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m process_context \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39mstart_processes(\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapping_function,\n\u001b[1;32m    138\u001b[0m     args\u001b[38;5;241m=\u001b[39mprocess_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     join\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# we will join ourselves to get the process references\u001b[39;00m\n\u001b[1;32m    142\u001b[0m )\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocs \u001b[38;5;241m=\u001b[39m process_context\u001b[38;5;241m.\u001b[39mprocesses\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mprocess_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    147\u001b[0m worker_output \u001b[38;5;241m=\u001b[39m return_queue\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:158\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    156\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-- Process \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with the following error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m error_index\n\u001b[1;32m    157\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m original_trace\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ProcessRaisedException(msg, error_index, failed_process\u001b[38;5;241m.\u001b[39mpid)\n",
      "\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 1 terminated with the following error:\nTraceback (most recent call last):\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/fabric/utilities/data.py\", line 487, in __getattr__\n    return self[key]\nKeyError: 'train_gold_clusters'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 68, in _wrap\n    fn(i, *args)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/multiprocessing.py\", line 173, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 579, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 986, in _run\n    results = self._run_stage()\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 1032, in _run_stage\n    self.fit_loop.run()\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py\", line 205, in run\n    self.advance()\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py\", line 363, in advance\n    self.epoch_loop.run(self._data_fetcher)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 138, in run\n    self.advance(data_fetcher)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 242, in advance\n    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 191, in run\n    self._optimizer_step(batch_idx, closure)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 269, in _optimizer_step\n    call._call_lightning_module_hook(\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py\", line 157, in _call_lightning_module_hook\n    output = fn(*args, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/core/module.py\", line 1303, in optimizer_step\n    optimizer.step(closure=optimizer_closure)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/core/optimizer.py\", line 152, in step\n    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/strategies/ddp.py\", line 270, in optimizer_step\n    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py\", line 239, in optimizer_step\n    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/plugins/precision/precision.py\", line 122, in optimizer_step\n    return optimizer.step(closure=closure, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/torch/optim/lr_scheduler.py\", line 75, in wrapper\n    return wrapped(*args, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/torch/optim/optimizer.py\", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/pytorch_transformers/optimization.py\", line 139, in step\n    loss = closure()\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/plugins/precision/precision.py\", line 108, in _wrap_closure\n    closure_result = closure()\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 144, in __call__\n    self._result = self.closure(*args, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 129, in closure\n    step_output = self._step_fn()\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 319, in _training_step\n    training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py\", line 309, in _call_strategy_hook\n    output = fn(*args, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py\", line 390, in training_step\n    return self._forward_redirection(self.model, self.lightning_module, \"training_step\", *args, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py\", line 642, in __call__\n    wrapper_output = wrapper_module(*args, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1523, in forward\n    else self._run_ddp_forward(*inputs, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1359, in _run_ddp_forward\n    return self.module(*inputs, **kwargs)  # type: ignore[index]\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py\", line 635, in wrapped_forward\n    out = method(*_args, **_kwargs)\n  File \"/tmp/ipykernel_490235/3504015694.py\", line 272, in training_step\n    f = self.forward(batch_context_inputs, candidate_idxs, n_gold, mention_idxs)\n  File \"/tmp/ipykernel_490235/3504015694.py\", line 100, in forward\n    cluster_mens = self.hparams.train_gold_clusters[cluster_ent]\n  File \"/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/lightning/fabric/utilities/data.py\", line 489, in __getattr__\n    raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{key}'\") from e\nAttributeError: 'AttributeDict' object has no attribute 'train_gold_clusters'\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=data_module )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arboel_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
