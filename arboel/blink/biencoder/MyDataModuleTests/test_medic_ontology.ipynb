{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create MEDIC entity dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for an_em contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/an_em/an_em.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for anat_em contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/anat_em/anat_em.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for ask_a_patient contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/ask_a_patient/ask_a_patient.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bc5cdr contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bc5cdr/bc5cdr.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bc7_litcovid contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bc7_litcovid/bc7_litcovid.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bio_sim_verb contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bio_sim_verb/bio_sim_verb.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bio_simlex contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bio_simlex/bio_simlex.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bioasq_2021_mesinesp contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bioasq_2021_mesinesp/bioasq_2021_mesinesp.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bioasq_task_b contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bioasq_task_b/bioasq_task_b.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bioasq_task_c_2017 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bioasq_task_c_2017/bioasq_task_c_2017.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bioid contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bioid/bioid.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bioinfer contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bioinfer/bioinfer.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for biology_how_why_corpus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/biology_how_why_corpus/biology_how_why_corpus.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for biomrc contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/biomrc/biomrc.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bionlp_shared_task_2009 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bionlp_shared_task_2009/bionlp_shared_task_2009.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bionlp_st_2011_epi contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bionlp_st_2011_epi/bionlp_st_2011_epi.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bionlp_st_2011_ge contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bionlp_st_2011_ge/bionlp_st_2011_ge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bionlp_st_2011_id contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bionlp_st_2011_id/bionlp_st_2011_id.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bionlp_st_2011_rel contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bionlp_st_2011_rel/bionlp_st_2011_rel.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bionlp_st_2013_cg contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bionlp_st_2013_cg/bionlp_st_2013_cg.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bionlp_st_2013_ge contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bionlp_st_2013_ge/bionlp_st_2013_ge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bionlp_st_2013_gro contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bionlp_st_2013_gro/bionlp_st_2013_gro.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bionlp_st_2013_pc contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bionlp_st_2013_pc/bionlp_st_2013_pc.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bionlp_st_2019_bb contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bionlp_st_2019_bb/bionlp_st_2019_bb.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for biored contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/biored/biored.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for biorelex contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/biorelex/biorelex.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bioscope contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bioscope/bioscope.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for biosses contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/biosses/biosses.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for blurb contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/blurb/blurb.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for bronco contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/bronco/bronco.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for cantemist contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/cantemist/cantemist.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for cardiode contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/cardiode/cardiode.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for cas contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/cas/cas.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for cellfinder contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/cellfinder/cellfinder.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for chebi_nactem contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/chebi_nactem/chebi_nactem.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for chemdner contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/chemdner/chemdner.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for chemprot contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/chemprot/chemprot.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for chia contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/chia/chia.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for citation_gia_test_collection contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/citation_gia_test_collection/citation_gia_test_collection.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for codiesp contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/codiesp/codiesp.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for cpi contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/cpi/cpi.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for ctebmsp contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/ctebmsp/ctebmsp.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for czi_drsm contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/czi_drsm/czi_drsm.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for ddi_corpus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/ddi_corpus/ddi_corpus.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for distemist contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/distemist/distemist.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for drugprot contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/drugprot/drugprot.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for ebm_pico contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/ebm_pico/ebm_pico.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for ehr_rel contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/ehr_rel/ehr_rel.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for essai contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/essai/essai.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for euadr contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/euadr/euadr.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for evidence_inference contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/evidence_inference/evidence_inference.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for gad contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/gad/gad.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for genetag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/genetag/genetag.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for genia_ptm_event_corpus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/genia_ptm_event_corpus/genia_ptm_event_corpus.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for genia_relation_corpus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/genia_relation_corpus/genia_relation_corpus.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for genia_term_corpus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/genia_term_corpus/genia_term_corpus.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for geokhoj_v1 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/geokhoj_v1/geokhoj_v1.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for ggponc2 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/ggponc2/ggponc2.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for gnormplus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/gnormplus/gnormplus.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for hallmarks_of_cancer contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/hallmarks_of_cancer/hallmarks_of_cancer.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for hprd50 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/hprd50/hprd50.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for iepa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/iepa/iepa.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for jnlpba contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/jnlpba/jnlpba.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for linnaeus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/linnaeus/linnaeus.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for lll contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/lll/lll.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for mayosrs contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/mayosrs/mayosrs.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for med_qa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/med_qa/med_qa.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for medal contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/medal/medal.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for meddialog contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/meddialog/meddialog.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for meddocan contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/meddocan/meddocan.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for medhop contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/medhop/medhop.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for medical_data contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/medical_data/medical_data.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for mediqa_nli contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/mediqa_nli/mediqa_nli.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for mediqa_qa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/mediqa_qa/mediqa_qa.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for mediqa_rqe contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/mediqa_rqe/mediqa_rqe.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for medmentions contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/medmentions/medmentions.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for mednli contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/mednli/mednli.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for meqsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/meqsum/meqsum.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for minimayosrs contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/minimayosrs/minimayosrs.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for mirna contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/mirna/mirna.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for mlee contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/mlee/mlee.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for mqp contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/mqp/mqp.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for msh_wsd contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/msh_wsd/msh_wsd.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for muchmore contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/muchmore/muchmore.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for multi_xscience contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/multi_xscience/multi_xscience.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for n2c2_2006_deid contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/n2c2_2006_deid/n2c2_2006_deid.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for n2c2_2006_smokers contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/n2c2_2006_smokers/n2c2_2006_smokers.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for n2c2_2008 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/n2c2_2008/n2c2_2008.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for n2c2_2009 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/n2c2_2009/n2c2_2009.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for n2c2_2010 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/n2c2_2010/n2c2_2010.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for n2c2_2011 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/n2c2_2011/n2c2_2011.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for n2c2_2014_deid contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/n2c2_2014_deid/n2c2_2014_deid.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for n2c2_2018_track1 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/n2c2_2018_track1/n2c2_2018_track1.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for n2c2_2018_track2 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/n2c2_2018_track2/n2c2_2018_track2.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for ncbi_disease contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/ncbi_disease/ncbi_disease.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for nlm_gene contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/nlm_gene/nlm_gene.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for nlm_wsd contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/nlm_wsd/nlm_wsd.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for nlmchem contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/nlmchem/nlmchem.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for ntcir_13_medweb contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/ntcir_13_medweb/ntcir_13_medweb.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for osiris contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/osiris/osiris.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for paramed contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/paramed/paramed.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for pcr contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/pcr/pcr.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for pdr contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/pdr/pdr.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for pharmaconer contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/pharmaconer/pharmaconer.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for pico_extraction contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/pico_extraction/pico_extraction.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for pmc_patients contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/pmc_patients/pmc_patients.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for progene contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/progene/progene.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for psytar contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/psytar/psytar.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for pubhealth contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/pubhealth/pubhealth.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for pubmed_qa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/pubmed_qa/pubmed_qa.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for pubtator_central contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/pubtator_central/pubtator_central.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for quaero contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/quaero/quaero.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for scai_chemical contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/scai_chemical/scai_chemical.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for scai_disease contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/scai_disease/scai_disease.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for scicite contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/scicite/scicite.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for scielo contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/scielo/scielo.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for scifact contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/scifact/scifact.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for sciq contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/sciq/sciq.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for scitail contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/scitail/scitail.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for sem_eval_2024_task_2 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/sem_eval_2024_task_2/sem_eval_2024_task_2.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for seth_corpus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/seth_corpus/seth_corpus.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for spl_adr_200db contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/spl_adr_200db/spl_adr_200db.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for swedish_medical_ner contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/swedish_medical_ner/swedish_medical_ner.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for tmvar_v1 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/tmvar_v1/tmvar_v1.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for tmvar_v2 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/tmvar_v2/tmvar_v2.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for tmvar_v3 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/tmvar_v3/tmvar_v3.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for twadrl contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/twadrl/twadrl.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for umnsrs contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/umnsrs/umnsrs.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:923: FutureWarning: The repository for verspoor_2013 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /home2/cye73/biomedical-entity-linking/biomedical/bigbio/hub/hub_repos/verspoor_2013/verspoor_2013.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import ujson\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from typing import Optional, Union\n",
    "\n",
    "from bigbio.dataloader import BigBioConfigHelpers\n",
    "\n",
    "sys.path.append('../../../..')\n",
    "sys.path.append('..')\n",
    "from DataModule import process_mention_dataset, process_umls_ontology, process_obo_ontology\n",
    "from umls_utils import UmlsMappings\n",
    "from bigbio_utils import CUIS_TO_REMAP, CUIS_TO_EXCLUDE, DATASET_NAMES, VALIDATION_DOCUMENT_IDS\n",
    "from bigbio_utils import dataset_to_documents, dataset_to_df, resolve_abbreviation, get_left_context, get_right_context\n",
    "from bioel.ontology import BiomedicalOntology\n",
    "\n",
    "conhelps = BigBioConfigHelpers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Specify the path to your TSV file\n",
    "file_path = '/mitchell/entity-linking/kbs/medic.tsv'\n",
    "\n",
    "key_dict = [\"DiseaseName\", \n",
    "            \"DiseaseID\", \n",
    "            \"AltDiseaseIDs\", \n",
    "            \"Definition\", \n",
    "            \"ParentIDs\", \n",
    "            \"TreeNumbers\", \n",
    "            \"ParentTreeNumbers\",\n",
    "            \"Synonyms\",\n",
    "            \"SlimMappings\"]\n",
    "# Open the TSV file\n",
    "with open(file_path, newline='') as tsvfile:\n",
    "    # Create a CSV reader specifying the delimiter as a tab character\n",
    "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "    \n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "    \n",
    "    ontology = []\n",
    "    # Iterate over the rows in the file\n",
    "    for row in reader:\n",
    "        dict = {}\n",
    "        # Print the current row\n",
    "        if counter > 28 :\n",
    "            for i, elements in enumerate(row) :\n",
    "                dict[key_dict[i]] = elements\n",
    "            disease_ids = [dict[\"DiseaseID\"]]  # Start with DiseaseID in the list\n",
    "            # If AltDiseaseIDs exists and is not empty, extend the list with its elements\n",
    "            if \"AltDiseaseIDs\" in dict and dict[\"AltDiseaseIDs\"]:\n",
    "                # Split AltDiseaseIDs by comma and extend the disease_ids list\n",
    "                disease_ids.extend(dict[\"AltDiseaseIDs\"].split(','))\n",
    "            # Replace DiseaseID with the combined list\n",
    "            dict[\"DiseaseID\"] = disease_ids\n",
    "            \n",
    "            ontology.append(dict)\n",
    "        # Increment the counter\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'DiseaseName': '15q24 Microdeletion',\n",
       "  'DiseaseID': ['MESH:C579849', 'DO:DOID:0060395'],\n",
       "  'AltDiseaseIDs': 'DO:DOID:0060395',\n",
       "  'Definition': '',\n",
       "  'ParentIDs': 'MESH:D002872|MESH:D008607|MESH:D025063',\n",
       "  'TreeNumbers': 'C10.597.606.360/C579849|C16.131.260/C579849|C16.320.180/C579849|C23.550.210.050.500.500/C579849|C23.888.592.604.646/C579849|F03.625.539/C579849',\n",
       "  'ParentTreeNumbers': 'C10.597.606.360|C16.131.260|C16.320.180|C23.550.210.050.500.500|C23.888.592.604.646|F03.625.539',\n",
       "  'Synonyms': '15q24 Deletion|15q24 Microdeletion Syndrome|Interstitial Deletion of Chromosome 15q24',\n",
       "  'SlimMappings': 'Congenital abnormality|Genetic disease (inborn)|Mental disorder|Nervous system disease|Pathology (process)|Signs and symptoms'},\n",
       " {'DiseaseName': '16p11.2 Deletion Syndrome',\n",
       "  'DiseaseID': ['MESH:C579850'],\n",
       "  'AltDiseaseIDs': '',\n",
       "  'Definition': '',\n",
       "  'ParentIDs': 'MESH:D001321|MESH:D002872|MESH:D008607|MESH:D025063',\n",
       "  'TreeNumbers': 'C10.597.606.360/C579850|C16.131.260/C579850|C16.320.180/C579850|C23.550.210.050.500.500/C579850|C23.888.592.604.646/C579850|F03.625.164.113.500/C579850|F03.625.539/C579850',\n",
       "  'ParentTreeNumbers': 'C10.597.606.360|C16.131.260|C16.320.180|C23.550.210.050.500.500|C23.888.592.604.646|F03.625.164.113.500|F03.625.539',\n",
       "  'Synonyms': '',\n",
       "  'SlimMappings': 'Congenital abnormality|Genetic disease (inborn)|Mental disorder|Nervous system disease|Pathology (process)|Signs and symptoms'},\n",
       " {'DiseaseName': '17,20-Lyase Deficiency, Isolated',\n",
       "  'DiseaseID': ['MESH:C567076'],\n",
       "  'AltDiseaseIDs': '',\n",
       "  'Definition': '',\n",
       "  'ParentIDs': 'MESH:D000312',\n",
       "  'TreeNumbers': 'C12.050.351.875.253.090.500/C567076|C12.200.706.316.090.500/C567076|C12.800.316.090.500/C567076|C16.131.939.316.129.500/C567076|C16.320.033/C567076|C16.320.565.925.249/C567076|C18.452.648.925.249/C567076|C19.053.440/C567076|C19.391.119.090.500/C567076',\n",
       "  'ParentTreeNumbers': 'C12.050.351.875.253.090.500|C12.200.706.316.090.500|C12.800.316.090.500|C16.131.939.316.129.500|C16.320.033|C16.320.565.925.249|C18.452.648.925.249|C19.053.440|C19.391.119.090.500',\n",
       "  'Synonyms': '17-Alpha-Hydroxylase-17,20-Lyase Deficiency, Combined Complete|17-Alpha-Hydroxylase-17,20-Lyase Deficiency, Combined Partial',\n",
       "  'SlimMappings': 'Congenital abnormality|Endocrine system disease|Genetic disease (inborn)|Metabolic disease|Urogenital disease (female)|Urogenital disease (male)'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ontology[i] for i in range(2,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# # Specify the path to your TSV file\n",
    "# file_path = '/mitchell/entity-linking/kbs/medic.tsv'\n",
    "\n",
    "# # Open the TSV file\n",
    "# with open(file_path, newline='') as tsvfile:\n",
    "#     # Create a CSV reader specifying the delimiter as a tab character\n",
    "#     reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "    \n",
    "#     # Initialize a counter\n",
    "#     counter = 0\n",
    "    \n",
    "#     # Iterate over the rows in the file\n",
    "#     for row in reader:\n",
    "#         # Print the current row\n",
    "#         if counter > 28: \n",
    "#             print(row)\n",
    "        \n",
    "#         # Increment the counter\n",
    "#         counter += 1\n",
    "        \n",
    "#         # Check if we've printed the first 5 rows\n",
    "#         if counter == 31:\n",
    "#             break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medic ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_medic(medic_dir):\n",
    "    \"This is needed because it's neither obo nor umls, it's a sort of preprocessing\"\n",
    "\n",
    "    key_dict = [\"DiseaseName\", \n",
    "                \"DiseaseID\", \n",
    "                \"AltDiseaseIDs\", \n",
    "                \"Definition\", \n",
    "                \"ParentIDs\", \n",
    "                \"TreeNumbers\", \n",
    "                \"ParentTreeNumbers\",\n",
    "                \"Synonyms\",\n",
    "                \"SlimMappings\"]\n",
    "    # Open the TSV file\n",
    "    with open(medic_dir, newline='') as tsvfile:\n",
    "        # Create a CSV reader specifying the delimiter as a tab character\n",
    "        reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "        \n",
    "        # Initialize a counter\n",
    "        counter = 0\n",
    "        \n",
    "        ontology = []\n",
    "        # Iterate over the rows in the file\n",
    "        for row in reader:\n",
    "            dict = {}\n",
    "            # Print the current row\n",
    "            if counter > 28 :\n",
    "                for i, elements in enumerate(row) :\n",
    "                    dict[key_dict[i]] = elements\n",
    "                \n",
    "                ontology.append(dict)\n",
    "            # Increment the counter\n",
    "            counter += 1\n",
    "    \n",
    "    return ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'DiseaseName': '15q24 Microdeletion',\n",
       "  'DiseaseID': 'MESH:C579849',\n",
       "  'AltDiseaseIDs': 'DO:DOID:0060395',\n",
       "  'Definition': '',\n",
       "  'ParentIDs': 'MESH:D002872|MESH:D008607|MESH:D025063',\n",
       "  'TreeNumbers': 'C10.597.606.360/C579849|C16.131.260/C579849|C16.320.180/C579849|C23.550.210.050.500.500/C579849|C23.888.592.604.646/C579849|F03.625.539/C579849',\n",
       "  'ParentTreeNumbers': 'C10.597.606.360|C16.131.260|C16.320.180|C23.550.210.050.500.500|C23.888.592.604.646|F03.625.539',\n",
       "  'Synonyms': '15q24 Deletion|15q24 Microdeletion Syndrome|Interstitial Deletion of Chromosome 15q24',\n",
       "  'SlimMappings': 'Congenital abnormality|Genetic disease (inborn)|Mental disorder|Nervous system disease|Pathology (process)|Signs and symptoms'},\n",
       " {'DiseaseName': '16p11.2 Deletion Syndrome',\n",
       "  'DiseaseID': 'MESH:C579850',\n",
       "  'AltDiseaseIDs': '',\n",
       "  'Definition': '',\n",
       "  'ParentIDs': 'MESH:D001321|MESH:D002872|MESH:D008607|MESH:D025063',\n",
       "  'TreeNumbers': 'C10.597.606.360/C579850|C16.131.260/C579850|C16.320.180/C579850|C23.550.210.050.500.500/C579850|C23.888.592.604.646/C579850|F03.625.164.113.500/C579850|F03.625.539/C579850',\n",
       "  'ParentTreeNumbers': 'C10.597.606.360|C16.131.260|C16.320.180|C23.550.210.050.500.500|C23.888.592.604.646|F03.625.164.113.500|F03.625.539',\n",
       "  'Synonyms': '',\n",
       "  'SlimMappings': 'Congenital abnormality|Genetic disease (inborn)|Mental disorder|Nervous system disease|Pathology (process)|Signs and symptoms'},\n",
       " {'DiseaseName': '17,20-Lyase Deficiency, Isolated',\n",
       "  'DiseaseID': 'MESH:C567076',\n",
       "  'AltDiseaseIDs': '',\n",
       "  'Definition': '',\n",
       "  'ParentIDs': 'MESH:D000312',\n",
       "  'TreeNumbers': 'C12.050.351.875.253.090.500/C567076|C12.200.706.316.090.500/C567076|C12.800.316.090.500/C567076|C16.131.939.316.129.500/C567076|C16.320.033/C567076|C16.320.565.925.249/C567076|C18.452.648.925.249/C567076|C19.053.440/C567076|C19.391.119.090.500/C567076',\n",
       "  'ParentTreeNumbers': 'C12.050.351.875.253.090.500|C12.200.706.316.090.500|C12.800.316.090.500|C16.131.939.316.129.500|C16.320.033|C16.320.565.925.249|C18.452.648.925.249|C19.053.440|C19.391.119.090.500',\n",
       "  'Synonyms': '17-Alpha-Hydroxylase-17,20-Lyase Deficiency, Combined Complete|17-Alpha-Hydroxylase-17,20-Lyase Deficiency, Combined Partial',\n",
       "  'SlimMappings': 'Congenital abnormality|Endocrine system disease|Genetic disease (inborn)|Metabolic disease|Urogenital disease (female)|Urogenital disease (male)'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ontology_dir = '/mitchell/entity-linking/kbs/medic.tsv'\n",
    "ontology = load_medic(ontology_dir)\n",
    "[ontology[i] for i in range(2,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medic_get_canonical_name(entities):\n",
    "    '''\n",
    "    Get name of entities in the ontology\n",
    "    data: list of dict\n",
    "    '''\n",
    "    canonical_names = {entity['DiseaseID']: entity['DiseaseName'] for entity in entities}\n",
    "    return canonical_names\n",
    "\n",
    "def medic_get_aliases(entities):\n",
    "    '''\n",
    "    Get aliases of entities in the ontology\n",
    "    data: list of dict\n",
    "    '''\n",
    "    aliases = {entity['DiseaseID']: entity['Synonyms'] for entity in entities}\n",
    "    return aliases\n",
    "\n",
    "def medic_get_definition(entities):\n",
    "    '''\n",
    "    Get definition of entities in the ontology\n",
    "    data: list of dict\n",
    "    '''\n",
    "    definitions_dict = {entity['DiseaseID']: entity['Definition'] for entity in entities if entity['Definition'] is not None}\n",
    "    return definitions_dict\n",
    "\n",
    "def medic_get_types(entities):\n",
    "    '''\n",
    "    Get type of entities in the ontology\n",
    "    data: list of dict\n",
    "    '''\n",
    "    # Extract tuples of CUI and types from the Data\n",
    "    types = {entity['DiseaseID']: entity['SlimMappings'] for entity in entities}\n",
    "    return types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('MESH:C538288', '10p Deletion Syndrome (Partial)'), ('MESH:C535484', '13q deletion syndrome'), ('MESH:C579849', '15q24 Microdeletion'), ('MESH:C579850', '16p11.2 Deletion Syndrome')]\n"
     ]
    }
   ],
   "source": [
    "cui2name = medic_get_canonical_name(ontology)\n",
    "print(list(cui2name.items())[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('MESH:C538288', 'Chromosome 10, 10p- Partial|Chromosome 10, monosomy 10p|Chromosome 10, Partial Deletion (short arm)|Monosomy 10p'), ('MESH:C535484', \"Chromosome 13q deletion|Chromosome 13q deletion syndrome|Chromosome 13q monosomy|Chromosome 13q syndrome|Deletion 13q|Deletion 13q syndrome|Monosomy 13q|Monosomy 13q syndrome|Orbeli's syndrome|Orbeli syndrome\"), ('MESH:C579849', '15q24 Deletion|15q24 Microdeletion Syndrome|Interstitial Deletion of Chromosome 15q24'), ('MESH:C579850', ''), ('MESH:C567076', '17-Alpha-Hydroxylase-17,20-Lyase Deficiency, Combined Complete|17-Alpha-Hydroxylase-17,20-Lyase Deficiency, Combined Partial')]\n"
     ]
    }
   ],
   "source": [
    "cui2alias = medic_get_aliases(ontology)\n",
    "print(list(cui2alias.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('MESH:C538288', ''), ('MESH:C535484', ''), ('MESH:C579849', ''), ('MESH:C579850', ''), ('MESH:C567076', '')]\n"
     ]
    }
   ],
   "source": [
    "cui2def = medic_get_definition(ontology)\n",
    "print(list(cui2def.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('MESH:C538288', 'Congenital abnormality|Genetic disease (inborn)|Pathology (process)'), ('MESH:C535484', 'Congenital abnormality|Genetic disease (inborn)|Pathology (process)'), ('MESH:C579849', 'Congenital abnormality|Genetic disease (inborn)|Mental disorder|Nervous system disease|Pathology (process)|Signs and symptoms'), ('MESH:C579850', 'Congenital abnormality|Genetic disease (inborn)|Mental disorder|Nervous system disease|Pathology (process)|Signs and symptoms'), ('MESH:C567076', 'Congenital abnormality|Endocrine system disease|Genetic disease (inborn)|Metabolic disease|Urogenital disease (female)|Urogenital disease (male)')]\n"
     ]
    }
   ],
   "source": [
    "cui2type = medic_get_types(ontology)\n",
    "print(list(cui2type.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_medic_ontology(ontology,\n",
    "                           data_path, \n",
    "                           ontology_dir, \n",
    "                        ):\n",
    "    '''\n",
    "    This function prepares the entity data : dictionary.pickle\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    - ontology : str (only umls for now)\n",
    "    Ontology associated with the dataset\n",
    "    - data_path : str\n",
    "    Path where to load and save dictionary.pickle\n",
    "    - ontology_dir : str\n",
    "    Path to medic data\n",
    "    '''\n",
    "    \n",
    "    entities = load_medic(ontology_dir)\n",
    "    \n",
    "    # Get canonical name of entities in the ontology\n",
    "    cui2name = medic_get_canonical_name(entities)\n",
    "    # Get aliases of entities in the ontology\n",
    "    cui2alias = medic_get_aliases(entities)\n",
    "    # Get definition of entities in the ontology\n",
    "    cui2definition = medic_get_definition(entities)\n",
    "    # Get types of entities in the ontology\n",
    "    cui2tui = medic_get_types(entities)\n",
    "\n",
    "\n",
    "    # Check if the directory exists, and create it if it does not\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "\n",
    "    ontology_entities = []\n",
    "    for cui, name in tqdm(cui2name.items()):\n",
    "        d = {}\n",
    "        ent_type = cui2tui[cui]\n",
    "        d['type'] = ent_type\n",
    "        # other_aliases = [x for x in cui2alias[cui] if x != name]\n",
    "        # joined_aliases = ' ; '.join(other_aliases)\n",
    "        d['cui'] = f\"{cui}\"\n",
    "        d['title'] = name\n",
    "        if cui2definition[cui] != \"\":\n",
    "            definition = cui2definition[cui]\n",
    "        else:\n",
    "            definition = None\n",
    "\n",
    "        if cui2alias[cui] is not None:\n",
    "            if definition is not None:\n",
    "                d['description'] = f\"{name} ( {ent_type} : {cui2alias[cui]} ) [ {definition} ]\"\n",
    "            else:\n",
    "                d['description'] = f\"{name} ( {ent_type} : {cui2alias[cui]} )\"\n",
    "        else:\n",
    "            if definition is not None:\n",
    "                d['description'] = f\"{name} ( {ent_type} ) [ {definition} ]\"\n",
    "            else:\n",
    "                d['description'] = f\"{name} ( {ent_type} )\"\n",
    "\n",
    "        ontology_entities.append(d)\n",
    "\n",
    "    pickle.dump(ontology_entities, open(os.path.join(data_path, 'dictionary.pickle'), 'wb'))\n",
    "    entities = pickle.load(open(os.path.join(data_path, 'dictionary.pickle'), 'rb'))\n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home2/cye73/data/arboel/ncbi_disease\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 13189/13189 [00:00<00:00, 708977.46it/s]\n"
     ]
    }
   ],
   "source": [
    "ontology_dir = '/mitchell/entity-linking/kbs/medic.tsv'\n",
    "ontology = \"MEDIC\"\n",
    "model = \"arboel\"\n",
    "dataset = \"ncbi_disease\"\n",
    "abs_path = \"/home2/cye73/data\"\n",
    "data_path = os.path.join(abs_path, model, dataset)\n",
    "print(data_path)\n",
    "\n",
    "new_entities = process_medic_ontology(ontology = ontology,\n",
    "                                    data_path= data_path,\n",
    "                                    ontology_dir = ontology_dir, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'Congenital abnormality|Genetic disease (inborn)|Pathology (process)',\n",
       "  'cui': 'MESH:C538288',\n",
       "  'title': '10p Deletion Syndrome (Partial)',\n",
       "  'description': '10p Deletion Syndrome (Partial) ( Congenital abnormality|Genetic disease (inborn)|Pathology (process) : Chromosome 10, 10p- Partial|Chromosome 10, monosomy 10p|Chromosome 10, Partial Deletion (short arm)|Monosomy 10p )'},\n",
       " {'type': 'Congenital abnormality|Genetic disease (inborn)|Pathology (process)',\n",
       "  'cui': 'MESH:C535484',\n",
       "  'title': '13q deletion syndrome',\n",
       "  'description': \"13q deletion syndrome ( Congenital abnormality|Genetic disease (inborn)|Pathology (process) : Chromosome 13q deletion|Chromosome 13q deletion syndrome|Chromosome 13q monosomy|Chromosome 13q syndrome|Deletion 13q|Deletion 13q syndrome|Monosomy 13q|Monosomy 13q syndrome|Orbeli's syndrome|Orbeli syndrome )\"},\n",
       " {'type': 'Congenital abnormality|Genetic disease (inborn)|Mental disorder|Nervous system disease|Pathology (process)|Signs and symptoms',\n",
       "  'cui': 'MESH:C579849',\n",
       "  'title': '15q24 Microdeletion',\n",
       "  'description': '15q24 Microdeletion ( Congenital abnormality|Genetic disease (inborn)|Mental disorder|Nervous system disease|Pathology (process)|Signs and symptoms : 15q24 Deletion|15q24 Microdeletion Syndrome|Interstitial Deletion of Chromosome 15q24 )'},\n",
       " {'type': 'Congenital abnormality|Genetic disease (inborn)|Mental disorder|Nervous system disease|Pathology (process)|Signs and symptoms',\n",
       "  'cui': 'MESH:C579850',\n",
       "  'title': '16p11.2 Deletion Syndrome',\n",
       "  'description': '16p11.2 Deletion Syndrome ( Congenital abnormality|Genetic disease (inborn)|Mental disorder|Nervous system disease|Pathology (process)|Signs and symptoms :  )'},\n",
       " {'type': 'Congenital abnormality|Endocrine system disease|Genetic disease (inborn)|Metabolic disease|Urogenital disease (female)|Urogenital disease (male)',\n",
       "  'cui': 'MESH:C567076',\n",
       "  'title': '17,20-Lyase Deficiency, Isolated',\n",
       "  'description': '17,20-Lyase Deficiency, Isolated ( Congenital abnormality|Endocrine system disease|Genetic disease (inborn)|Metabolic disease|Urogenital disease (female)|Urogenital disease (male) : 17-Alpha-Hydroxylase-17,20-Lyase Deficiency, Combined Complete|17-Alpha-Hydroxylase-17,20-Lyase Deficiency, Combined Partial )'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_entities[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for preparing the mentions in the dataset into the right format for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_mention_dataset(ontology,\n",
    "                            dataset,\n",
    "                            data_path,\n",
    "                            ontology_type,\n",
    "                            ontology_dir: Optional[str] = None,\n",
    "                            mention_id: Optional[bool] = True,\n",
    "                            context_doc_id: Optional[bool] = True,\n",
    "                            label: Optional[bool] = True\n",
    "                            ): \n",
    "    '''\n",
    "    This function prepares the mentions data :  Creates the train.jsonl, valid.jsonl, test.jsonl\n",
    "    Each .jsonl contains data in the following format : \n",
    "    {'mention': mention, \n",
    "    'mention_id': ID of the mention, (optional)\n",
    "    'context_left': context before mention,\n",
    "    'context_right': context after mention, \n",
    "    'context_doc_id': ID of the doc, (optional)\n",
    "    'type': type\n",
    "    'label_id': label_id,\n",
    "    'label': entity description, (optional)\n",
    "    'label_title': entity title\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    - ontology : str (only umls for now)\n",
    "    Ontology associated with the dataset\n",
    "    - dataset : str\n",
    "    Name of the dataset\n",
    "    - data_path : str\n",
    "    Path where to load and save dictionary.pickle\n",
    "    - ontology_type : str\n",
    "    'obo' or 'umls' and possibly others\n",
    "    - umls_dir : str\n",
    "    Path to the ontology (umls, medic etc...)\n",
    "    '''\n",
    "    data = conhelps.for_config_name(f'{dataset}_bigbio_kb').load_dataset()\n",
    "    exclude = CUIS_TO_EXCLUDE[dataset]\n",
    "    remap = CUIS_TO_REMAP[dataset]\n",
    "\n",
    "    'If dictionary already processed, load it else process and load it'\n",
    "    entity_dictionary_pkl_path = os.path.join(data_path, 'dictionary.pickle')\n",
    "    \n",
    "    if os.path.isfile(entity_dictionary_pkl_path): \n",
    "        print(\"Loading stored processed entity dictionary...\")\n",
    "        with open(entity_dictionary_pkl_path, 'rb') as read_handle:\n",
    "            entities = pickle.load(read_handle)\n",
    "    else :\n",
    "        if ontology_type == \"obo\" :\n",
    "            entities = process_obo_ontology(ontology, data_path)\n",
    "        elif ontology_type == \"medic\" : \n",
    "            entities = process_medic_ontology(ontology, data_path, ontology_dir)\n",
    "        elif ontology_type == \"umls\" : \n",
    "            entities = process_umls_ontology(ontology, data_path, ontology_dir)\n",
    "        else : \n",
    "            print(\"ERROR!\")\n",
    "\n",
    "    entity_dictionary = {d['cui']:d for d in tqdm(entities)} #CC1\n",
    "\n",
    "    # if dataset == 'ncbi_disease': #CC2\n",
    "    #     # Need to redo this since we have multiple synonymous CUIs for ncbi_disease\n",
    "    #     entity_dictionary = {cui:d for d in tqdm(entities) for cui in d['cui']}\n",
    "    #     cui_synsets = {}\n",
    "    #     for subdict in tqdm(entities): \n",
    "    #         for cui in subdict['cui']:\n",
    "    #             if cui in subdict:\n",
    "    #                 print(cui, cui_synsets[cui], subdict['cui'])\n",
    "    #             cui_synsets[cui] = subdict['cui'] \n",
    "    #     with open(os.path.join(data_path, 'cui_synsets.json'), 'w') as f:\n",
    "    #         f.write(ujson.dumps(cui_synsets, indent=2))\n",
    "\n",
    "    if dataset in VALIDATION_DOCUMENT_IDS:\n",
    "        validation_pmids = VALIDATION_DOCUMENT_IDS[dataset]\n",
    "    else:\n",
    "        print(\"ERROR!!!\")\n",
    "        \n",
    "    # Convert BigBio dataset to pandas DataFrame\n",
    "    df = dataset_to_df(data, entity_remapping_dict=remap, cuis_to_exclude=exclude, val_split_ids=validation_pmids)\n",
    "    print(df)\n",
    "    # print(df)\n",
    "    # Return dictionary of documents in BigBio dataset\n",
    "    docs = dataset_to_documents(data)\n",
    "    label_len = df['db_ids'].map(lambda x: len(x)).max()\n",
    "    print(\"Max labels on one doc:\", label_len)\n",
    "\n",
    "    for split in df.split.unique():\n",
    "        print(split)\n",
    "\n",
    "        ents_in_split = []\n",
    "        for d in tqdm(df.query(\"split == @split\").to_dict(orient='records'),\n",
    "                      desc=f\"Creating correct mention format for {split} dataset\"):\n",
    "            abbrev_resolved = False\n",
    "            offsets = d['offsets']\n",
    "            doc_id = d['document_id']\n",
    "            doc = docs[doc_id]\n",
    "            mention = d['text']\n",
    "            \n",
    "            # Get offsets and context\n",
    "            start = offsets[0][0] # start on the mention\n",
    "            end = offsets[-1][-1] # end of the mention\n",
    "            before_context = doc[:start] # left context\n",
    "            after_context = doc[end:] # right context\n",
    "            \n",
    "            \n",
    "            # ArboEL can't handle multi-labels, so we randomly choose one.\n",
    "            if len(d['db_ids']) == 1:\n",
    "                label_id = d['db_ids'][0]\n",
    "\n",
    "            # ncbi_disease is a special case that requires extra care\n",
    "            # elif dataset == 'ncbi_disease':\n",
    "            #     labels = []\n",
    "            #     used_cuis = set([])\n",
    "            #     choosable_ids = []\n",
    "            #     for db_id in d['db_ids']:\n",
    "            #         if db_id in used_cuis:\n",
    "            #             continue\n",
    "            #         else:\n",
    "            #             used_cuis.update(set(entity_dictionary[db_id]['cuis']))\n",
    "            #         choosable_ids.append(db_id)\n",
    "\n",
    "            #     label_id = np.random.choice(choosable_ids)\n",
    "            \n",
    "            else:\n",
    "                label_id = np.random.choice(d['db_ids'])\n",
    "\n",
    "            # Check if we missed something\n",
    "            if label_id not in entity_dictionary:\n",
    "                #print(label_id)\n",
    "                continue\n",
    "            \n",
    "\n",
    "            output = [{\n",
    "                'mention': mention, \n",
    "                'context_left': before_context,\n",
    "                'context_right': after_context, \n",
    "                'type': d['type'][0],\n",
    "                'label_id': label_id,\n",
    "                'label_title': entity_dictionary[label_id]['title'],\n",
    "            }]\n",
    "            \n",
    "            # print(\"I am here :\", output)\n",
    "            \n",
    "            if mention_id:\n",
    "                output[0]['mention_id'] = d.get('mention_id', None)\n",
    "        \n",
    "            if context_doc_id:\n",
    "                output[0]['context_doc_id'] = d.get('document_id', None)\n",
    "                \n",
    "            if context_doc_id:\n",
    "                output[0]['label'] = d.get(entity_dictionary[label_id]['description'], None)\n",
    "            \n",
    "            ents_in_split.extend(output)\n",
    "            \n",
    "\n",
    "        split_name = split\n",
    "        if split =='validation':\n",
    "            split_name = 'valid'\n",
    "        with open(os.path.join(data_path, f'{split_name}.jsonl'), 'w') as f:\n",
    "            f.write('\\n'.join([ujson.dumps(x) for x in ents_in_split]))\n",
    "        return ents_in_split\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home2/cye73/data_test/arboel/ncbi_disease\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:1454: FutureWarning: The repository for bigbio/ncbi_disease contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/bigbio/ncbi_disease\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stored processed entity dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 13189/13189 [00:00<00:00, 3142392.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     document_id         offsets                                     text  \\\n",
      "2       10021369      [[43, 76]]        adenomatous polyposis coli tumour   \n",
      "3       10021369     [[93, 132]]  adenomatous polyposis coli (APC) tumour   \n",
      "1       10021369    [[357, 372]]                          colon carcinoma   \n",
      "4       10021369    [[955, 970]]                          colon carcinoma   \n",
      "0       10021369  [[1090, 1096]]                                   cancer   \n",
      "...          ...             ...                                      ...   \n",
      "6804     9988281   [[996, 1015]]                      breast malignancies   \n",
      "6794     9988281  [[1123, 1147]]                 invasive lobular cancers   \n",
      "6795     9988281  [[1152, 1179]]              low-grade ductal carcinomas   \n",
      "6797     9988281  [[1269, 1286]]                        ductal carcinomas   \n",
      "6798     9988281  [[1387, 1410]]                  sporadic breast cancers   \n",
      "\n",
      "                   type          db_ids  split  mention_id  \n",
      "2            [Modifier]  [MESH:D011125]  train  10021369.1  \n",
      "3            [Modifier]  [MESH:D011125]  train  10021369.2  \n",
      "1            [Modifier]  [MESH:D003110]  train  10021369.3  \n",
      "4            [Modifier]  [MESH:D003110]  train  10021369.4  \n",
      "0     [SpecificDisease]  [MESH:D009369]  train  10021369.5  \n",
      "...                 ...             ...    ...         ...  \n",
      "6804  [SpecificDisease]  [MESH:D001943]   test   9988281.7  \n",
      "6794     [DiseaseClass]  [MESH:D018275]   test   9988281.8  \n",
      "6795  [SpecificDisease]  [MESH:D044584]   test   9988281.9  \n",
      "6797  [SpecificDisease]  [MESH:D044584]   test  9988281.10  \n",
      "6798  [SpecificDisease]  [MESH:D001943]   test  9988281.11  \n",
      "\n",
      "[6805 rows x 7 columns]\n",
      "Max labels on one doc: 5\n",
      "train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating correct mention format for train dataset: 100%|| 5065/5065 [00:00<00:00, 260160.05it/s]\n"
     ]
    }
   ],
   "source": [
    "ontology = \"MEDIC\"\n",
    "model = \"arboel\"\n",
    "dataset = \"ncbi_disease\"\n",
    "abs_path = \"/home2/cye73/data_test\"\n",
    "data_path = os.path.join(abs_path, model, dataset)\n",
    "print(data_path)\n",
    "\n",
    "ontology_type = \"medic\"\n",
    "ontology_dir = '/mitchell/entity-linking/kbs/medic.tsv'\n",
    "\n",
    "# ontology = \"MeSH\"\n",
    "# model = \"arboel\"\n",
    "# dataset = \"bc5cdr\"\n",
    "# abs_path = \"/home2/cye73/data\"\n",
    "# data_path = os.path.join(abs_path, model, dataset)\n",
    "# print(data_path)\n",
    "\n",
    "# ontology_type = \"umls\"\n",
    "# ontology_dir = \"/mitchell/entity-linking/2017AA/META/\"\n",
    "\n",
    "mentions = process_mention_dataset(ontology = ontology,\n",
    "                        dataset = dataset,\n",
    "                        data_path = data_path,\n",
    "                        ontology_type = ontology_type,\n",
    "                        ontology_dir = ontology_dir, \n",
    "                        mention_id = True,\n",
    "                        context_doc_id = True,\n",
    "                        label = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mention': 'adenomatous polyposis coli tumour',\n",
       " 'context_left': 'Identification of APC2, a homologue of the ',\n",
       " 'context_right': ' suppressor.\\nThe adenomatous polyposis coli (APC) tumour-suppressor protein controls the Wnt signalling pathway by forming a complex with glycogen synthase kinase 3beta (GSK-3beta), axin/conductin and betacatenin. Complex formation induces the rapid degradation of betacatenin. In colon carcinoma cells, loss of APC leads to the accumulation of betacatenin in the nucleus, where it binds to and activates the Tcf-4 transcription factor (reviewed in [1] [2]). Here, we report the identification and genomic structure of APC homologues. Mammalian APC2, which closely resembles APC in overall domain structure, was functionally analyzed and shown to contain two SAMP domains, both of which are required for binding to conductin. Like APC, APC2 regulates the formation of active betacatenin-Tcf complexes, as demonstrated using transient transcriptional activation assays in APC -/- colon carcinoma cells. Human APC2 maps to chromosome 19p13. 3. APC and APC2 may therefore have comparable functions in development and cancer.',\n",
       " 'type': 'Modifier',\n",
       " 'label_id': 'MESH:D011125',\n",
       " 'label_title': 'Adenomatous Polyposis Coli',\n",
       " 'mention_id': '10021369.1',\n",
       " 'context_doc_id': '10021369',\n",
       " 'label': None}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "mention: adenomatous polyposis coli tumour\n",
      "context_left: Identification of APC2, a homologue of the \n",
      "context_right:  suppressor.\n",
      "The adenomatous polyposis coli (APC) tumour-suppressor protein controls the Wnt signalling pathway by forming a complex with glycogen synthase kinase 3beta (GSK-3beta), axin/conductin and betacatenin. Complex formation induces the rapid degradation of betacatenin. In colon carcinoma cells, loss of APC leads to the accumulation of betacatenin in the nucleus, where it binds to and activates the Tcf-4 transcription factor (reviewed in [1] [2]). Here, we report the identification and genomic structure of APC homologues. Mammalian APC2, which closely resembles APC in overall domain structure, was functionally analyzed and shown to contain two SAMP domains, both of which are required for binding to conductin. Like APC, APC2 regulates the formation of active betacatenin-Tcf complexes, as demonstrated using transient transcriptional activation assays in APC -/- colon carcinoma cells. Human APC2 maps to chromosome 19p13. 3. APC and APC2 may therefore have comparable functions in development and cancer.\n",
      "type: Modifier\n",
      "label_id: MESH:D011125\n",
      "label_title: Adenomatous Polyposis Coli\n",
      "mention_id: 10021369.1\n",
      "context_doc_id: 10021369\n",
      "label: None\n",
      "------\n",
      "mention: adenomatous polyposis coli (APC) tumour\n",
      "context_left: Identification of APC2, a homologue of the adenomatous polyposis coli tumour suppressor.\n",
      "The \n",
      "context_right: -suppressor protein controls the Wnt signalling pathway by forming a complex with glycogen synthase kinase 3beta (GSK-3beta), axin/conductin and betacatenin. Complex formation induces the rapid degradation of betacatenin. In colon carcinoma cells, loss of APC leads to the accumulation of betacatenin in the nucleus, where it binds to and activates the Tcf-4 transcription factor (reviewed in [1] [2]). Here, we report the identification and genomic structure of APC homologues. Mammalian APC2, which closely resembles APC in overall domain structure, was functionally analyzed and shown to contain two SAMP domains, both of which are required for binding to conductin. Like APC, APC2 regulates the formation of active betacatenin-Tcf complexes, as demonstrated using transient transcriptional activation assays in APC -/- colon carcinoma cells. Human APC2 maps to chromosome 19p13. 3. APC and APC2 may therefore have comparable functions in development and cancer.\n",
      "type: Modifier\n",
      "label_id: MESH:D011125\n",
      "label_title: Adenomatous Polyposis Coli\n",
      "mention_id: 10021369.2\n",
      "context_doc_id: 10021369\n",
      "label: None\n",
      "------\n",
      "mention: colon carcinoma\n",
      "context_left: Identification of APC2, a homologue of the adenomatous polyposis coli tumour suppressor.\n",
      "The adenomatous polyposis coli (APC) tumour-suppressor protein controls the Wnt signalling pathway by forming a complex with glycogen synthase kinase 3beta (GSK-3beta), axin/conductin and betacatenin. Complex formation induces the rapid degradation of betacatenin. In \n",
      "context_right:  cells, loss of APC leads to the accumulation of betacatenin in the nucleus, where it binds to and activates the Tcf-4 transcription factor (reviewed in [1] [2]). Here, we report the identification and genomic structure of APC homologues. Mammalian APC2, which closely resembles APC in overall domain structure, was functionally analyzed and shown to contain two SAMP domains, both of which are required for binding to conductin. Like APC, APC2 regulates the formation of active betacatenin-Tcf complexes, as demonstrated using transient transcriptional activation assays in APC -/- colon carcinoma cells. Human APC2 maps to chromosome 19p13. 3. APC and APC2 may therefore have comparable functions in development and cancer.\n",
      "type: Modifier\n",
      "label_id: MESH:D003110\n",
      "label_title: Colonic Neoplasms\n",
      "mention_id: 10021369.3\n",
      "context_doc_id: 10021369\n",
      "label: None\n",
      "------\n",
      "mention: colon carcinoma\n",
      "context_left: Identification of APC2, a homologue of the adenomatous polyposis coli tumour suppressor.\n",
      "The adenomatous polyposis coli (APC) tumour-suppressor protein controls the Wnt signalling pathway by forming a complex with glycogen synthase kinase 3beta (GSK-3beta), axin/conductin and betacatenin. Complex formation induces the rapid degradation of betacatenin. In colon carcinoma cells, loss of APC leads to the accumulation of betacatenin in the nucleus, where it binds to and activates the Tcf-4 transcription factor (reviewed in [1] [2]). Here, we report the identification and genomic structure of APC homologues. Mammalian APC2, which closely resembles APC in overall domain structure, was functionally analyzed and shown to contain two SAMP domains, both of which are required for binding to conductin. Like APC, APC2 regulates the formation of active betacatenin-Tcf complexes, as demonstrated using transient transcriptional activation assays in APC -/- \n",
      "context_right:  cells. Human APC2 maps to chromosome 19p13. 3. APC and APC2 may therefore have comparable functions in development and cancer.\n",
      "type: Modifier\n",
      "label_id: MESH:D003110\n",
      "label_title: Colonic Neoplasms\n",
      "mention_id: 10021369.4\n",
      "context_doc_id: 10021369\n",
      "label: None\n",
      "------\n",
      "mention: cancer\n",
      "context_left: Identification of APC2, a homologue of the adenomatous polyposis coli tumour suppressor.\n",
      "The adenomatous polyposis coli (APC) tumour-suppressor protein controls the Wnt signalling pathway by forming a complex with glycogen synthase kinase 3beta (GSK-3beta), axin/conductin and betacatenin. Complex formation induces the rapid degradation of betacatenin. In colon carcinoma cells, loss of APC leads to the accumulation of betacatenin in the nucleus, where it binds to and activates the Tcf-4 transcription factor (reviewed in [1] [2]). Here, we report the identification and genomic structure of APC homologues. Mammalian APC2, which closely resembles APC in overall domain structure, was functionally analyzed and shown to contain two SAMP domains, both of which are required for binding to conductin. Like APC, APC2 regulates the formation of active betacatenin-Tcf complexes, as demonstrated using transient transcriptional activation assays in APC -/- colon carcinoma cells. Human APC2 maps to chromosome 19p13. 3. APC and APC2 may therefore have comparable functions in development and \n",
      "context_right: .\n",
      "type: SpecificDisease\n",
      "label_id: MESH:D009369\n",
      "label_title: Neoplasms\n",
      "mention_id: 10021369.5\n",
      "context_doc_id: 10021369\n",
      "label: None\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "abs_path2 = \"/home2/cye73/data_test/arboel/ncbi_disease/train.jsonl\"\n",
    "train_data = []\n",
    "\n",
    "# Open the file in text mode ('r'), not binary mode ('rb'), since we're reading text data\n",
    "with open(abs_path2, 'r') as read_handle:\n",
    "    for line in read_handle:\n",
    "        # Each line is a complete JSON object\n",
    "        train_data.append(json.loads(line))\n",
    "\n",
    "# Now, train_data is a list of dictionaries, where each dictionary is a line from your jsonl file\n",
    "for i in range(5) :\n",
    "    print(\"------\") \n",
    "    for key, value in train_data[i].items():\n",
    "        print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_medic(ontology_dir):\n",
    "    \"This is needed because it's neither obo nor umls, it's a sort of preprocessing\"\n",
    "\n",
    "    key_dict = [\"DiseaseName\", \n",
    "                \"DiseaseID\", \n",
    "                \"AltDiseaseIDs\", \n",
    "                \"Definition\", \n",
    "                \"ParentIDs\", \n",
    "                \"TreeNumbers\", \n",
    "                \"ParentTreeNumbers\",\n",
    "                \"Synonyms\",\n",
    "                \"SlimMappings\"]\n",
    "    # Open the TSV file\n",
    "    with open(ontology_dir, newline='') as tsvfile:\n",
    "        # Create a CSV reader specifying the delimiter as a tab character\n",
    "        reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "        \n",
    "        # Initialize a counter\n",
    "        counter = 0\n",
    "        \n",
    "        ontology = []\n",
    "        # Iterate over the rows in the file\n",
    "        for row in reader:\n",
    "            dict = {}\n",
    "            # Print the current row\n",
    "            if counter > 28 :\n",
    "                for i, elements in enumerate(row) :\n",
    "                    dict[key_dict[i]] = elements\n",
    "                disease_ids = [dict[\"DiseaseID\"]]\n",
    "                'Put the alternative disease IDs in DiseaseID (they are also valid cuis)'\n",
    "                # If AltDiseaseIDs exists and is not empty, extend the list with its elements\n",
    "                if \"AltDiseaseIDs\" in dict and dict[\"AltDiseaseIDs\"]:\n",
    "                    # Split AltDiseaseIDs by comma and extend the disease_ids list\n",
    "                    disease_ids.extend(dict[\"AltDiseaseIDs\"].split(','))\n",
    "                # Replace DiseaseID with the combined list\n",
    "                dict[\"DiseaseID\"] = disease_ids\n",
    "                    \n",
    "                ontology.append(dict)\n",
    "            # Increment the counter\n",
    "            counter += 1\n",
    "    \n",
    "    return ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'DiseaseName': '18-Hydroxylase deficiency',\n",
       "  'DiseaseID': ['MESH:C537806', 'OMIM:203400|OMIM:610600'],\n",
       "  'AltDiseaseIDs': 'OMIM:203400|OMIM:610600',\n",
       "  'Definition': '',\n",
       "  'ParentIDs': 'MESH:D006994',\n",
       "  'TreeNumbers': 'C19.053.500.480/C537806',\n",
       "  'ParentTreeNumbers': 'C19.053.500.480',\n",
       "  'Synonyms': '18-alpha hydroxylase deficiency|18-HYDROXYLASE DEFICIENCY|18-Oxidase Deficiency|Aldosterone deficiency 1|Aldosterone deficiency due to defect in 18-hydroxylase|ALDOSTERONE DEFICIENCY DUE TO DEFECT IN STEROID 18-HYDROXYLASE|ALDOSTERONE DEFICIENCY DUE TO DEFICIENCY OF STEROID 18-OXIDASE|ALDOSTERONE DEFICIENCY I|ALDOSTERONE DEFICIENCY II|Aldosterone Deficiency Type I|Aldosterone Deficiency Type II|CMO I Deficiency|CMO II Deficiency|Corticosterone methyloxidase type 1 deficiency|Corticosterone Methyloxidase Type I Deficiency|Corticosterone Methyloxidase Type II Deficiency|FHHA1A|FHHA1B|HYPERRENINEMIC HYPOALDOSTERONISM, FAMILIAL, 1|Hyperreninemic Hypoaldosteronism, Familial, Type I|Steroid 18-Hydroxylase Deficiency|Steroid 18-Oxidase Deficiency',\n",
       "  'SlimMappings': 'Endocrine system disease'},\n",
       " {'DiseaseName': '22q11 Deletion Syndrome',\n",
       "  'DiseaseID': ['MESH:D058165'],\n",
       "  'AltDiseaseIDs': '',\n",
       "  'Definition': 'Condition with a variable constellation of phenotypes due to deletion polymorphisms at chromosome location 22q11. It encompasses several syndromes with overlapping abnormalities including the DIGEORGE SYNDROME, VELOCARDIOFACIAL SYNDROME, and CONOTRUNCAL AMOMALY FACE SYNDROME. In addition, variable developmental problems and schizoid features are also associated with this syndrome. (From BMC Med Genet. 2009 Feb 25;10:16) Not all deletions at 22q11 result in the 22q11deletion syndrome.',\n",
       "  'ParentIDs': 'MESH:D000015|MESH:D006330|MESH:D007011|MESH:D019465|MESH:D025063|MESH:D044148',\n",
       "  'TreeNumbers': 'C05.660.207.103|C14.240.400.021|C14.280.400.044|C15.604.451.249|C16.131.077.019|C16.131.240.400.021|C16.131.260.019|C16.131.482.249|C16.131.621.207.103|C16.320.180.019|C19.642.482.500',\n",
       "  'ParentTreeNumbers': 'C05.660.207|C14.240.400|C14.280.400|C15.604.451|C16.131.077|C16.131.240.400|C16.131.260|C16.131.482|C16.131.621.207|C16.320.180|C19.642.482',\n",
       "  'Synonyms': '22q11 Deletion Syndromes|Deletion Syndrome, 22q11|Deletion Syndromes, 22q11|Syndrome, 22q11 Deletion|Syndromes, 22q11 Deletion',\n",
       "  'SlimMappings': 'Cardiovascular disease|Congenital abnormality|Endocrine system disease|Genetic disease (inborn)|Lymphatic disease|Musculoskeletal disease'},\n",
       " {'DiseaseName': '2,4-Dienoyl-CoA Reductase Deficiency',\n",
       "  'DiseaseID': ['MESH:C565624', 'OMIM:616034'],\n",
       "  'AltDiseaseIDs': 'OMIM:616034',\n",
       "  'Definition': '',\n",
       "  'ParentIDs': 'MESH:D008052',\n",
       "  'TreeNumbers': 'C16.320.565.398/C565624|C18.452.584.563/C565624|C18.452.648.398/C565624',\n",
       "  'ParentTreeNumbers': 'C16.320.565.398|C18.452.584.563|C18.452.648.398',\n",
       "  'Synonyms': 'DECRD',\n",
       "  'SlimMappings': 'Genetic disease (inborn)|Metabolic disease'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ontology_dir = '/mitchell/entity-linking/kbs/medic.tsv'\n",
    "ontology = load_medic(ontology_dir)\n",
    "[ontology[i] for i in range(6,9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medic_get_canonical_name(entities):\n",
    "    '''\n",
    "    Get name of entities in the ontology\n",
    "    entities: list of dict, where each dict represents an entity and\n",
    "    'DiseaseID' is expected to be a list of IDs.\n",
    "    '''\n",
    "    canonical_names = {}\n",
    "    for entity in entities:\n",
    "        for disease_id in entity['DiseaseID']:\n",
    "            canonical_names[disease_id] = entity['DiseaseName']\n",
    "    return canonical_names\n",
    "\n",
    "def medic_get_aliases(entities):\n",
    "    '''\n",
    "    Get aliases of entities in the ontology\n",
    "    data: list of dict\n",
    "    '''\n",
    "    aliases = {}\n",
    "    for entity in entities:\n",
    "        for disease_id in entity['DiseaseID']:\n",
    "            aliases[disease_id] = entity['Synonyms']\n",
    "    return aliases\n",
    "\n",
    "\n",
    "def medic_get_definition(entities):\n",
    "    '''\n",
    "    Get definition of entities in the ontology\n",
    "    entities: list of dict\n",
    "    '''\n",
    "    definitions_dict = {}\n",
    "    for entity in entities:\n",
    "        if entity['Definition']:\n",
    "            for disease_id in entity['DiseaseID']:\n",
    "                definitions_dict[disease_id] = entity['Definition']\n",
    "    return definitions_dict\n",
    "\n",
    "def medic_get_types(entities):\n",
    "    '''\n",
    "    Get type of entities in the ontology\n",
    "    entities: list of dict\n",
    "    '''\n",
    "    types = {}\n",
    "    for entity in entities:\n",
    "        for disease_id in entity['DiseaseID']:\n",
    "            types[disease_id] = entity['SlimMappings']\n",
    "    return types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_medic2_ontology(ontology, data_path, ontology_dir):\n",
    "    '''\n",
    "    This function prepares the entity data : dictionary.pickle\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    - ontology : str (only umls for now)\n",
    "        Ontology associated with the dataset\n",
    "    - data_path : str\n",
    "        Path where to load and save dictionary.pickle\n",
    "    - ontology_dir : str\n",
    "        Path to medic data\n",
    "    '''\n",
    "    \n",
    "    entities = load_medic(ontology_dir)  # Assuming this returns a list of dictionaries like the provided example\n",
    "    \n",
    "    ontology_entities = []\n",
    "    for entity in tqdm(entities):\n",
    "        # Combining 'DiseaseID' and 'AltDiseaseIDs' into a single list, ensuring no duplicates\n",
    "        cui_list = [entity['DiseaseID']]\n",
    "        alt_ids = entity['AltDiseaseIDs'].split('|') if entity['AltDiseaseIDs'] else []\n",
    "        for alt_id in alt_ids:\n",
    "            if alt_id not in cui_list:\n",
    "                cui_list.append(alt_id)\n",
    "                \n",
    "        if entity['Synonyms'] != \"\":\n",
    "            if entity['Definition'] != \"\":\n",
    "                new_entity = {\n",
    "                    'type': entity['SlimMappings'],\n",
    "                    'cui': cui_list,\n",
    "                    'title': entity['DiseaseName'],\n",
    "                    'description': f\"{entity['DiseaseName']} ( {entity['SlimMappings']} : {entity['Synonyms']} ) [{entity['Definition']}]\"\n",
    "                }\n",
    "            else : \n",
    "                new_entity = {\n",
    "                    'type': entity['SlimMappings'],\n",
    "                    'cui': cui_list,\n",
    "                    'title': entity['DiseaseName'],\n",
    "                    'description': f\"{entity['DiseaseName']} ( {entity['SlimMappings']} : {entity['Synonyms']} )\"\n",
    "                }\n",
    "                \n",
    "        else : \n",
    "            if entity['Definition'] != \"\":\n",
    "                new_entity = {\n",
    "                        'type': entity['SlimMappings'],\n",
    "                        'cui': cui_list,\n",
    "                        'title': entity['DiseaseName'],\n",
    "                        'description': f\"{entity['DiseaseName']} ( {entity['SlimMappings']}) [{entity['Definition']}]\"\n",
    "                    }\n",
    "            else : \n",
    "                new_entity = {\n",
    "                        'type': entity['SlimMappings'],\n",
    "                        'cui': cui_list,\n",
    "                        'title': entity['DiseaseName'],\n",
    "                        'description': f\"{entity['DiseaseName']} ( {entity['SlimMappings']})\"\n",
    "                    }\n",
    "                \n",
    "            \n",
    "        ontology_entities.append(new_entity)\n",
    "\n",
    "    # Save entities to pickle file\n",
    "    with open(os.path.join(data_path, 'dictionary.pickle'), 'wb') as f:\n",
    "        pickle.dump(ontology_entities, f)\n",
    "\n",
    "    # Optional: Load and return to confirm save was successful\n",
    "    with open(os.path.join(data_path, 'dictionary.pickle'), 'rb') as f:\n",
    "        entities = pickle.load(f)\n",
    "        \n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home2/cye73/data/arboel/ncbi_disease\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 13189/13189 [00:00<00:00, 70474.68it/s]\n"
     ]
    }
   ],
   "source": [
    "ontology_dir = '/mitchell/entity-linking/kbs/medic.tsv'\n",
    "ontology = \"MEDIC\"\n",
    "model = \"arboel\"\n",
    "dataset = \"ncbi_disease\"\n",
    "abs_path = \"/home2/cye73/data\"\n",
    "data_path = os.path.join(abs_path, model, dataset)\n",
    "print(data_path)\n",
    "\n",
    "entities = process_medic2_ontology(ontology = ontology,\n",
    "                                    data_path= data_path,\n",
    "                                    ontology_dir = ontology_dir, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'Congenital abnormality|Genetic disease (inborn)|Mental disorder|Nervous system disease|Pathology (process)|Signs and symptoms',\n",
       " 'cui': [['MESH:C579849', 'DO:DOID:0060395'], 'DO:DOID:0060395'],\n",
       " 'title': '15q24 Microdeletion',\n",
       " 'description': '15q24 Microdeletion ( Congenital abnormality|Genetic disease (inborn)|Mental disorder|Nervous system disease|Pathology (process)|Signs and symptoms : 15q24 Deletion|15q24 Microdeletion Syndrome|Interstitial Deletion of Chromosome 15q24 )'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict :\n",
      " {'type': 'Congenital abnormality|Genetic disease (inborn)|Mental disorder|Nervous system disease|Pathology (process)|Signs and symptoms', 'cui': [['MESH:C579849', 'DO:DOID:0060395'], 'DO:DOID:0060395'], 'title': '15q24 Microdeletion', 'description': '15q24 Microdeletion ( Congenital abnormality|Genetic disease (inborn)|Mental disorder|Nervous system disease|Pathology (process)|Signs and symptoms : 15q24 Deletion|15q24 Microdeletion Syndrome|Interstitial Deletion of Chromosome 15q24 )'}\n",
      "dict2 :\n",
      " {'type': 'Disease', 'cui': 'MESH:C579849', 'title': '15q24 Microdeletion', 'cuis': ['MESH:C579849'], 'description': '15q24 Microdeletion ( Disease : 15q24 Deletion ; 15q24 Microdeletion Syndrome ; Interstitial Deletion of Chromosome 15q24 )'}\n"
     ]
    }
   ],
   "source": [
    "#path_entity = '/home2/cye73/data_test2/arboel/ncbi_disease/dictionary.pickle'\n",
    "path_entity = '/home2/cye73/data/arboel/ncbi_disease/dictionary.pickle'\n",
    "path_entity2 = '/home2/cye73/arboEL2/data/arboel/ncbi_disease/dictionary.pickle'\n",
    "with open(path_entity, 'rb') as read_handle:\n",
    "    dict = pickle.load(read_handle)\n",
    "with open(path_entity2, 'rb') as read_handle:\n",
    "    dict2 = pickle.load(read_handle)\n",
    "\n",
    "print(\"dict :\\n\", dict[2])\n",
    "print(\"dict2 :\\n\", dict2[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 13189/13189 [00:00<00:00, 575834.32it/s]\n"
     ]
    }
   ],
   "source": [
    "cui_synsets = {}\n",
    "for subdict in tqdm(entities): \n",
    "    for cui in subdict['cui']:\n",
    "        if cui in subdict:\n",
    "            print(cui, cui_synsets[cui], subdict['cui'])\n",
    "        cui_synsets[cui] = subdict['cui'] \n",
    "with open(os.path.join(data_path, 'cui_synsets.json'), 'w') as f:\n",
    "    f.write(ujson.dumps(cui_synsets, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Optional' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_mention2_dataset\u001b[39m(ontology,\n\u001b[1;32m      2\u001b[0m                             dataset,\n\u001b[1;32m      3\u001b[0m                             data_path,\n\u001b[1;32m      4\u001b[0m                             ontology_type,\n\u001b[0;32m----> 5\u001b[0m                             ontology_dir: \u001b[43mOptional\u001b[49m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m                             mention_id: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m                             context_doc_id: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m                             label: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      9\u001b[0m                             ): \n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    This function prepares the mentions data :  Creates the train.jsonl, valid.jsonl, test.jsonl\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    Each .jsonl contains data in the following format : \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    Path to the ontology (umls, medic etc...)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     data \u001b[38;5;241m=\u001b[39m conhelps\u001b[38;5;241m.\u001b[39mfor_config_name(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_bigbio_kb\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mload_dataset()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Optional' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_mention2_dataset(ontology,\n",
    "                            dataset,\n",
    "                            data_path,\n",
    "                            ontology_type,\n",
    "                            ontology_dir: Optional[str] = None,\n",
    "                            mention_id: Optional[bool] = True,\n",
    "                            context_doc_id: Optional[bool] = True,\n",
    "                            label: Optional[bool] = True\n",
    "                            ): \n",
    "    '''\n",
    "    This function prepares the mentions data :  Creates the train.jsonl, valid.jsonl, test.jsonl\n",
    "    Each .jsonl contains data in the following format : \n",
    "    {'mention': mention, \n",
    "    'mention_id': ID of the mention, (optional)\n",
    "    'context_left': context before mention,\n",
    "    'context_right': context after mention, \n",
    "    'context_doc_id': ID of the doc, (optional)\n",
    "    'type': type\n",
    "    'label_id': label_id,\n",
    "    'label': entity description, (optional)\n",
    "    'label_title': entity title\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    - ontology : str (only umls for now)\n",
    "    Ontology associated with the dataset\n",
    "    - dataset : str\n",
    "    Name of the dataset\n",
    "    - data_path : str\n",
    "    Path where to load and save dictionary.pickle\n",
    "    - ontology_type : str\n",
    "    'obo' or 'umls' and possibly others\n",
    "    - umls_dir : str\n",
    "    Path to the ontology (umls, medic etc...)\n",
    "    '''\n",
    "    data = conhelps.for_config_name(f'{dataset}_bigbio_kb').load_dataset()\n",
    "    exclude = CUIS_TO_EXCLUDE[dataset]\n",
    "    remap = CUIS_TO_REMAP[dataset]\n",
    "\n",
    "    'If dictionary already processed, load it else process and load it'\n",
    "    entity_dictionary_pkl_path = os.path.join(data_path, 'dictionary.pickle')\n",
    "    \n",
    "    if os.path.isfile(entity_dictionary_pkl_path): \n",
    "        print(\"Loading stored processed entity dictionary...\")\n",
    "        with open(entity_dictionary_pkl_path, 'rb') as read_handle:\n",
    "            entities = pickle.load(read_handle)\n",
    "    else :\n",
    "        if ontology_type == \"obo\" :\n",
    "            entities = process_obo_ontology(ontology, data_path)\n",
    "        elif ontology_type == \"medic\" : \n",
    "            entities = process_medic2_ontology(ontology, data_path, ontology_dir)\n",
    "        elif ontology_type == \"umls\" : \n",
    "            entities = process_umls_ontology(ontology, data_path, ontology_dir)\n",
    "        else : \n",
    "            print(\"ERROR!\")\n",
    "\n",
    "    entity_dictionary = {d['cui']:d for d in tqdm(entities)} #CC1\n",
    "\n",
    "    if dataset == 'ncbi_disease': #CC2\n",
    "        # Need to redo this since we have multiple synonymous CUIs for ncbi_disease\n",
    "        entity_dictionary = {cui:d for d in tqdm(entities) for cui in d['cui']}\n",
    "        cui_synsets = {}\n",
    "        for subdict in tqdm(entities): \n",
    "            for cui in subdict['cui']:\n",
    "                if cui in subdict:\n",
    "                    print(cui, cui_synsets[cui], subdict['cui'])\n",
    "                cui_synsets[cui] = subdict['cui'] \n",
    "        with open(os.path.join(data_path, 'cui_synsets.json'), 'w') as f:\n",
    "            f.write(ujson.dumps(cui_synsets, indent=2))\n",
    "\n",
    "    if dataset in VALIDATION_DOCUMENT_IDS:\n",
    "        validation_pmids = VALIDATION_DOCUMENT_IDS[dataset]\n",
    "    else:\n",
    "        print(\"ERROR!!!\")\n",
    "        \n",
    "    # Convert BigBio dataset to pandas DataFrame\n",
    "    df = dataset_to_df(data, entity_remapping_dict=remap, cuis_to_exclude=exclude, val_split_ids=validation_pmids)\n",
    "    # Return dictionary of documents in BigBio dataset\n",
    "    docs = dataset_to_documents(data)\n",
    "    label_len = df['db_ids'].map(lambda x: len(x)).max()\n",
    "    print(\"Max labels on one doc:\", label_len)\n",
    "\n",
    "    for split in df.split.unique():\n",
    "        print(split)\n",
    "\n",
    "        ents_in_split = []\n",
    "        for d in tqdm(df.query(\"split == @split\").to_dict(orient='records'),\n",
    "                      desc=f\"Creating correct mention format for {split} dataset\"):\n",
    "            abbrev_resolved = False\n",
    "            offsets = d['offsets']\n",
    "            doc_id = d['document_id']\n",
    "            doc = docs[doc_id]\n",
    "            mention = d['text']\n",
    "            \n",
    "            # Get offsets and context\n",
    "            start = offsets[0][0] # start on the mention\n",
    "            end = offsets[-1][-1] # end of the mention\n",
    "            before_context = doc[:start] # left context\n",
    "            after_context = doc[end:] # right context\n",
    "            \n",
    "            \n",
    "            # ArboEL can't handle multi-labels, so we randomly choose one.\n",
    "            if len(d['db_ids']) == 1:\n",
    "                label_id = d['db_ids'][0]\n",
    "\n",
    "            # ncbi_disease is a special case that requires extra care\n",
    "            elif dataset == 'ncbi_disease':\n",
    "                labels = []\n",
    "                used_cuis = set([])\n",
    "                choosable_ids = []\n",
    "                for db_id in d['db_ids']:\n",
    "                    if db_id in used_cuis:\n",
    "                        continue\n",
    "                    else:\n",
    "                        used_cuis.update(set(entity_dictionary[db_id]['cuis']))\n",
    "                    choosable_ids.append(db_id)\n",
    "\n",
    "                label_id = np.random.choice(choosable_ids)\n",
    "            \n",
    "            else:\n",
    "                label_id = np.random.choice(d['db_ids'])\n",
    "\n",
    "            # Check if we missed something\n",
    "            if label_id not in entity_dictionary:\n",
    "                # print(label_id)\n",
    "                continue\n",
    "            \n",
    "\n",
    "            \n",
    "            output = [{\n",
    "                'mention': mention, \n",
    "                'context_left': before_context,\n",
    "                'context_right': after_context, \n",
    "                'type': d['type'][0],\n",
    "                'label_id': label_id,\n",
    "                'label_title': entity_dictionary[label_id]['title'],\n",
    "            }]\n",
    "            \n",
    "            if mention_id:\n",
    "                output[0]['mention_id'] = d.get('mention_id', None)\n",
    "        \n",
    "            if context_doc_id:\n",
    "                output[0]['context_doc_id'] = d.get('document_id', None)\n",
    "                \n",
    "            if context_doc_id:\n",
    "                output[0]['label'] = d.get(entity_dictionary[label_id]['description'], None)\n",
    "\n",
    "            ents_in_split.extend(output)\n",
    "\n",
    "        split_name = split\n",
    "        if split =='validation':\n",
    "            split_name = 'valid'\n",
    "        with open(os.path.join(data_path, f'{split_name}.jsonl'), 'w') as f:\n",
    "            f.write('\\n'.join([ujson.dumps(x) for x in ents_in_split]))\n",
    "        return output\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home2/cye73/data_test/arboel/ncbi_disease\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:1454: FutureWarning: The repository for bigbio/ncbi_disease contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/bigbio/ncbi_disease\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stored processed entity dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 13189/13189 [00:00<00:00, 2602986.80it/s]\n",
      "100%|| 13189/13189 [00:00<00:00, 1408818.71it/s]\n",
      "100%|| 13189/13189 [00:00<00:00, 716776.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max labels on one doc: 5\n",
      "train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating correct mention format for train dataset:   0%|          | 14/5065 [00:00<00:00, 213528.20it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'MESH:D016889'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [94], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m ontology_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/mitchell/entity-linking/kbs/medic.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# ontology_dir=\"/mitchell/entity-linking/2017AA/META/\"\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m entity_dictionary \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_mention_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43montology\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43montology\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                        \u001b[49m\u001b[43montology_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43montology_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                        \u001b[49m\u001b[43montology_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43montology_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmention_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcontext_doc_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [93], line 115\u001b[0m, in \u001b[0;36mprocess_mention_dataset\u001b[0;34m(ontology, dataset, data_path, ontology_type, ontology_dir, mention_id, context_doc_id, label)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m         used_cuis\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mset\u001b[39m(\u001b[43mentity_dictionary\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdb_id\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuis\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m    116\u001b[0m     choosable_ids\u001b[38;5;241m.\u001b[39mappend(db_id)\n\u001b[1;32m    118\u001b[0m label_id \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(choosable_ids)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'MESH:D016889'"
     ]
    }
   ],
   "source": [
    "ontology = \"MEDIC\"\n",
    "model = \"arboel\"\n",
    "dataset = \"ncbi_disease\"\n",
    "abs_path = \"/home2/cye73/data_test\"\n",
    "data_path = os.path.join(abs_path, model, dataset)\n",
    "print(data_path)\n",
    "\n",
    "ontology_type = \"medic\"\n",
    "ontology_dir = '/mitchell/entity-linking/kbs/medic.tsv'\n",
    "# ontology_dir=\"/mitchell/entity-linking/2017AA/META/\"\n",
    "\n",
    "entity_dictionary = process_mention2_dataset(ontology = ontology,\n",
    "                        dataset = dataset,\n",
    "                        data_path = data_path,\n",
    "                        ontology_type = ontology_type,\n",
    "                        ontology_dir = ontology_dir, \n",
    "                        mention_id = True,\n",
    "                        context_doc_id = True,\n",
    "                        label = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "mention: adenomatous polyposis coli tumour\n",
      "context_left: Identification of APC2, a homologue of the \n",
      "context_right:  suppressor.\n",
      "The adenomatous polyposis coli (APC) tumour-suppressor protein controls the Wnt signalling pathway by forming a complex with glycogen synthase kinase 3beta (GSK-3beta), axin/conductin and betacatenin. Complex formation induces the rapid degradation of betacatenin. In colon carcinoma cells, loss of APC leads to the accumulation of betacatenin in the nucleus, where it binds to and activates the Tcf-4 transcription factor (reviewed in [1] [2]). Here, we report the identification and genomic structure of APC homologues. Mammalian APC2, which closely resembles APC in overall domain structure, was functionally analyzed and shown to contain two SAMP domains, both of which are required for binding to conductin. Like APC, APC2 regulates the formation of active betacatenin-Tcf complexes, as demonstrated using transient transcriptional activation assays in APC -/- colon carcinoma cells. Human APC2 maps to chromosome 19p13. 3. APC and APC2 may therefore have comparable functions in development and cancer.\n",
      "type: Modifier\n",
      "label_id: MESH:D011125\n",
      "label_title: Adenomatous Polyposis Coli\n",
      "mention_id: 10021369.1\n",
      "context_doc_id: 10021369\n",
      "label: None\n",
      "------\n",
      "mention: adenomatous polyposis coli (APC) tumour\n",
      "context_left: Identification of APC2, a homologue of the adenomatous polyposis coli tumour suppressor.\n",
      "The \n",
      "context_right: -suppressor protein controls the Wnt signalling pathway by forming a complex with glycogen synthase kinase 3beta (GSK-3beta), axin/conductin and betacatenin. Complex formation induces the rapid degradation of betacatenin. In colon carcinoma cells, loss of APC leads to the accumulation of betacatenin in the nucleus, where it binds to and activates the Tcf-4 transcription factor (reviewed in [1] [2]). Here, we report the identification and genomic structure of APC homologues. Mammalian APC2, which closely resembles APC in overall domain structure, was functionally analyzed and shown to contain two SAMP domains, both of which are required for binding to conductin. Like APC, APC2 regulates the formation of active betacatenin-Tcf complexes, as demonstrated using transient transcriptional activation assays in APC -/- colon carcinoma cells. Human APC2 maps to chromosome 19p13. 3. APC and APC2 may therefore have comparable functions in development and cancer.\n",
      "type: Modifier\n",
      "label_id: MESH:D011125\n",
      "label_title: Adenomatous Polyposis Coli\n",
      "mention_id: 10021369.2\n",
      "context_doc_id: 10021369\n",
      "label: None\n",
      "------\n",
      "mention: colon carcinoma\n",
      "context_left: Identification of APC2, a homologue of the adenomatous polyposis coli tumour suppressor.\n",
      "The adenomatous polyposis coli (APC) tumour-suppressor protein controls the Wnt signalling pathway by forming a complex with glycogen synthase kinase 3beta (GSK-3beta), axin/conductin and betacatenin. Complex formation induces the rapid degradation of betacatenin. In \n",
      "context_right:  cells, loss of APC leads to the accumulation of betacatenin in the nucleus, where it binds to and activates the Tcf-4 transcription factor (reviewed in [1] [2]). Here, we report the identification and genomic structure of APC homologues. Mammalian APC2, which closely resembles APC in overall domain structure, was functionally analyzed and shown to contain two SAMP domains, both of which are required for binding to conductin. Like APC, APC2 regulates the formation of active betacatenin-Tcf complexes, as demonstrated using transient transcriptional activation assays in APC -/- colon carcinoma cells. Human APC2 maps to chromosome 19p13. 3. APC and APC2 may therefore have comparable functions in development and cancer.\n",
      "type: Modifier\n",
      "label_id: MESH:D003110\n",
      "label_title: Colonic Neoplasms\n",
      "mention_id: 10021369.3\n",
      "context_doc_id: 10021369\n",
      "label: None\n",
      "------\n",
      "mention: colon carcinoma\n",
      "context_left: Identification of APC2, a homologue of the adenomatous polyposis coli tumour suppressor.\n",
      "The adenomatous polyposis coli (APC) tumour-suppressor protein controls the Wnt signalling pathway by forming a complex with glycogen synthase kinase 3beta (GSK-3beta), axin/conductin and betacatenin. Complex formation induces the rapid degradation of betacatenin. In colon carcinoma cells, loss of APC leads to the accumulation of betacatenin in the nucleus, where it binds to and activates the Tcf-4 transcription factor (reviewed in [1] [2]). Here, we report the identification and genomic structure of APC homologues. Mammalian APC2, which closely resembles APC in overall domain structure, was functionally analyzed and shown to contain two SAMP domains, both of which are required for binding to conductin. Like APC, APC2 regulates the formation of active betacatenin-Tcf complexes, as demonstrated using transient transcriptional activation assays in APC -/- \n",
      "context_right:  cells. Human APC2 maps to chromosome 19p13. 3. APC and APC2 may therefore have comparable functions in development and cancer.\n",
      "type: Modifier\n",
      "label_id: MESH:D003110\n",
      "label_title: Colonic Neoplasms\n",
      "mention_id: 10021369.4\n",
      "context_doc_id: 10021369\n",
      "label: None\n",
      "------\n",
      "mention: cancer\n",
      "context_left: Identification of APC2, a homologue of the adenomatous polyposis coli tumour suppressor.\n",
      "The adenomatous polyposis coli (APC) tumour-suppressor protein controls the Wnt signalling pathway by forming a complex with glycogen synthase kinase 3beta (GSK-3beta), axin/conductin and betacatenin. Complex formation induces the rapid degradation of betacatenin. In colon carcinoma cells, loss of APC leads to the accumulation of betacatenin in the nucleus, where it binds to and activates the Tcf-4 transcription factor (reviewed in [1] [2]). Here, we report the identification and genomic structure of APC homologues. Mammalian APC2, which closely resembles APC in overall domain structure, was functionally analyzed and shown to contain two SAMP domains, both of which are required for binding to conductin. Like APC, APC2 regulates the formation of active betacatenin-Tcf complexes, as demonstrated using transient transcriptional activation assays in APC -/- colon carcinoma cells. Human APC2 maps to chromosome 19p13. 3. APC and APC2 may therefore have comparable functions in development and \n",
      "context_right: .\n",
      "type: SpecificDisease\n",
      "label_id: MESH:D009369\n",
      "label_title: Neoplasms\n",
      "mention_id: 10021369.5\n",
      "context_doc_id: 10021369\n",
      "label: None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle \n",
    "import json\n",
    "\n",
    "ontology = \"MEDIC\"\n",
    "model = \"arboel\"\n",
    "dataset = \"ncbi_disease\"\n",
    "abs_path = \"/home2/cye73/data_test\"\n",
    "data_path = os.path.join(abs_path, model, dataset)\n",
    "\n",
    "mentions = []\n",
    "\n",
    "with open(os.path.join(data_path, \"train.jsonl\"), 'r')  as read_handle :\n",
    "    for line in read_handle:\n",
    "        mentions.append(json.loads(line))\n",
    "\n",
    "for i in range(5) :\n",
    "    print(\"------\") \n",
    "    for key, value in mentions[i].items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST 2\n",
    "## Only this one works, rest is just testing !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_medic3(medic_dir):\n",
    "    \"This is needed because it's neither obo nor umls, it's a sort of preprocessing\"\n",
    "\n",
    "    key_dict = [\"DiseaseName\", \n",
    "                \"DiseaseID\", \n",
    "                \"AltDiseaseIDs\", \n",
    "                \"Definition\", \n",
    "                \"ParentIDs\", \n",
    "                \"TreeNumbers\", \n",
    "                \"ParentTreeNumbers\",\n",
    "                \"Synonyms\",\n",
    "                \"SlimMappings\"]\n",
    "    # Open the TSV file\n",
    "    with open(medic_dir, newline='') as tsvfile:\n",
    "        # Create a CSV reader specifying the delimiter as a tab character\n",
    "        reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "        \n",
    "        # Initialize a counter\n",
    "        counter = 0\n",
    "        \n",
    "        ontology = []\n",
    "        # Iterate over the rows in the file\n",
    "        for row in reader:\n",
    "            dict = {}\n",
    "            # Print the current row\n",
    "            if counter > 28 :\n",
    "                for i, elements in enumerate(row) :\n",
    "                    dict[key_dict[i]] = elements\n",
    "                \n",
    "                ontology.append(dict)\n",
    "            # Increment the counter\n",
    "            counter += 1\n",
    "    \n",
    "    return ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'DiseaseName': '18-Hydroxylase deficiency',\n",
       "  'DiseaseID': 'MESH:C537806',\n",
       "  'AltDiseaseIDs': 'OMIM:203400|OMIM:610600',\n",
       "  'Definition': '',\n",
       "  'ParentIDs': 'MESH:D006994',\n",
       "  'TreeNumbers': 'C19.053.500.480/C537806',\n",
       "  'ParentTreeNumbers': 'C19.053.500.480',\n",
       "  'Synonyms': '18-alpha hydroxylase deficiency|18-HYDROXYLASE DEFICIENCY|18-Oxidase Deficiency|Aldosterone deficiency 1|Aldosterone deficiency due to defect in 18-hydroxylase|ALDOSTERONE DEFICIENCY DUE TO DEFECT IN STEROID 18-HYDROXYLASE|ALDOSTERONE DEFICIENCY DUE TO DEFICIENCY OF STEROID 18-OXIDASE|ALDOSTERONE DEFICIENCY I|ALDOSTERONE DEFICIENCY II|Aldosterone Deficiency Type I|Aldosterone Deficiency Type II|CMO I Deficiency|CMO II Deficiency|Corticosterone methyloxidase type 1 deficiency|Corticosterone Methyloxidase Type I Deficiency|Corticosterone Methyloxidase Type II Deficiency|FHHA1A|FHHA1B|HYPERRENINEMIC HYPOALDOSTERONISM, FAMILIAL, 1|Hyperreninemic Hypoaldosteronism, Familial, Type I|Steroid 18-Hydroxylase Deficiency|Steroid 18-Oxidase Deficiency',\n",
       "  'SlimMappings': 'Endocrine system disease'},\n",
       " {'DiseaseName': '22q11 Deletion Syndrome',\n",
       "  'DiseaseID': 'MESH:D058165',\n",
       "  'AltDiseaseIDs': '',\n",
       "  'Definition': 'Condition with a variable constellation of phenotypes due to deletion polymorphisms at chromosome location 22q11. It encompasses several syndromes with overlapping abnormalities including the DIGEORGE SYNDROME, VELOCARDIOFACIAL SYNDROME, and CONOTRUNCAL AMOMALY FACE SYNDROME. In addition, variable developmental problems and schizoid features are also associated with this syndrome. (From BMC Med Genet. 2009 Feb 25;10:16) Not all deletions at 22q11 result in the 22q11deletion syndrome.',\n",
       "  'ParentIDs': 'MESH:D000015|MESH:D006330|MESH:D007011|MESH:D019465|MESH:D025063|MESH:D044148',\n",
       "  'TreeNumbers': 'C05.660.207.103|C14.240.400.021|C14.280.400.044|C15.604.451.249|C16.131.077.019|C16.131.240.400.021|C16.131.260.019|C16.131.482.249|C16.131.621.207.103|C16.320.180.019|C19.642.482.500',\n",
       "  'ParentTreeNumbers': 'C05.660.207|C14.240.400|C14.280.400|C15.604.451|C16.131.077|C16.131.240.400|C16.131.260|C16.131.482|C16.131.621.207|C16.320.180|C19.642.482',\n",
       "  'Synonyms': '22q11 Deletion Syndromes|Deletion Syndrome, 22q11|Deletion Syndromes, 22q11|Syndrome, 22q11 Deletion|Syndromes, 22q11 Deletion',\n",
       "  'SlimMappings': 'Cardiovascular disease|Congenital abnormality|Endocrine system disease|Genetic disease (inborn)|Lymphatic disease|Musculoskeletal disease'},\n",
       " {'DiseaseName': '2,4-Dienoyl-CoA Reductase Deficiency',\n",
       "  'DiseaseID': 'MESH:C565624',\n",
       "  'AltDiseaseIDs': 'OMIM:616034',\n",
       "  'Definition': '',\n",
       "  'ParentIDs': 'MESH:D008052',\n",
       "  'TreeNumbers': 'C16.320.565.398/C565624|C18.452.584.563/C565624|C18.452.648.398/C565624',\n",
       "  'ParentTreeNumbers': 'C16.320.565.398|C18.452.584.563|C18.452.648.398',\n",
       "  'Synonyms': 'DECRD',\n",
       "  'SlimMappings': 'Genetic disease (inborn)|Metabolic disease'}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ontology_dir = '/mitchell/entity-linking/kbs/medic.tsv'\n",
    "ontology = load_medic3(ontology_dir)\n",
    "[ontology[i] for i in range(6,9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_medic3_ontology(ontology, data_path, ontology_dir):\n",
    "    '''\n",
    "    This function prepares the entity data : dictionary.pickle\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    - ontology : str (only umls for now)\n",
    "        Ontology associated with the dataset\n",
    "    - data_path : str\n",
    "        Path where to load and save dictionary.pickle\n",
    "    - ontology_dir : str\n",
    "        Path to medic data\n",
    "    '''\n",
    "    \n",
    "    entities = load_medic3(ontology_dir)  # Assuming this returns a list of dictionaries like the provided example\n",
    "    \n",
    "    ontology_entities = []\n",
    "    for entity in tqdm(entities):\n",
    "        # Combining 'DiseaseID' and 'AltDiseaseIDs' into a single list, ensuring no duplicates\n",
    "        cui_list = [entity['DiseaseID']]\n",
    "        alt_ids = entity['AltDiseaseIDs'].split('|') if entity['AltDiseaseIDs'] else []\n",
    "        for alt_id in alt_ids:\n",
    "            if alt_id not in cui_list and alt_id[:2] != \"DO\":\n",
    "                cui_list.append(alt_id)\n",
    "                \n",
    "        if entity['Synonyms'] != \"\":\n",
    "            if entity['Definition'] != \"\":\n",
    "                new_entity = {\n",
    "                    'type': 'Disease',\n",
    "                    'cui': entity['DiseaseID'],\n",
    "                    'title': entity['DiseaseName'],\n",
    "                    'cuis': cui_list,\n",
    "                    'description': f\"{entity['DiseaseName']} ( Disease : {entity['Synonyms']} ) [{entity['Definition']}]\"\n",
    "                }\n",
    "            else : \n",
    "                new_entity = {\n",
    "                    'type': 'Disease',\n",
    "                    'cui': entity['DiseaseID'],\n",
    "                    'title': entity['DiseaseName'],\n",
    "                    'cuis': cui_list,\n",
    "                    'description': f\"{entity['DiseaseName']} ( Disease : {entity['Synonyms']} )\"\n",
    "                }\n",
    "                \n",
    "        else : \n",
    "            if entity['Definition'] != \"\":\n",
    "                new_entity = {\n",
    "                        'type': 'Disease',\n",
    "                        'cui': entity['DiseaseID'],\n",
    "                        'title': entity['DiseaseName'],\n",
    "                        'cuis': cui_list,\n",
    "                        'description': f\"{entity['DiseaseName']} ( Disease) [{entity['Definition']}]\"\n",
    "                    }\n",
    "            else : \n",
    "                new_entity = {\n",
    "                        'type': 'Disease',\n",
    "                        'cui': entity['DiseaseID'],\n",
    "                        'title': entity['DiseaseName'],\n",
    "                        'cuis': cui_list,\n",
    "                        'description': f\"{entity['DiseaseName']} ( Disease)\"\n",
    "                    }\n",
    "                \n",
    "            \n",
    "        ontology_entities.append(new_entity)\n",
    "\n",
    "    # Save entities to pickle file\n",
    "    with open(os.path.join(data_path, 'dictionary.pickle'), 'wb') as f:\n",
    "        pickle.dump(ontology_entities, f)\n",
    "        \n",
    "    return ontology_entities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home2/cye73/data/arboel/ncbi_disease\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 13189/13189 [00:00<00:00, 540458.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'type': 'Disease',\n",
       " 'cui': 'MESH:C538288',\n",
       " 'title': '10p Deletion Syndrome (Partial)',\n",
       " 'cuis': ['MESH:C538288'],\n",
       " 'description': '10p Deletion Syndrome (Partial) ( Disease : Chromosome 10, 10p- Partial|Chromosome 10, monosomy 10p|Chromosome 10, Partial Deletion (short arm)|Monosomy 10p )'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ontology_dir = '/mitchell/entity-linking/kbs/medic.tsv'\n",
    "ontology = \"MEDIC\"\n",
    "model = \"arboel\"\n",
    "dataset = \"ncbi_disease\"\n",
    "abs_path = \"/home2/cye73/data\"\n",
    "data_path = os.path.join(abs_path, model, dataset)\n",
    "print(data_path)\n",
    "\n",
    "entities = process_medic3_ontology(ontology = ontology,\n",
    "                                    data_path= data_path,\n",
    "                                    ontology_dir = ontology_dir, )\n",
    "\n",
    "entities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict :\n",
      " {'type': 'Disease', 'cui': 'MESH:C537806', 'title': '18-Hydroxylase deficiency', 'cuis': ['MESH:C537806', 'OMIM:203400', 'OMIM:610600'], 'description': '18-Hydroxylase deficiency ( Disease : 18-alpha hydroxylase deficiency|18-HYDROXYLASE DEFICIENCY|18-Oxidase Deficiency|Aldosterone deficiency 1|Aldosterone deficiency due to defect in 18-hydroxylase|ALDOSTERONE DEFICIENCY DUE TO DEFECT IN STEROID 18-HYDROXYLASE|ALDOSTERONE DEFICIENCY DUE TO DEFICIENCY OF STEROID 18-OXIDASE|ALDOSTERONE DEFICIENCY I|ALDOSTERONE DEFICIENCY II|Aldosterone Deficiency Type I|Aldosterone Deficiency Type II|CMO I Deficiency|CMO II Deficiency|Corticosterone methyloxidase type 1 deficiency|Corticosterone Methyloxidase Type I Deficiency|Corticosterone Methyloxidase Type II Deficiency|FHHA1A|FHHA1B|HYPERRENINEMIC HYPOALDOSTERONISM, FAMILIAL, 1|Hyperreninemic Hypoaldosteronism, Familial, Type I|Steroid 18-Hydroxylase Deficiency|Steroid 18-Oxidase Deficiency )'}\n",
      "dict2 :\n",
      " {'type': 'Disease', 'cui': 'MESH:C537806', 'title': '18-Hydroxylase deficiency', 'cuis': ['MESH:C537806', 'OMIM:203400', 'OMIM:610600'], 'description': '18-Hydroxylase deficiency ( Disease : 18-alpha hydroxylase deficiency ; 18-HYDROXYLASE DEFICIENCY ; 18-Oxidase Deficiency ; Aldosterone deficiency 1 ; Aldosterone deficiency due to defect in 18-hydroxylase ; ALDOSTERONE DEFICIENCY DUE TO DEFECT IN STEROID 18-HYDROXYLASE ; ALDOSTERONE DEFICIENCY DUE TO DEFICIENCY OF STEROID 18-OXIDASE ; ALDOSTERONE DEFICIENCY I ; ALDOSTERONE DEFICIENCY II ; Aldosterone Deficiency Type I ; Aldosterone Deficiency Type II ; CMO I Deficiency ; CMO II Deficiency ; Corticosterone methyloxidase type 1 deficiency ; Corticosterone Methyloxidase Type I Deficiency ; Corticosterone Methyloxidase Type II Deficiency ; FHHA1A ; FHHA1B ; HYPERRENINEMIC HYPOALDOSTERONISM, FAMILIAL, 1 ; Hyperreninemic Hypoaldosteronism, Familial, Type I ; Steroid 18-Hydroxylase Deficiency ; Steroid 18-Oxidase Deficiency )'}\n"
     ]
    }
   ],
   "source": [
    "#path_entity = '/home2/cye73/data_test2/arboel/ncbi_disease/dictionary.pickle'\n",
    "path_entity = f'/home2/cye73/data/arboel/{dataset}/dictionary.pickle'\n",
    "path_entity2 = f'/home2/cye73/arboEL2/data/arboel/{dataset}/dictionary.pickle'\n",
    "with open(path_entity, 'rb') as read_handle:\n",
    "    dict = pickle.load(read_handle)\n",
    "with open(path_entity2, 'rb') as read_handle:\n",
    "    dict2 = pickle.load(read_handle)\n",
    "\n",
    "print(\"dict :\\n\", dict[6])\n",
    "print(\"dict2 :\\n\", dict2[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dict2[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict2 :\n",
      " {'type': 'Disease', 'cui': 'MESH:C538288', 'title': '10p Deletion Syndrome (Partial)', 'cuis': ['MESH:C538288'], 'description': '10p Deletion Syndrome (Partial) ( Disease : Chromosome 10, 10p- Partial ; Chromosome 10, monosomy 10p ; Chromosome 10, Partial Deletion (short arm) ; Monosomy 10p )'}\n",
      "dict2 :\n",
      " {'type': 'Disease', 'cui': 'MESH:C535484', 'title': '13q deletion syndrome', 'cuis': ['MESH:C535484'], 'description': \"13q deletion syndrome ( Disease : Chromosome 13q deletion ; Chromosome 13q deletion syndrome ; Chromosome 13q monosomy ; Chromosome 13q syndrome ; Deletion 13q ; Deletion 13q syndrome ; Monosomy 13q ; Monosomy 13q syndrome ; Orbeli's syndrome ; Orbeli syndrome )\"}\n",
      "dict2 :\n",
      " {'type': 'Disease', 'cui': 'MESH:C579849', 'title': '15q24 Microdeletion', 'cuis': ['MESH:C579849'], 'description': '15q24 Microdeletion ( Disease : 15q24 Deletion ; 15q24 Microdeletion Syndrome ; Interstitial Deletion of Chromosome 15q24 )'}\n",
      "dict2 :\n",
      " {'type': 'Disease', 'cui': 'MESH:C579850', 'title': '16p11.2 Deletion Syndrome', 'cuis': ['MESH:C579850'], 'description': '16p11.2 Deletion Syndrome ( Disease :  )'}\n",
      "dict2 :\n",
      " {'type': 'Disease', 'cui': 'MESH:C567076', 'title': '17,20-Lyase Deficiency, Isolated', 'cuis': ['MESH:C567076'], 'description': '17,20-Lyase Deficiency, Isolated ( Disease : 17-Alpha-Hydroxylase-17,20-Lyase Deficiency, Combined Complete ; 17-Alpha-Hydroxylase-17,20-Lyase Deficiency, Combined Partial )'}\n",
      "dict2 :\n",
      " {'type': 'Disease', 'cui': 'MESH:C537805', 'title': '17-Hydroxysteroid Dehydrogenase Deficiency', 'cuis': ['MESH:C537805', 'OMIM:264300'], 'description': '17-Hydroxysteroid Dehydrogenase Deficiency ( Disease : 17 alpha ketosteroid reductase deficiency of testis ; 17-Beta Hydroxysteroid Dehydrogenase 3 Deficiency ; 17 Beta-hydroxysteroid dehydrogenase deficiency ; 17-Beta Hydroxysteroid Dehydrogenase III Deficiency ; 17-Ketosteroid Reductase Deficiency Of Testis ; 17-Ksr Deficiency ; Male pseudohermaphroditism with gynecomastia ; Neutral 17 beta-hydroxysteroid oxidoreductase deficiency ; Neutral 17-Beta-Hydroxysteroid Oxidoreductase Deficiency ; Pseudohermaphroditism, Male, with Gynecomastia ; PSEUDOHERMAPHRODITISM, MALE, WITH GYNECOMASTIA POLYCYSTIC OVARY SYNDROME DUE TO 17-KETOSTEROID REDUCTASE DEFICIENCY, INCLUDED )'}\n",
      "dict2 :\n",
      " {'type': 'Disease', 'cui': 'MESH:C537806', 'title': '18-Hydroxylase deficiency', 'cuis': ['MESH:C537806', 'OMIM:203400', 'OMIM:610600'], 'description': '18-Hydroxylase deficiency ( Disease : 18-alpha hydroxylase deficiency ; 18-HYDROXYLASE DEFICIENCY ; 18-Oxidase Deficiency ; Aldosterone deficiency 1 ; Aldosterone deficiency due to defect in 18-hydroxylase ; ALDOSTERONE DEFICIENCY DUE TO DEFECT IN STEROID 18-HYDROXYLASE ; ALDOSTERONE DEFICIENCY DUE TO DEFICIENCY OF STEROID 18-OXIDASE ; ALDOSTERONE DEFICIENCY I ; ALDOSTERONE DEFICIENCY II ; Aldosterone Deficiency Type I ; Aldosterone Deficiency Type II ; CMO I Deficiency ; CMO II Deficiency ; Corticosterone methyloxidase type 1 deficiency ; Corticosterone Methyloxidase Type I Deficiency ; Corticosterone Methyloxidase Type II Deficiency ; FHHA1A ; FHHA1B ; HYPERRENINEMIC HYPOALDOSTERONISM, FAMILIAL, 1 ; Hyperreninemic Hypoaldosteronism, Familial, Type I ; Steroid 18-Hydroxylase Deficiency ; Steroid 18-Oxidase Deficiency )'}\n",
      "dict2 :\n",
      " {'type': 'Disease', 'cui': 'MESH:D058165', 'title': '22q11 Deletion Syndrome', 'cuis': ['MESH:D058165'], 'description': '22q11 Deletion Syndrome ( Disease : 22q11 Deletion Syndromes ; Deletion Syndrome, 22q11 ; Deletion Syndromes, 22q11 ; Syndrome, 22q11 Deletion ; Syndromes, 22q11 Deletion ) [ Condition with a variable constellation of phenotypes due to deletion polymorphisms at chromosome location 22q11. It encompasses several syndromes with overlapping abnormalities including the DIGEORGE SYNDROME, VELOCARDIOFACIAL SYNDROME, and CONOTRUNCAL AMOMALY FACE SYNDROME. In addition, variable developmental problems and schizoid features are also associated with this syndrome. (From BMC Med Genet. 2009 Feb 25;10:16) Not all deletions at 22q11 result in the 22q11deletion syndrome. ]'}\n",
      "dict2 :\n",
      " {'type': 'Disease', 'cui': 'MESH:C565624', 'title': '2,4-Dienoyl-CoA Reductase Deficiency', 'cuis': ['MESH:C565624', 'OMIM:616034'], 'description': '2,4-Dienoyl-CoA Reductase Deficiency ( Disease : DECRD )'}\n",
      "dict2 :\n",
      " {'type': 'Disease', 'cui': 'MESH:C535305', 'title': '2-hydroxyethyl methacrylate sensitization', 'cuis': ['MESH:C535305'], 'description': '2-hydroxyethyl methacrylate sensitization ( Disease : Sensitization to 2-hydroxyethyl methacrylate )'}\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\"dict2 :\\n\", dict2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict :\n",
      " {'type': 'Disease', 'cui': 'MESH:C538288', 'title': '10p Deletion Syndrome (Partial)', 'cuis': ['MESH:C538288'], 'description': '10p Deletion Syndrome (Partial) ( Disease : Chromosome 10, 10p- Partial|Chromosome 10, monosomy 10p|Chromosome 10, Partial Deletion (short arm)|Monosomy 10p )'}\n",
      "dict :\n",
      " {'type': 'Disease', 'cui': 'MESH:C535484', 'title': '13q deletion syndrome', 'cuis': ['MESH:C535484'], 'description': \"13q deletion syndrome ( Disease : Chromosome 13q deletion|Chromosome 13q deletion syndrome|Chromosome 13q monosomy|Chromosome 13q syndrome|Deletion 13q|Deletion 13q syndrome|Monosomy 13q|Monosomy 13q syndrome|Orbeli's syndrome|Orbeli syndrome )\"}\n",
      "dict :\n",
      " {'type': 'Disease', 'cui': 'MESH:C579849', 'title': '15q24 Microdeletion', 'cuis': ['MESH:C579849'], 'description': '15q24 Microdeletion ( Disease : 15q24 Deletion|15q24 Microdeletion Syndrome|Interstitial Deletion of Chromosome 15q24 )'}\n",
      "dict :\n",
      " {'type': 'Disease', 'cui': 'MESH:C579850', 'title': '16p11.2 Deletion Syndrome', 'cuis': ['MESH:C579850'], 'description': '16p11.2 Deletion Syndrome ( Disease)'}\n",
      "dict :\n",
      " {'type': 'Disease', 'cui': 'MESH:C567076', 'title': '17,20-Lyase Deficiency, Isolated', 'cuis': ['MESH:C567076'], 'description': '17,20-Lyase Deficiency, Isolated ( Disease : 17-Alpha-Hydroxylase-17,20-Lyase Deficiency, Combined Complete|17-Alpha-Hydroxylase-17,20-Lyase Deficiency, Combined Partial )'}\n",
      "dict :\n",
      " {'type': 'Disease', 'cui': 'MESH:C537805', 'title': '17-Hydroxysteroid Dehydrogenase Deficiency', 'cuis': ['MESH:C537805', 'OMIM:264300'], 'description': '17-Hydroxysteroid Dehydrogenase Deficiency ( Disease : 17 alpha ketosteroid reductase deficiency of testis|17-Beta Hydroxysteroid Dehydrogenase 3 Deficiency|17 Beta-hydroxysteroid dehydrogenase deficiency|17-Beta Hydroxysteroid Dehydrogenase III Deficiency|17-Ketosteroid Reductase Deficiency Of Testis|17-Ksr Deficiency|Male pseudohermaphroditism with gynecomastia|Neutral 17 beta-hydroxysteroid oxidoreductase deficiency|Neutral 17-Beta-Hydroxysteroid Oxidoreductase Deficiency|Pseudohermaphroditism, Male, with Gynecomastia|PSEUDOHERMAPHRODITISM, MALE, WITH GYNECOMASTIA POLYCYSTIC OVARY SYNDROME DUE TO 17-KETOSTEROID REDUCTASE DEFICIENCY, INCLUDED )'}\n",
      "dict :\n",
      " {'type': 'Disease', 'cui': 'MESH:C537806', 'title': '18-Hydroxylase deficiency', 'cuis': ['MESH:C537806', 'OMIM:203400', 'OMIM:610600'], 'description': '18-Hydroxylase deficiency ( Disease : 18-alpha hydroxylase deficiency|18-HYDROXYLASE DEFICIENCY|18-Oxidase Deficiency|Aldosterone deficiency 1|Aldosterone deficiency due to defect in 18-hydroxylase|ALDOSTERONE DEFICIENCY DUE TO DEFECT IN STEROID 18-HYDROXYLASE|ALDOSTERONE DEFICIENCY DUE TO DEFICIENCY OF STEROID 18-OXIDASE|ALDOSTERONE DEFICIENCY I|ALDOSTERONE DEFICIENCY II|Aldosterone Deficiency Type I|Aldosterone Deficiency Type II|CMO I Deficiency|CMO II Deficiency|Corticosterone methyloxidase type 1 deficiency|Corticosterone Methyloxidase Type I Deficiency|Corticosterone Methyloxidase Type II Deficiency|FHHA1A|FHHA1B|HYPERRENINEMIC HYPOALDOSTERONISM, FAMILIAL, 1|Hyperreninemic Hypoaldosteronism, Familial, Type I|Steroid 18-Hydroxylase Deficiency|Steroid 18-Oxidase Deficiency )'}\n",
      "dict :\n",
      " {'type': 'Disease', 'cui': 'MESH:D058165', 'title': '22q11 Deletion Syndrome', 'cuis': ['MESH:D058165'], 'description': '22q11 Deletion Syndrome ( Disease : 22q11 Deletion Syndromes|Deletion Syndrome, 22q11|Deletion Syndromes, 22q11|Syndrome, 22q11 Deletion|Syndromes, 22q11 Deletion ) [Condition with a variable constellation of phenotypes due to deletion polymorphisms at chromosome location 22q11. It encompasses several syndromes with overlapping abnormalities including the DIGEORGE SYNDROME, VELOCARDIOFACIAL SYNDROME, and CONOTRUNCAL AMOMALY FACE SYNDROME. In addition, variable developmental problems and schizoid features are also associated with this syndrome. (From BMC Med Genet. 2009 Feb 25;10:16) Not all deletions at 22q11 result in the 22q11deletion syndrome.]'}\n",
      "dict :\n",
      " {'type': 'Disease', 'cui': 'MESH:C565624', 'title': '2,4-Dienoyl-CoA Reductase Deficiency', 'cuis': ['MESH:C565624', 'OMIM:616034'], 'description': '2,4-Dienoyl-CoA Reductase Deficiency ( Disease : DECRD )'}\n",
      "dict :\n",
      " {'type': 'Disease', 'cui': 'MESH:C535305', 'title': '2-hydroxyethyl methacrylate sensitization', 'cuis': ['MESH:C535305'], 'description': '2-hydroxyethyl methacrylate sensitization ( Disease : Sensitization to 2-hydroxyethyl methacrylate )'}\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\"dict :\\n\", dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_mention3_dataset(ontology,\n",
    "                            dataset,\n",
    "                            data_path,\n",
    "                            ontology_type,\n",
    "                            ontology_dir: Optional[str] = None,\n",
    "                            mention_id: Optional[bool] = True,\n",
    "                            context_doc_id: Optional[bool] = True,\n",
    "                            label: Optional[bool] = True\n",
    "                            ): \n",
    "    '''\n",
    "    This function prepares the mentions data :  Creates the train.jsonl, valid.jsonl, test.jsonl\n",
    "    Each .jsonl contains data in the following format : \n",
    "    {'mention': mention, \n",
    "    'mention_id': ID of the mention, (optional)\n",
    "    'context_left': context before mention,\n",
    "    'context_right': context after mention, \n",
    "    'context_doc_id': ID of the doc, (optional)\n",
    "    'type': type\n",
    "    'label_id': label_id,\n",
    "    'label': entity description, (optional)\n",
    "    'label_title': entity title\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    - ontology : str (only umls for now)\n",
    "    Ontology associated with the dataset\n",
    "    - dataset : str\n",
    "    Name of the dataset\n",
    "    - data_path : str\n",
    "    Path where to load and save dictionary.pickle\n",
    "    - ontology_type : str\n",
    "    'obo' or 'umls' and possibly others\n",
    "    - umls_dir : str\n",
    "    Path to the ontology (umls, medic etc...)\n",
    "    '''\n",
    "    data = conhelps.for_config_name(f'{dataset}_bigbio_kb').load_dataset()\n",
    "    exclude = CUIS_TO_EXCLUDE[dataset]\n",
    "    remap = CUIS_TO_REMAP[dataset]\n",
    "\n",
    "    'If dictionary already processed, load it else process and load it'\n",
    "    entity_dictionary_pkl_path = os.path.join(data_path, 'dictionary.pickle')\n",
    "    \n",
    "    if os.path.isfile(entity_dictionary_pkl_path): \n",
    "        print(\"Loading stored processed entity dictionary...\")\n",
    "        with open(entity_dictionary_pkl_path, 'rb') as read_handle:\n",
    "            entities = pickle.load(read_handle)\n",
    "    else :\n",
    "        if ontology_type == \"obo\" :\n",
    "            entities = process_obo_ontology(ontology, data_path)\n",
    "        elif ontology_type == \"medic\" : \n",
    "            entities = process_medic3_ontology(ontology, data_path, ontology_dir)\n",
    "        elif ontology_type == \"umls\" : \n",
    "            entities = process_umls_ontology(ontology, data_path, ontology_dir)\n",
    "        else : \n",
    "            print(\"ERROR!\")\n",
    "        \n",
    "    entity_dictionary = {d['cui']:d for d in tqdm(entities)} #CC1\n",
    "\n",
    "    if dataset == 'ncbi_disease': #CC2\n",
    "        # Need to redo this since we have multiple synonymous CUIs for ncbi_disease\n",
    "        entity_dictionary = {cui:d for d in tqdm(entities) for cui in d['cuis']}\n",
    "        cui_synsets = {}\n",
    "        for subdict in tqdm(entities): \n",
    "            for cui in subdict['cuis']:\n",
    "                if cui in subdict:\n",
    "                    print(cui, cui_synsets[cui], subdict['cuis'])\n",
    "                cui_synsets[cui] = subdict['cuis'] \n",
    "        with open(os.path.join(data_path, 'cui_synsets.json'), 'w') as f:\n",
    "            f.write(ujson.dumps(cui_synsets, indent=2))\n",
    "\n",
    "    if dataset in VALIDATION_DOCUMENT_IDS:\n",
    "        validation_pmids = VALIDATION_DOCUMENT_IDS[dataset]\n",
    "    else:\n",
    "        print(\"ERROR!!!\")\n",
    "        \n",
    "    # Convert BigBio dataset to pandas DataFrame\n",
    "    df = dataset_to_df(data, entity_remapping_dict=remap, cuis_to_exclude=exclude, val_split_ids=validation_pmids)\n",
    "    # Return dictionary of documents in BigBio dataset\n",
    "    docs = dataset_to_documents(data)\n",
    "    label_len = df['db_ids'].map(lambda x: len(x)).max()\n",
    "    print(\"Max labels on one doc:\", label_len)\n",
    "\n",
    "    for split in df.split.unique():\n",
    "        print(split)\n",
    "\n",
    "        ents_in_split = []\n",
    "        for d in tqdm(df.query(\"split == @split\").to_dict(orient='records'),\n",
    "                      desc=f\"Creating correct mention format for {split} dataset\"):\n",
    "            abbrev_resolved = False\n",
    "            offsets = d['offsets']\n",
    "            doc_id = d['document_id']\n",
    "            doc = docs[doc_id]\n",
    "            mention = d['text']\n",
    "            \n",
    "            # Get offsets and context\n",
    "            start = offsets[0][0] # start on the mention\n",
    "            end = offsets[-1][-1] # end of the mention\n",
    "            before_context = doc[:start] # left context\n",
    "            after_context = doc[end:] # right context\n",
    "            \n",
    "            \n",
    "            # ArboEL can't handle multi-labels, so we randomly choose one.\n",
    "            if len(d['db_ids']) == 1:\n",
    "                label_id = d['db_ids'][0]\n",
    "\n",
    "            # ncbi_disease is a special case that requires extra care\n",
    "            elif dataset == 'ncbi_disease':\n",
    "                labels = []\n",
    "                used_cuis = set([])\n",
    "                choosable_ids = []\n",
    "                for db_id in d['db_ids']:\n",
    "                    if db_id in used_cuis:\n",
    "                        continue\n",
    "                    else:\n",
    "                        used_cuis.update(set(entity_dictionary[db_id]['cuis']))\n",
    "                    choosable_ids.append(db_id)\n",
    "\n",
    "                label_id = np.random.choice(choosable_ids)\n",
    "            \n",
    "            else:\n",
    "                label_id = np.random.choice(d['db_ids'])\n",
    "\n",
    "            # Check if we missed something\n",
    "            if label_id not in entity_dictionary:\n",
    "                print(label_id)\n",
    "                continue\n",
    "            \n",
    "\n",
    "            \n",
    "            output = [\n",
    "                {\n",
    "                    \"mention\": mention,\n",
    "                    \"context_left\": before_context,\n",
    "                    \"context_right\": after_context,\n",
    "                    \"type\": d[\"type\"][0],\n",
    "                    \"label_id\": label_id,\n",
    "                    \"label_title\": entity_dictionary[label_id][\"title\"],\n",
    "                    \"label\": entity_dictionary[label_id][\"description\"],\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            if mention_id:\n",
    "                output[0][\"mention_id\"] = d.get(\"mention_id\", None)\n",
    "\n",
    "            if context_doc_id:\n",
    "                if ontology == \"medic\":\n",
    "                    output[0][\"context_doc_id\"] = d.get(\"document_id\", None)\n",
    "                else:\n",
    "                    output[0][\"context_doc_id\"] = d.get(\"context_doc_id\", None)\n",
    "\n",
    "            ents_in_split.extend(output)\n",
    "\n",
    "        split_name = split\n",
    "        if split =='validation':\n",
    "            split_name = 'valid'\n",
    "        with open(os.path.join(data_path, f'{split_name}.jsonl'), 'w') as f:\n",
    "            f.write('\\n'.join([ujson.dumps(x) for x in ents_in_split]))\n",
    "            \n",
    "    return output\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home2/cye73/data/arboel/ncbi_disease\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:1454: FutureWarning: The repository for bigbio/ncbi_disease contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/bigbio/ncbi_disease\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stored processed entity dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 13189/13189 [00:00<00:00, 2804495.59it/s]\n",
      "100%|| 13189/13189 [00:00<00:00, 2448704.15it/s]\n",
      "100%|| 13189/13189 [00:00<00:00, 2200512.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max labels on one doc: 5\n",
      "['train' 'validation' 'test']\n",
      "train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating correct mention format for train dataset: 100%|| 5065/5065 [00:00<00:00, 318545.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating correct mention format for validation dataset: 100%|| 780/780 [00:00<00:00, 255610.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating correct mention format for test dataset: 100%|| 960/960 [00:00<00:00, 359383.42it/s]\n"
     ]
    }
   ],
   "source": [
    "ontology = \"medic\"\n",
    "model = \"arboel\"\n",
    "dataset = \"ncbi_disease\"\n",
    "abs_path = \"/home2/cye73/data\"\n",
    "data_path = os.path.join(abs_path, model, dataset)\n",
    "print(data_path)\n",
    "\n",
    "ontology_type = \"medic\"\n",
    "ontology_dir = '/mitchell/entity-linking/kbs/medic.tsv'\n",
    "# ontology_dir=\"/mitchell/entity-linking/2017AA/META/\"\n",
    "\n",
    "# data_path = f'/home2/cye73/arboEL2/data/arboel/{dataset}'\n",
    "\n",
    "entity_dictionary = process_mention3_dataset(ontology = ontology,\n",
    "                        dataset = dataset,\n",
    "                        data_path = data_path,\n",
    "                        ontology_type = ontology_type,\n",
    "                        ontology_dir = ontology_dir, \n",
    "                        mention_id = True,\n",
    "                        context_doc_id = True,\n",
    "                        label = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "mention: adenomatous polyposis coli tumour\n",
      "context_left: Identification of APC2, a homologue of the \n",
      "context_right:  suppressor.\n",
      "The adenomatous polyposis coli (APC) tumour-suppressor protein controls the Wnt signalling pathway by forming a complex with glycogen synthase kinase 3beta (GSK-3beta), axin/conductin and betacatenin. Complex formation induces the rapid degradation of betacatenin. In colon carcinoma cells, loss of APC leads to the accumulation of betacatenin in the nucleus, where it binds to and activates the Tcf-4 transcription factor (reviewed in [1] [2]). Here, we report the identification and genomic structure of APC homologues. Mammalian APC2, which closely resembles APC in overall domain structure, was functionally analyzed and shown to contain two SAMP domains, both of which are required for binding to conductin. Like APC, APC2 regulates the formation of active betacatenin-Tcf complexes, as demonstrated using transient transcriptional activation assays in APC -/- colon carcinoma cells. Human APC2 maps to chromosome 19p13. 3. APC and APC2 may therefore have comparable functions in development and cancer.\n",
      "type: Modifier\n",
      "label_id: MESH:D011125\n",
      "label_title: Adenomatous Polyposis Coli\n",
      "label: Adenomatous Polyposis Coli ( Disease : AAPC, INCLUDED|ADENOMA, PERIAMPULLARY, SOMATIC, INCLUDED|Adenomatous Intestinal Polyposes|Adenomatous Intestinal Polyposis|Adenomatous Polyposes, Familial|ADENOMATOUS POLYPOSIS COLI, ATTENUATED, INCLUDED|Adenomatous Polyposis Coli, Familial|Adenomatous Polyposis Colus|Adenomatous Polyposis, Familial|Adenomatous Polyposis of the Colon|AFAP, INCLUDED|APC|BRAIN TUMOR-POLYPOSIS SYNDROME 2, INCLUDED|BTPS2, INCLUDED|Coli, Adenomatous Polyposis|Coli, Familial Polyposis|Coli, Hereditary Polyposis|Coli, Polyposis|Colus, Adenomatous Polyposis|Colus, Familial Polyposis|Colus, Hereditary Polyposis|Colus, Polyposis|Familial Adenomatous Polyposes|Familial Adenomatous Polyposis|FAMILIAL ADENOMATOUS POLYPOSIS 1|FAMILIAL ADENOMATOUS POLYPOSIS 3|FAMILIAL ADENOMATOUS POLYPOSIS 4|FAMILIAL ADENOMATOUS POLYPOSIS, ATTENUATED, INCLUDED|Familial Adenomatous Polyposis Coli|Familial Adenomatous Polyposis of the Colon|Familial Intestinal Polyposes|Familial Intestinal Polyposis|Familial Multiple Polyposes|Familial Multiple Polyposi|Familial Multiple Polyposis|Familial Multiple Polyposis Syndrome|Familial Multiple Polyposus|Familial Polyposis Coli|Familial Polyposis Colus|Familial Polyposis of the Colon|Familial Polyposis Syndrome|Familial Polyposis Syndromes|FAP1|FAP3|FAP4|FPC|GS, INCLUDED|Hereditary Polyposis Coli|Hereditary Polyposis Colus|Intestinal Polyposes, Familial|Intestinal Polyposis, Adenomatous|Intestinal Polyposis, Familial|Multiple Polyposes, Familial|Multiple Polyposi, Familial|Multiple Polyposis, Familial|Multiple Polyposus, Familial|Myh-Associated Polyposes|Myh Associated Polyposis|Myh-Associated Polyposis|Polyposes, Familial Adenomatous|Polyposes, Familial Multiple|Polyposes, Myh-Associated|Polyposi, Familial Multiple|Polyposis, Adenomatous Intestinal|POLYPOSIS, ADENOMATOUS INTESTINAL GARDNER SYNDROME, INCLUDED|Polyposis Coli|Polyposis Coli, Adenomatous|Polyposis Coli, Familial|Polyposis Coli, Hereditary|Polyposis Colus|Polyposis Colus, Adenomatous|Polyposis Colus, Familial|Polyposis Colus, Hereditary|Polyposis, Familial Adenomatous|Polyposis, Familial Multiple|Polyposis, Myh-Associated|Polyposis Syndrome, Familial|Polyposus, Familial Multiple ) [A polyposis syndrome due to an autosomal dominant mutation of the APC genes (GENES, APC) on CHROMOSOME 5. The syndrome is characterized by the development of hundreds of ADENOMATOUS POLYPS in the COLON and RECTUM of affected individuals by early adulthood.]\n",
      "mention_id: 10021369.1\n",
      "context_doc_id: 10021369\n",
      "------\n",
      "mention: adenomatous polyposis coli (APC) tumour\n",
      "context_left: Identification of APC2, a homologue of the adenomatous polyposis coli tumour suppressor.\n",
      "The \n",
      "context_right: -suppressor protein controls the Wnt signalling pathway by forming a complex with glycogen synthase kinase 3beta (GSK-3beta), axin/conductin and betacatenin. Complex formation induces the rapid degradation of betacatenin. In colon carcinoma cells, loss of APC leads to the accumulation of betacatenin in the nucleus, where it binds to and activates the Tcf-4 transcription factor (reviewed in [1] [2]). Here, we report the identification and genomic structure of APC homologues. Mammalian APC2, which closely resembles APC in overall domain structure, was functionally analyzed and shown to contain two SAMP domains, both of which are required for binding to conductin. Like APC, APC2 regulates the formation of active betacatenin-Tcf complexes, as demonstrated using transient transcriptional activation assays in APC -/- colon carcinoma cells. Human APC2 maps to chromosome 19p13. 3. APC and APC2 may therefore have comparable functions in development and cancer.\n",
      "type: Modifier\n",
      "label_id: MESH:D011125\n",
      "label_title: Adenomatous Polyposis Coli\n",
      "label: Adenomatous Polyposis Coli ( Disease : AAPC, INCLUDED|ADENOMA, PERIAMPULLARY, SOMATIC, INCLUDED|Adenomatous Intestinal Polyposes|Adenomatous Intestinal Polyposis|Adenomatous Polyposes, Familial|ADENOMATOUS POLYPOSIS COLI, ATTENUATED, INCLUDED|Adenomatous Polyposis Coli, Familial|Adenomatous Polyposis Colus|Adenomatous Polyposis, Familial|Adenomatous Polyposis of the Colon|AFAP, INCLUDED|APC|BRAIN TUMOR-POLYPOSIS SYNDROME 2, INCLUDED|BTPS2, INCLUDED|Coli, Adenomatous Polyposis|Coli, Familial Polyposis|Coli, Hereditary Polyposis|Coli, Polyposis|Colus, Adenomatous Polyposis|Colus, Familial Polyposis|Colus, Hereditary Polyposis|Colus, Polyposis|Familial Adenomatous Polyposes|Familial Adenomatous Polyposis|FAMILIAL ADENOMATOUS POLYPOSIS 1|FAMILIAL ADENOMATOUS POLYPOSIS 3|FAMILIAL ADENOMATOUS POLYPOSIS 4|FAMILIAL ADENOMATOUS POLYPOSIS, ATTENUATED, INCLUDED|Familial Adenomatous Polyposis Coli|Familial Adenomatous Polyposis of the Colon|Familial Intestinal Polyposes|Familial Intestinal Polyposis|Familial Multiple Polyposes|Familial Multiple Polyposi|Familial Multiple Polyposis|Familial Multiple Polyposis Syndrome|Familial Multiple Polyposus|Familial Polyposis Coli|Familial Polyposis Colus|Familial Polyposis of the Colon|Familial Polyposis Syndrome|Familial Polyposis Syndromes|FAP1|FAP3|FAP4|FPC|GS, INCLUDED|Hereditary Polyposis Coli|Hereditary Polyposis Colus|Intestinal Polyposes, Familial|Intestinal Polyposis, Adenomatous|Intestinal Polyposis, Familial|Multiple Polyposes, Familial|Multiple Polyposi, Familial|Multiple Polyposis, Familial|Multiple Polyposus, Familial|Myh-Associated Polyposes|Myh Associated Polyposis|Myh-Associated Polyposis|Polyposes, Familial Adenomatous|Polyposes, Familial Multiple|Polyposes, Myh-Associated|Polyposi, Familial Multiple|Polyposis, Adenomatous Intestinal|POLYPOSIS, ADENOMATOUS INTESTINAL GARDNER SYNDROME, INCLUDED|Polyposis Coli|Polyposis Coli, Adenomatous|Polyposis Coli, Familial|Polyposis Coli, Hereditary|Polyposis Colus|Polyposis Colus, Adenomatous|Polyposis Colus, Familial|Polyposis Colus, Hereditary|Polyposis, Familial Adenomatous|Polyposis, Familial Multiple|Polyposis, Myh-Associated|Polyposis Syndrome, Familial|Polyposus, Familial Multiple ) [A polyposis syndrome due to an autosomal dominant mutation of the APC genes (GENES, APC) on CHROMOSOME 5. The syndrome is characterized by the development of hundreds of ADENOMATOUS POLYPS in the COLON and RECTUM of affected individuals by early adulthood.]\n",
      "mention_id: 10021369.2\n",
      "context_doc_id: 10021369\n",
      "------\n",
      "mention: colon carcinoma\n",
      "context_left: Identification of APC2, a homologue of the adenomatous polyposis coli tumour suppressor.\n",
      "The adenomatous polyposis coli (APC) tumour-suppressor protein controls the Wnt signalling pathway by forming a complex with glycogen synthase kinase 3beta (GSK-3beta), axin/conductin and betacatenin. Complex formation induces the rapid degradation of betacatenin. In \n",
      "context_right:  cells, loss of APC leads to the accumulation of betacatenin in the nucleus, where it binds to and activates the Tcf-4 transcription factor (reviewed in [1] [2]). Here, we report the identification and genomic structure of APC homologues. Mammalian APC2, which closely resembles APC in overall domain structure, was functionally analyzed and shown to contain two SAMP domains, both of which are required for binding to conductin. Like APC, APC2 regulates the formation of active betacatenin-Tcf complexes, as demonstrated using transient transcriptional activation assays in APC -/- colon carcinoma cells. Human APC2 maps to chromosome 19p13. 3. APC and APC2 may therefore have comparable functions in development and cancer.\n",
      "type: Modifier\n",
      "label_id: MESH:D003110\n",
      "label_title: Colonic Neoplasms\n",
      "label: Colonic Neoplasms ( Disease : Adenocarcinoma, Colon|Adenocarcinomas, Colon|Cancer, Colon|Cancer, Colonic|Cancer of Colon|Cancer of the Colon|Cancers, Colon|Cancers, Colonic|Colon Adenocarcinoma|Colon Adenocarcinomas|Colon Cancer|Colon Cancers|Colonic Cancer|Colonic Cancers|Colonic Neoplasm|Colon Neoplasm|Colon Neoplasms|Neoplasm, Colon|Neoplasm, Colonic|Neoplasms, Colon|Neoplasms, Colonic ) [Tumors or cancer of the COLON.]\n",
      "mention_id: 10021369.3\n",
      "context_doc_id: 10021369\n",
      "------\n",
      "mention: colon carcinoma\n",
      "context_left: Identification of APC2, a homologue of the adenomatous polyposis coli tumour suppressor.\n",
      "The adenomatous polyposis coli (APC) tumour-suppressor protein controls the Wnt signalling pathway by forming a complex with glycogen synthase kinase 3beta (GSK-3beta), axin/conductin and betacatenin. Complex formation induces the rapid degradation of betacatenin. In colon carcinoma cells, loss of APC leads to the accumulation of betacatenin in the nucleus, where it binds to and activates the Tcf-4 transcription factor (reviewed in [1] [2]). Here, we report the identification and genomic structure of APC homologues. Mammalian APC2, which closely resembles APC in overall domain structure, was functionally analyzed and shown to contain two SAMP domains, both of which are required for binding to conductin. Like APC, APC2 regulates the formation of active betacatenin-Tcf complexes, as demonstrated using transient transcriptional activation assays in APC -/- \n",
      "context_right:  cells. Human APC2 maps to chromosome 19p13. 3. APC and APC2 may therefore have comparable functions in development and cancer.\n",
      "type: Modifier\n",
      "label_id: MESH:D003110\n",
      "label_title: Colonic Neoplasms\n",
      "label: Colonic Neoplasms ( Disease : Adenocarcinoma, Colon|Adenocarcinomas, Colon|Cancer, Colon|Cancer, Colonic|Cancer of Colon|Cancer of the Colon|Cancers, Colon|Cancers, Colonic|Colon Adenocarcinoma|Colon Adenocarcinomas|Colon Cancer|Colon Cancers|Colonic Cancer|Colonic Cancers|Colonic Neoplasm|Colon Neoplasm|Colon Neoplasms|Neoplasm, Colon|Neoplasm, Colonic|Neoplasms, Colon|Neoplasms, Colonic ) [Tumors or cancer of the COLON.]\n",
      "mention_id: 10021369.4\n",
      "context_doc_id: 10021369\n",
      "------\n",
      "mention: cancer\n",
      "context_left: Identification of APC2, a homologue of the adenomatous polyposis coli tumour suppressor.\n",
      "The adenomatous polyposis coli (APC) tumour-suppressor protein controls the Wnt signalling pathway by forming a complex with glycogen synthase kinase 3beta (GSK-3beta), axin/conductin and betacatenin. Complex formation induces the rapid degradation of betacatenin. In colon carcinoma cells, loss of APC leads to the accumulation of betacatenin in the nucleus, where it binds to and activates the Tcf-4 transcription factor (reviewed in [1] [2]). Here, we report the identification and genomic structure of APC homologues. Mammalian APC2, which closely resembles APC in overall domain structure, was functionally analyzed and shown to contain two SAMP domains, both of which are required for binding to conductin. Like APC, APC2 regulates the formation of active betacatenin-Tcf complexes, as demonstrated using transient transcriptional activation assays in APC -/- colon carcinoma cells. Human APC2 maps to chromosome 19p13. 3. APC and APC2 may therefore have comparable functions in development and \n",
      "context_right: .\n",
      "type: SpecificDisease\n",
      "label_id: MESH:D009369\n",
      "label_title: Neoplasms\n",
      "label: Neoplasms ( Disease : Benign Neoplasm|Benign Neoplasms|Cancer|Cancers|Malignancies|Malignancy|Malignant Neoplasm|Malignant Neoplasms|Neoplasia|Neoplasias|Neoplasm|Neoplasm, Benign|Neoplasm, Malignant|Neoplasms, Benign|Neoplasms, Malignant|Tumor|Tumors ) [New abnormal growth of tissue. Malignant neoplasms show a greater degree of anaplasia and have the properties of invasion and metastasis, compared to benign neoplasms.]\n",
      "mention_id: 10021369.5\n",
      "context_doc_id: 10021369\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle \n",
    "import json\n",
    "\n",
    "ontology = \"MEDIC\"\n",
    "model = \"arboel\"\n",
    "dataset = \"ncbi_disease\"\n",
    "abs_path = \"/home2/cye73/data\"\n",
    "data_path = os.path.join(abs_path, model, dataset)\n",
    "\n",
    "mentions = []\n",
    "\n",
    "with open(os.path.join(data_path, \"train.jsonl\"), 'r')  as read_handle :\n",
    "    for line in read_handle:\n",
    "        mentions.append(json.loads(line))\n",
    "\n",
    "for i in range(5) :\n",
    "    print(\"------\") \n",
    "    for key, value in mentions[i].items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST 3\n",
    "### Use load_medic from ontology.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology_dir = '/mitchell/entity-linking/kbs/medic.tsv'\n",
    "# ontology = \"MEDIC\"\n",
    "model = \"arboel\"\n",
    "dataset = \"ncbi_disease\"\n",
    "abs_path = \"/home2/cye73/data\"\n",
    "data_path = os.path.join(abs_path, model, dataset)\n",
    "print(data_path)\n",
    "\n",
    "ontology = BiomedicalOntology(name=\"MEDIC\")\n",
    "ontology.load_medic(path = ontology_dir)\n",
    "\n",
    "for entity in ontology.entities : \n",
    "    print(entity.cui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_medic4_ontology(ontology, data_path, ontology_dir):\n",
    "    '''\n",
    "    This function prepares the entity data : dictionary.pickle\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    - ontology : str (only umls for now)\n",
    "        Ontology associated with the dataset\n",
    "    - data_path : str\n",
    "        Path where to load and save dictionary.pickle\n",
    "    - ontology_dir : str\n",
    "        Path to medic ontology\n",
    "    '''\n",
    "    \n",
    "    ontology = BiomedicalOntology(name=\"MEDIC\")\n",
    "    ontology.load_medic(path = ontology_dir)\n",
    "    \n",
    "    equivalent_cuis = False\n",
    "    if ontology.entities[0].equivalant_cuis is not None : \n",
    "        equivalent_cuis = True\n",
    "        \n",
    "    'If dictionary already processed, load it else process and load it'\n",
    "    entity_dictionary_pkl_path = os.path.join(data_path, 'dictionary.pickle')\n",
    "    \n",
    "    if os.path.isfile(entity_dictionary_pkl_path): \n",
    "        print(\"Loading stored processed entity dictionary...\")\n",
    "        with open(entity_dictionary_pkl_path, 'rb') as read_handle:\n",
    "            entities = pickle.load(read_handle)\n",
    "        \n",
    "        return entities, equivalent_cuis\n",
    "    \n",
    "    ontology_entities = []\n",
    "    for entity in tqdm(ontology.entities):      \n",
    "        if entity.aliases != \"\":\n",
    "            if entity.definition != \"\":\n",
    "                new_entity = {\n",
    "                    'type': 'Disease',\n",
    "                    'cui': entity.cui,\n",
    "                    'title': entity.name,\n",
    "                    'cuis': entity.equivalant_cuis,\n",
    "                    'description': f\"{entity.name} ( Disease : {entity.aliases} ) [{entity.definition}]\"\n",
    "                }\n",
    "            else : \n",
    "                new_entity = {\n",
    "                    'type': 'Disease',\n",
    "                    'cui': entity.cui,\n",
    "                    'title': entity.name,\n",
    "                    'cuis': entity.equivalant_cuis,\n",
    "                    'description': f\"{entity.name} ( Disease : {entity.aliases} )\"\n",
    "                }\n",
    "                \n",
    "        else : \n",
    "            if entity.definition != \"\":\n",
    "                new_entity = {\n",
    "                        'type': 'Disease',\n",
    "                        'cui': entity.cui,\n",
    "                        'title': entity.name,\n",
    "                        'cuis': entity.equivalant_cuis,\n",
    "                        'description': f\"{entity.name} ( Disease) [{entity.definition}]\"\n",
    "                    }\n",
    "            else : \n",
    "                new_entity = {\n",
    "                        'type': 'Disease',\n",
    "                        'cui': entity.cui,\n",
    "                        'title': entity.name,\n",
    "                        'cuis': entity.equivalant_cuis,\n",
    "                        'description': f\"{entity.name} ( Disease)\"\n",
    "                    }\n",
    "        ontology_entities.append(new_entity)\n",
    "    \n",
    "    # Check if the directory exists, and create it if it does not\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "\n",
    "    # Save entities to pickle file\n",
    "    with open(os.path.join(data_path, 'dictionary.pickle'), 'wb') as f:\n",
    "        pickle.dump(ontology_entities, f)\n",
    "        \n",
    "    return ontology_entities, equivalent_cuis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 13189/13189 [00:00<00:00, 769212.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'type': 'Disease',\n",
       " 'cui': 'MESH:C538288',\n",
       " 'title': '10p Deletion Syndrome (Partial)',\n",
       " 'cuis': ['MESH:C538288'],\n",
       " 'description': '10p Deletion Syndrome (Partial) ( Disease : Chromosome 10, 10p- Partial|Chromosome 10, monosomy 10p|Chromosome 10, Partial Deletion (short arm)|Monosomy 10p )'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities = process_medic4_ontology(ontology = \"medic\", \n",
    "                            data_path = data_path, \n",
    "                            ontology_dir = ontology_dir)\n",
    "\n",
    "entities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_mention4_dataset(ontology,\n",
    "                            dataset,\n",
    "                            data_path,\n",
    "                            ontology_dir: Optional[str] = None,\n",
    "                            mention_id: Optional[bool] = True,\n",
    "                            context_doc_id: Optional[bool] = True,\n",
    "                            ): \n",
    "    '''\n",
    "    This function prepares the mentions data :  Creates the train.jsonl, valid.jsonl, test.jsonl\n",
    "    Each .jsonl contains data in the following format : \n",
    "    {'mention': mention, \n",
    "    'mention_id': ID of the mention, (optional)\n",
    "    'context_left': context before mention,\n",
    "    'context_right': context after mention, \n",
    "    'context_doc_id': ID of the doc, (optional)\n",
    "    'type': type\n",
    "    'label_id': label_id,\n",
    "    'label': entity description, (optional)\n",
    "    'label_title': entity title\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    - ontology : str\n",
    "    Ontology associated with the dataset\n",
    "    - dataset : str\n",
    "    Name of the dataset\n",
    "    - data_path : str\n",
    "    Path where to load and save dictionary.pickle\n",
    "    - umls_dir : str\n",
    "    Path to the ontology (umls, medic etc...)\n",
    "    '''\n",
    "    data = conhelps.for_config_name(f'{dataset}_bigbio_kb').load_dataset()\n",
    "    exclude = CUIS_TO_EXCLUDE[dataset]\n",
    "    remap = CUIS_TO_REMAP[dataset]\n",
    "\n",
    "    if ontology == \"obo\" :\n",
    "        entities, equivalant_cuis = process_obo_ontology(ontology, data_path)\n",
    "    elif ontology == \"medic\" : \n",
    "        entities, equivalant_cuis = process_medic4_ontology(ontology, data_path, ontology_dir)\n",
    "    elif ontology == \"umls\" : \n",
    "        entities, equivalant_cuis = process_umls_ontology(ontology, data_path, ontology_dir)\n",
    "    else : \n",
    "        print(\"ERROR!\")\n",
    "        \n",
    "    entity_dictionary = {d['cui']:d for d in tqdm(entities)} #CC1\n",
    "\n",
    "    \"For ontology with multiples cuis\"\n",
    "    if equivalant_cuis : \n",
    "        # Need to redo this since we have multiple synonymous CUIs for ncbi_disease\n",
    "        entity_dictionary = {cui:d for d in tqdm(entities) for cui in d['cuis']}\n",
    "        cui_synsets = {}\n",
    "        for subdict in tqdm(entities): \n",
    "            for cui in subdict['cuis']:\n",
    "                if cui in subdict:\n",
    "                    print(cui, cui_synsets[cui], subdict['cuis'])\n",
    "                cui_synsets[cui] = subdict['cuis'] \n",
    "        with open(os.path.join(data_path, 'cui_synsets.json'), 'w') as f:\n",
    "            f.write(ujson.dumps(cui_synsets, indent=2))\n",
    "\n",
    "    if dataset in VALIDATION_DOCUMENT_IDS:\n",
    "        validation_pmids = VALIDATION_DOCUMENT_IDS[dataset]\n",
    "    else:\n",
    "        print(\"ERROR!!!\")\n",
    "        \n",
    "    # Convert BigBio dataset to pandas DataFrame\n",
    "    df = dataset_to_df(data, entity_remapping_dict=remap, cuis_to_exclude=exclude, val_split_ids=validation_pmids)\n",
    "    # Return dictionary of documents in BigBio dataset\n",
    "    docs = dataset_to_documents(data)\n",
    "    label_len = df['db_ids'].map(lambda x: len(x)).max()\n",
    "    print(\"Max labels on one doc:\", label_len)\n",
    "\n",
    "    for split in df.split.unique():\n",
    "        print(split)\n",
    "\n",
    "        ents_in_split = []\n",
    "        for d in tqdm(df.query(\"split == @split\").to_dict(orient='records'),\n",
    "                      desc=f\"Creating correct mention format for {split} dataset\"):\n",
    "            abbrev_resolved = False\n",
    "            offsets = d['offsets']\n",
    "            doc_id = d['document_id']\n",
    "            doc = docs[doc_id]\n",
    "            mention = d['text']\n",
    "            \n",
    "            # Get offsets and context\n",
    "            start = offsets[0][0] # start on the mention\n",
    "            end = offsets[-1][-1] # end of the mention\n",
    "            before_context = doc[:start] # left context\n",
    "            after_context = doc[end:] # right context\n",
    "            \n",
    "            \n",
    "            # ArboEL can't handle multi-labels, so we randomly choose one.\n",
    "            if len(d['db_ids']) == 1:\n",
    "                label_id = d['db_ids'][0]\n",
    "\n",
    "            # For ontology with multiples cuis\n",
    "            elif equivalant_cuis : \n",
    "                labels = []\n",
    "                used_cuis = set([])\n",
    "                choosable_ids = []\n",
    "                for db_id in d['db_ids']:\n",
    "                    if db_id in used_cuis:\n",
    "                        continue\n",
    "                    else:\n",
    "                        used_cuis.update(set(entity_dictionary[db_id]['cuis']))\n",
    "                    choosable_ids.append(db_id)\n",
    "\n",
    "                label_id = np.random.choice(choosable_ids)\n",
    "            \n",
    "            else:\n",
    "                label_id = np.random.choice(d['db_ids'])\n",
    "\n",
    "            # Check if we missed something\n",
    "            if label_id not in entity_dictionary:\n",
    "                print(label_id)\n",
    "                continue\n",
    "            \n",
    "\n",
    "            \n",
    "            output = [\n",
    "                {\n",
    "                    \"mention\": mention,\n",
    "                    \"context_left\": before_context,\n",
    "                    \"context_right\": after_context,\n",
    "                    \"type\": d[\"type\"][0],\n",
    "                    \"label_id\": label_id,\n",
    "                    \"label_title\": entity_dictionary[label_id][\"title\"],\n",
    "                    \"label\": entity_dictionary[label_id][\"description\"],\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            if mention_id:\n",
    "                output[0][\"mention_id\"] = d.get(\"mention_id\", None)\n",
    "\n",
    "            if context_doc_id:\n",
    "                output[0][\"context_doc_id\"] = d.get(\"document_id\", None)\n",
    "\n",
    "            ents_in_split.extend(output)\n",
    "\n",
    "        split_name = split\n",
    "        if split =='validation':\n",
    "            split_name = 'valid'\n",
    "        with open(os.path.join(data_path, f'{split_name}.jsonl'), 'w') as f:\n",
    "            f.write('\\n'.join([ujson.dumps(x) for x in ents_in_split]))\n",
    "            \n",
    "    return output\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/cye73/conda_envs/arboel_2/lib/python3.9/site-packages/datasets/load.py:1454: FutureWarning: The repository for bigbio/ncbi_disease contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/bigbio/ncbi_disease\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stored processed entity dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 13189/13189 [00:00<00:00, 1521415.72it/s]\n",
      "100%|| 13189/13189 [00:00<00:00, 1559546.54it/s]\n",
      "100%|| 13189/13189 [00:00<00:00, 1249546.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max labels on one doc: 5\n",
      "train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating correct mention format for train dataset: 100%|| 5065/5065 [00:00<00:00, 198853.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating correct mention format for validation dataset: 100%|| 780/780 [00:00<00:00, 167677.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating correct mention format for test dataset: 100%|| 960/960 [00:00<00:00, 253623.82it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mention': 'sporadic breast cancers',\n",
       " 'context_left': 'Localization of human BRCA1 and its loss in high-grade, non-inherited breast carcinomas.\\nAlthough the link between the BRCA1 tumour-suppressor gene and hereditary breast and ovarian cancer is established, the role, if any, of BRCA1 in non-familial cancers is unclear. BRCA1 mutations are rare in sporadic cancers, but loss of BRCA1 resulting from reduced expression or incorrect subcellular localization is postulated to be important in non-familial breast and ovarian cancers. Epigenetic loss, however, has not received general acceptance due to controversy regarding the subcellular localization of BRCA1 proteins, reports of which have ranged from exclusively nuclear, to conditionally nuclear, to the ER/golgi, to cytoplasmic invaginations into the nucleus. In an attempt to resolve this issue, we have comprehensively characterized 19 anti-BRCA1 antibodies. These reagents detect a 220-kD protein localized in discrete nuclear foci in all epithelial cell lines, including those derived from breast malignancies. Immunohistochemical staining of human breast specimens also revealed BRCA1 nuclear foci in benign breast, invasive lobular cancers and low-grade ductal carcinomas. Conversely, BRCA1 expression was reduced or undetectable in the majority of high-grade, ductal carcinomas, suggesting that absence of BRCA1 may contribute to the pathogenesis of a significant percentage of ',\n",
       " 'context_right': '..',\n",
       " 'type': 'SpecificDisease',\n",
       " 'label_id': 'MESH:D001943',\n",
       " 'label_title': 'Breast Neoplasms',\n",
       " 'label': 'Breast Neoplasms ( Disease : Breast Cancer|BREAST CANCER, FAMILIAL BREAST CANCER, FAMILIAL MALE, INCLUDED|Breast Carcinoma|Breast Carcinomas|Breast Malignant Neoplasm|Breast Malignant Neoplasms|Breast Malignant Tumor|Breast Malignant Tumors|Breast Neoplasm|Breast Tumor|Breast Tumors|Cancer, Breast|Cancer, Mammary|Cancer of Breast|Cancer of the Breast|Cancers, Mammary|Carcinoma, Breast|Carcinoma, Human Mammary|Carcinomas, Breast|Carcinomas, Human Mammary|Human Mammary Carcinoma|Human Mammary Carcinomas|Human Mammary Neoplasm|Human Mammary Neoplasms|Malignant Neoplasm of Breast|Malignant Tumor of Breast|Mammary Cancer|Mammary Cancers|Mammary Carcinoma, Human|Mammary Carcinomas, Human|Mammary Neoplasm, Human|Mammary Neoplasms, Human|Neoplasm, Breast|Neoplasm, Human Mammary|Neoplasms, Breast|Neoplasms, Human Mammary|Tumor, Breast|Tumors, Breast ) [Tumors or cancer of the human BREAST.]',\n",
       " 'mention_id': '9988281.11',\n",
       " 'context_doc_id': '9988281'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions = process_mention4_dataset(ontology = \"medic\",\n",
    "                                    dataset = dataset,\n",
    "                                    data_path = data_path,\n",
    "                                    ontology_dir = ontology_dir,\n",
    "                                    mention_id = True,\n",
    "                                    context_doc_id = True,\n",
    "                                    )\n",
    "\n",
    "mentions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST 4\n",
    "### INCLUDE resolve_abbrevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arboel_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
